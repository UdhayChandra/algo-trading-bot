{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsmsCo5WanEn"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyR2kLiVnBH8",
        "outputId": "fa221827-8951-42ab-df25-b2051aa61e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting automated dependency check...\n",
            "Cell 0: Constants and Initial Configuration - Loaded Successfully.\n",
            "\n",
            "--- Running Cell 0 Standalone Test ---\n",
            "\n",
            "--- Checking Core Dependencies ---\n",
            "‚úÖ 'pandas' is already installed.\n",
            "‚úÖ 'numpy' is already installed.\n",
            "‚úÖ 'tensorflow' is already installed.\n",
            "‚úÖ 'keras-tuner' is already installed.\n",
            "‚úÖ 'scikit-learn' is already installed.\n",
            "‚úÖ 'ta' is already installed.\n",
            "‚úÖ 'upstox-python-sdk' is already installed.\n",
            "‚úÖ 'python-telegram-bot' is already installed.\n",
            "‚úÖ 'websocket-client' is already installed.\n",
            "‚úÖ 'requests' is already installed.\n",
            "‚úÖ 'PyYAML' is already installed.\n",
            "‚úÖ 'pydantic' is already installed.\n",
            "‚úÖ 'python-dotenv' is already installed.\n",
            "‚úÖ 'joblib' is already installed.\n",
            "‚úÖ 'pytz' is already installed.\n",
            "‚úÖ 'nest-asyncio' is already installed.\n",
            "‚úÖ 'matplotlib' is already installed.\n",
            "‚úÖ 'protobuf' is already installed.\n",
            "\n",
            "üéâ All dependencies were already satisfied. No new installations needed.\n",
            "\n",
            "--- Dependency check complete ---\n",
            "Default Base Project Path: /content/drive/MyDrive/main\n",
            "--- Test Complete ---\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 0: Dependency check & Download ---\n",
        "\n",
        "# --- I. Standard Library Imports ---\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "print(\"üöÄ Starting automated dependency check...\")\n",
        "\n",
        "def check_and_install_dependencies():\n",
        "    \"\"\"\n",
        "    Checks for a predefined list of packages. If a package is not found,\n",
        "    it attempts to install it via pip.\n",
        "\n",
        "    This function handles packages where the pip installation name is\n",
        "    different from the library import name (e.g., 'scikit-learn' vs 'sklearn').\n",
        "    \"\"\"\n",
        "    packages = {\n",
        "        # Core Data Science & Numerics\n",
        "        'pandas': 'pandas',\n",
        "        'numpy': 'numpy',\n",
        "\n",
        "        # Machine Learning & AI\n",
        "        'tensorflow': 'tensorflow',\n",
        "        'keras-tuner': 'keras_tuner',\n",
        "        'scikit-learn': 'sklearn',\n",
        "\n",
        "        # Financial Technical Analysis\n",
        "        'ta': 'ta',\n",
        "\n",
        "        # API Clients & Web\n",
        "        'upstox-python-sdk': 'upstox_client',\n",
        "        'python-telegram-bot': 'telegram',\n",
        "        'websocket-client': 'websocket',\n",
        "        'requests': 'requests',\n",
        "\n",
        "        # Configuration & Utilities\n",
        "        'PyYAML': 'yaml',\n",
        "        'pydantic': 'pydantic',\n",
        "        'python-dotenv': 'dotenv',\n",
        "        'joblib': 'joblib',\n",
        "        'pytz': 'pytz',\n",
        "        'nest-asyncio': 'nest_asyncio',\n",
        "\n",
        "        # Plotting & Visualization\n",
        "        'matplotlib': 'matplotlib',\n",
        "\n",
        "        # Google's Protocol Buffers (often a dependency)\n",
        "        'protobuf': 'google.protobuf',\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Checking Core Dependencies ---\")\n",
        "    all_good = True\n",
        "    for package_name, import_name in packages.items():\n",
        "        try:\n",
        "            # Try to import the package\n",
        "            importlib.import_module(import_name)\n",
        "            print(f\"‚úÖ '{package_name}' is already installed.\")\n",
        "        except ImportError:\n",
        "            all_good = False\n",
        "            print(f\"‚ö†Ô∏è Package '{package_name}' not found. Attempting to install...\")\n",
        "            try:\n",
        "\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "                print(f\"‚úÖ Successfully installed '{package_name}'.\")\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"‚ùå ERROR: Failed to install '{package_name}'. Please install it manually.\", file=sys.stderr)\n",
        "                print(f\"   Error details: {e}\", file=sys.stderr)\n",
        "                sys.exit(1)\n",
        "\n",
        "    if all_good:\n",
        "        print(\"\\nüéâ All dependencies were already satisfied. No new installations needed.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ All required dependencies have been checked and installed.\")\n",
        "        print(\"üí° NOTE: In some environments (like Jupyter or Colab), you may need to RESTART the kernel for the new packages to be recognized.\")\n",
        "\n",
        "\n",
        "# --- II. Core Application Paths and Constants ---\n",
        "DEFAULT_BASE_PROJECT_PATH = \"/content/drive/MyDrive/main\"\n",
        "\n",
        "# --- III. Date, Time, and Market Constants ---\n",
        "UPSTOX_DATE_FORMAT = \"%Y-%m-%d\"\n",
        "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
        "MARKET_TIMEZONE_STR = \"Asia/Kolkata\"\n",
        "\n",
        "\n",
        "# --- IV. Model and Trading Logic Constants ---\n",
        "CLASS_LABELS = {\n",
        "    0: 'BUY',\n",
        "    1: 'HOLD',\n",
        "    2: 'SELL'\n",
        "}\n",
        "\n",
        "\n",
        "# --- V. Upstox API Related Constants ---\n",
        "UPSTOX_HISTORY_INTERVAL_MAP: dict[str, str] = {\n",
        "    \"1minute\": \"1minute\", \"1min\": \"1minute\", \"1m\": \"1minute\", \"minute\": \"1minute\",\n",
        "    \"3minute\": \"3minute\", \"3min\": \"3minute\", \"3m\": \"3minute\",\n",
        "    \"5minute\": \"5minute\", \"5min\": \"5minute\", \"5m\": \"5minute\",\n",
        "    \"10minute\": \"10minute\", \"10min\": \"10minute\", \"10m\": \"10minute\",\n",
        "    \"15minute\": \"15minute\", \"15min\": \"15minute\", \"15m\": \"15minute\",\n",
        "    \"30minute\": \"30minute\", \"30min\": \"30minute\", \"30m\": \"30minute\",\n",
        "    \"60minute\": \"60minute\", \"1hour\": \"60minute\", \"1hr\": \"60minute\", \"1h\": \"60minute\", \"hour\": \"60minute\",\n",
        "    \"day\": \"day\", \"1day\": \"day\", \"1d\": \"day\", \"daily\": \"day\",\n",
        "    \"week\": \"week\", \"1week\": \"week\", \"1w\": \"week\", \"weekly\": \"week\",\n",
        "    \"month\": \"month\", \"1month\": \"month\", \"1mo\": \"month\", \"monthly\": \"month\"\n",
        "}\n",
        "\n",
        "print(\"Cell 0: Constants and Initial Configuration - Loaded Successfully.\")\n",
        "\n",
        "# --- Standalone Execution Test Block ---\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Running Cell 0 Standalone Test ---\")\n",
        "    check_and_install_dependencies()\n",
        "    print(\"\\n--- Dependency check complete ---\")\n",
        "    print(f\"Default Base Project Path: {DEFAULT_BASE_PROJECT_PATH}\")\n",
        "    print(\"--- Test Complete ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHBDdK65nWQu",
        "outputId": "cab9fe22-0c4b-4dbb-ecb8-5ed996749daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 1: Initial Setup, Imports, and Configuration Loading\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:89] - Logger initialized successfully for the TradingBot (Timezone: IST).\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:99] - python-telegram-bot library (v20+ compatible) found and imported successfully.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:113] - Matplotlib configured with 'Agg' backend for non-interactive plotting.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:169] - Successfully imported core Upstox SDK classes directly.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:195] - Successfully imported FeedResponse from Upstox SDK.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:201] - Official Upstox Python SDK (v2.x) base module loaded.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:221] - websocket-client library is available (for potential direct WebSocket usage if needed).\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:254] - Successfully loaded .env file from BASE_PROJECT_PATH: /content/drive/MyDrive/main/.env\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.load_config_from_yaml:284] - Configuration loaded successfully from: /content/drive/MyDrive/main/config.yaml\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:395] - Running in Google Colab environment.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:417] - Mixed precision training configured to be enabled, but no GPU available or TensorFlow cannot access it. Mixed precision not activated.\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:429] - XLA JIT compilation for model training will be attempted (set via model.compile(jit_compile=True)).\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:435] - TensorFlow version: 2.18.0\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-3-24434631>.<cell line: 0>:449] - No Physical GPU available for TensorFlow, or TensorFlow cannot detect it. TensorFlow operations will use CPU.\n",
            "Cell 1: Initial Setup, Imports, and Configuration Loading - Complete.\n",
            "\n",
            "--- Cell 1 Standalone Test ---\n",
            "Logger Name: TradingBotLogger, Level: INFO\n",
            "Base Project Path: /content/drive/MyDrive/main\n",
            "Config Loaded (sample): ['directory_paths', 'upstox', 'trading_parameters', 'model_params', 'indicator_params', 'pattern_params', 'training_params', 'tuner_params', 'strategy_params', 'backtesting_params', 'live_trading_params', 'market_hours', 'ensemble_params']\n",
            "TensorFlow GPU Available: False\n",
            "Mixed Precision Enabled by Config: True\n",
            "XLA JIT Enabled by Config: True\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 1: Initial Setup, Imports, and Configuration Loading ---\n",
        "\n",
        "print(\"\\nInitializing Cell 1: Initial Setup, Imports, and Configuration Loading\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import yaml\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime, timedelta, date as datetime_date\n",
        "import pytz\n",
        "import joblib\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "import collections\n",
        "import uuid\n",
        "import asyncio\n",
        "\n",
        "# --- Machine Learning and Data Processing Libraries ---\n",
        "import ta\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model as keras_load_model # Use an alias for clarity\n",
        "from tensorflow.keras.layers import (\n",
        "    LSTM, Dense, Dropout, Bidirectional, Input, Conv1D, MaxPooling1D, Flatten,\n",
        "    BatchNormalization, LayerNormalization, Add, Activation, Multiply, GlobalAveragePooling1D,\n",
        "    MultiHeadAttention\n",
        ")\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import keras_tuner as kt # For hyperparameter tuning\n",
        "\n",
        "# --- Constants from Cell 0 (Assumed to be in the same global scope or imported if modularized) ---\n",
        "if 'DEFAULT_BASE_PROJECT_PATH' not in globals():\n",
        "    print(\"Warning: DEFAULT_BASE_PROJECT_PATH (from Cell 0) not found. Defining a fallback for Cell 1.\")\n",
        "    DEFAULT_BASE_PROJECT_PATH = \"/content/drive/MyDrive/main\"\n",
        "\n",
        "\n",
        "if 'MARKET_TIMEZONE_STR' not in globals():\n",
        "    print(\"Warning: MARKET_TIMEZONE_STR (from Cell 0) not found. Defining a fallback for Cell 1.\")\n",
        "    MARKET_TIMEZONE_STR = \"Asia/Kolkata\" # default for Indian markets\n",
        "\n",
        "class ISTFormatter(logging.Formatter):\n",
        "    \"\"\"A custom logging formatter to display timestamps in Indian Standard Time (IST).\"\"\"\n",
        "\n",
        "    # Get the timezone object defined by the 'MARKET_TIMEZONE_STR' constant from Cell 0\n",
        "    converter_tz = pytz.timezone(MARKET_TIMEZONE_STR)\n",
        "\n",
        "    def formatTime(self, record, datefmt=None):\n",
        "        # Convert the record's creation time (a UTC Unix timestamp) to a datetime object\n",
        "        utc_dt = datetime.fromtimestamp(record.created, pytz.utc)\n",
        "\n",
        "        # Convert the UTC datetime object to the target timezone (IST)\n",
        "        local_dt = utc_dt.astimezone(self.converter_tz)\n",
        "\n",
        "        # Format the localized datetime object\n",
        "        if datefmt:\n",
        "            s = local_dt.strftime(datefmt)\n",
        "        else:\n",
        "            s = local_dt.isoformat()\n",
        "        return s\n",
        "\n",
        "# --- Logger Initialization ---\n",
        "logger = logging.getLogger(\"TradingBotLogger\")\n",
        "if not logger.handlers:\n",
        "    logger.setLevel(logging.INFO)\n",
        "    console_handler = logging.StreamHandler(sys.stdout)\n",
        "\n",
        "    # --- Use the new ISTFormatter class instead of the default one ---\n",
        "    log_formatter = ISTFormatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - [%(module)s.%(funcName)s:%(lineno)d] - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "    logger.addHandler(console_handler)\n",
        "    logger.propagate = False\n",
        "    logger.info(\"Logger initialized successfully for the TradingBot (Timezone: IST).\")\n",
        "else:\n",
        "    logger.info(\"TradingBot logger was already initialized.\")\n",
        "\n",
        "# --- Telegram Bot Library Availability Check (for python-telegram-bot v20+) ---\n",
        "try:\n",
        "    from telegram.ext import Application as TelegramApplication\n",
        "    from telegram import Bot as TelegramBot\n",
        "    from telegram.error import TelegramError\n",
        "    TELEGRAM_BOT_AVAILABLE = True\n",
        "    logger.info(\"python-telegram-bot library (v20+ compatible) found and imported successfully.\")\n",
        "except ImportError:\n",
        "    TELEGRAM_BOT_AVAILABLE = False\n",
        "    TelegramApplication = None\n",
        "    TelegramBot = None\n",
        "    TelegramError = Exception\n",
        "    logger.warning(\"python-telegram-bot library not found. Telegram notifications will be disabled.\")\n",
        "\n",
        "# --- Matplotlib Configuration (for non-interactive backend) ---\n",
        "import matplotlib\n",
        "try:\n",
        "    matplotlib.use('Agg') # For generating plots without a GUI (e.g., saving to file)\n",
        "    import matplotlib.pyplot as plt\n",
        "    MATPLOTLIB_AVAILABLE = True\n",
        "    logger.info(\"Matplotlib configured with 'Agg' backend for non-interactive plotting.\")\n",
        "except ImportError:\n",
        "    MATPLOTLIB_AVAILABLE = False\n",
        "    plt = None\n",
        "    logger.warning(\"Matplotlib not found. Plotting functionalities will be disabled.\")\n",
        "except Exception as e_mpl:\n",
        "    MATPLOTLIB_AVAILABLE = False\n",
        "    plt = None\n",
        "    logger.error(f\"Error configuring Matplotlib: {e_mpl}. Plotting disabled.\", exc_info=True)\n",
        "\n",
        "\n",
        "# --- Upstox SDK Availability Check and Core Imports (for upstox-python-sdk v2.x) ---\n",
        "UPSTOX_SDK_AVAILABLE = False\n",
        "UPSTOX_PROTOBUF_MODULE_AVAILABLE = False\n",
        "FeedResponse = None\n",
        "UpstoxApiException = None\n",
        "# Explicitly declare API class variables for global scope, to be populated by imports\n",
        "upstox_client = None\n",
        "UserApi = None\n",
        "PortfolioApi = None\n",
        "OrderApi = None\n",
        "HistoryApi = None\n",
        "LoginApi = None\n",
        "MarketDataStreamer = None\n",
        "Configuration = None\n",
        "ApiClient = None\n",
        "\n",
        "\n",
        "try:\n",
        "    import upstox_client as sdk_module\n",
        "    upstox_client = sdk_module\n",
        "\n",
        "    try:\n",
        "        from upstox_client import (\n",
        "            UserApi as SDK_UserApi,\n",
        "            PortfolioApi as SDK_PortfolioApi,\n",
        "            OrderApi as SDK_OrderApi,\n",
        "            HistoryApi as SDK_HistoryApi,\n",
        "            LoginApi as SDK_LoginApi,\n",
        "            MarketDataStreamer as SDK_MarketDataStreamer,\n",
        "            Configuration as SDK_Configuration,\n",
        "            ApiClient as SDK_ApiClient\n",
        "        )\n",
        "        from upstox_client.rest import ApiException as SDKUpstoxApiException\n",
        "\n",
        "        # Assign to global variables\n",
        "        UserApi = SDK_UserApi\n",
        "        PortfolioApi = SDK_PortfolioApi\n",
        "        OrderApi = SDK_OrderApi\n",
        "        HistoryApi = SDK_HistoryApi\n",
        "        LoginApi = SDK_LoginApi\n",
        "        MarketDataStreamer = SDK_MarketDataStreamer\n",
        "        Configuration = SDK_Configuration\n",
        "        ApiClient = SDK_ApiClient\n",
        "        UpstoxApiException = SDKUpstoxApiException\n",
        "\n",
        "        logger.info(\"Successfully imported core Upstox SDK classes directly.\")\n",
        "\n",
        "    except ImportError as e_class_import:\n",
        "        logger.warning(f\"Could not import some Upstox SDK classes directly: {e_class_import}. \"\n",
        "                       f\"Will rely on accessing them via 'upstox_client.ClassName'.\")\n",
        "        # Fallback to accessing via module if direct imports fail but base module is present\n",
        "        if sdk_module:\n",
        "            UserApi = getattr(sdk_module, 'UserApi', None)\n",
        "            PortfolioApi = getattr(sdk_module, 'PortfolioApi', None)\n",
        "            OrderApi = getattr(sdk_module, 'OrderApi', None)\n",
        "            HistoryApi = getattr(sdk_module, 'HistoryApi', None)\n",
        "            LoginApi = getattr(sdk_module, 'LoginApi', None)\n",
        "            MarketDataStreamer = getattr(sdk_module, 'MarketDataStreamer', None)\n",
        "            Configuration = getattr(sdk_module, 'Configuration', None)\n",
        "            ApiClient = getattr(sdk_module, 'ApiClient', None)\n",
        "            if hasattr(sdk_module, 'rest') and hasattr(sdk_module.rest, 'ApiException'):\n",
        "                 UpstoxApiException = sdk_module.rest.ApiException\n",
        "            if not all([UserApi, PortfolioApi, OrderApi, HistoryApi, LoginApi, MarketDataStreamer, Configuration, ApiClient, UpstoxApiException]):\n",
        "                logger.error(\"Failed to assign all necessary Upstox SDK classes even via getattr.\")\n",
        "\n",
        "\n",
        "    # Attempt to import FeedResponse for decoding WebSocket Protobuf messages.\n",
        "    try:\n",
        "        from upstox_client.feeder.proto.MarketDataFeed_pb2 import FeedResponse as SDK_FeedResponse\n",
        "        FeedResponse = SDK_FeedResponse\n",
        "        UPSTOX_PROTOBUF_MODULE_AVAILABLE = True\n",
        "        logger.info(\"Successfully imported FeedResponse from Upstox SDK.\")\n",
        "    except ImportError:\n",
        "        logger.warning(\"Could not import FeedResponse from 'upstox_client.feeder.proto.MarketDataFeed_pb2'.\")\n",
        "        FeedResponse = None\n",
        "\n",
        "    UPSTOX_SDK_AVAILABLE = True # If 'import upstox_client' succeeded\n",
        "    logger.info(\"Official Upstox Python SDK (v2.x) base module loaded.\")\n",
        "\n",
        "except ImportError as e_sdk_import_base:\n",
        "    logger.error(f\"Failed to import the base Upstox SDK (upstox_client): {e_sdk_import_base}. \"\n",
        "                 \"Upstox API functionalities will be unavailable.\", exc_info=False)\n",
        "    UPSTOX_SDK_AVAILABLE = False\n",
        "    # Define placeholders if SDK import failed completely\n",
        "    class UpstoxApiExceptionPlaceholder(Exception):\n",
        "        def __init__(self, message=\"Upstox SDK not available.\", status=None, reason=None, http_resp=None, body=None, headers=None):\n",
        "            super().__init__(message); self.status = status; self.reason = reason; self.http_resp = http_resp; self.body = body; self.headers = headers\n",
        "    UpstoxApiException = UpstoxApiExceptionPlaceholder\n",
        "    # All other API classes (UserApi, PortfolioApi, etc.) remain None\n",
        "except Exception as e_sdk_other:\n",
        "    logger.error(f\"An unexpected error occurred during Upstox SDK import attempts: {e_sdk_other}\", exc_info=True)\n",
        "    UPSTOX_SDK_AVAILABLE = False\n",
        "\n",
        "# --- WebSocket Client Library Check (Optional, if direct WebSocket usage is planned outside SDK's streamer) ---\n",
        "try:\n",
        "    import websocket\n",
        "    WEBSOCKET_CLIENT_AVAILABLE = True\n",
        "    logger.info(\"websocket-client library is available (for potential direct WebSocket usage if needed).\")\n",
        "except ImportError:\n",
        "    WEBSOCKET_CLIENT_AVAILABLE = False\n",
        "    logger.warning(\"websocket-client library not found. Direct WebSocket functionality (if implemented outside SDK) will be unavailable.\")\n",
        "\n",
        "\n",
        "# --- Environment and Configuration File Loading ---\n",
        "# Determine the base project path (from environment variable or default from Cell 0)\n",
        "BASE_PROJECT_PATH = os.getenv('BASE_PROJECT_PATH', DEFAULT_BASE_PROJECT_PATH)\n",
        "if not os.path.isdir(BASE_PROJECT_PATH):\n",
        "    try:\n",
        "        os.makedirs(BASE_PROJECT_PATH, exist_ok=True) # exist_ok=True prevents error if dir already exists\n",
        "        logger.info(f\"Created BASE_PROJECT_PATH directory: {BASE_PROJECT_PATH}\")\n",
        "    except OSError as e_dir_create:\n",
        "        logger.error(f\"Could not create BASE_PROJECT_PATH '{BASE_PROJECT_PATH}': {e_dir_create}. \"\n",
        "                     \"Script might not function correctly if it relies on this path for storing files.\")\n",
        "        # Fallback to current working directory if base path creation fails, with a warning\n",
        "        BASE_PROJECT_PATH = os.getcwd()\n",
        "        logger.warning(f\"Fell back to using current working directory as BASE_PROJECT_PATH: {BASE_PROJECT_PATH}\")\n",
        "\n",
        "\n",
        "# Priority for .env file: Current Working Directory (CWD) > BASE_PROJECT_PATH.\n",
        "dotenv_path_cwd = os.path.join(os.getcwd(), '.env')\n",
        "dotenv_path_base_project = os.path.join(BASE_PROJECT_PATH, '.env')\n",
        "loaded_env_path = None\n",
        "\n",
        "if os.path.exists(dotenv_path_cwd):\n",
        "    if load_dotenv(dotenv_path_cwd, verbose=True, override=True): # override=True ensures .env takes precedence\n",
        "        loaded_env_path = dotenv_path_cwd\n",
        "        logger.info(f\"Successfully loaded .env file from CWD: {loaded_env_path}\")\n",
        "elif os.path.exists(dotenv_path_base_project): # Check base project path only if not in CWD\n",
        "    if load_dotenv(dotenv_path_base_project, verbose=True, override=True):\n",
        "        loaded_env_path = dotenv_path_base_project\n",
        "        logger.info(f\"Successfully loaded .env file from BASE_PROJECT_PATH: {loaded_env_path}\")\n",
        "\n",
        "if not loaded_env_path:\n",
        "    logger.info(f\".env file not found in CWD ('{dotenv_path_cwd}') or \"\n",
        "                f\"BASE_PROJECT_PATH ('{dotenv_path_base_project}'). \"\n",
        "                \"The script will rely on pre-set system environment variables or default configurations from config.yaml.\")\n",
        "\n",
        "# --- Configuration Loading Functions (from config.yaml) ---\n",
        "def load_config_from_yaml(default_config_filename: str = \"config.yaml\") -> dict:\n",
        "\n",
        "    config_path_in_base = os.path.join(BASE_PROJECT_PATH, default_config_filename)\n",
        "    config_path_in_cwd = os.path.join(os.getcwd(), default_config_filename)\n",
        "    config_path_to_use = None\n",
        "\n",
        "    if os.path.exists(config_path_in_base):\n",
        "        config_path_to_use = config_path_in_base\n",
        "    elif os.path.exists(config_path_in_cwd): # Check CWD if not found in base project path\n",
        "        config_path_to_use = config_path_in_cwd\n",
        "\n",
        "    if not config_path_to_use:\n",
        "        logger.warning(\n",
        "            f\"YAML config file '{default_config_filename}' not found in \"\n",
        "            f\"search paths ('{BASE_PROJECT_PATH}', '{os.getcwd()}'). \"\n",
        "            \"Using an empty configuration dictionary. The bot may not function as expected.\"\n",
        "        )\n",
        "        return {} # Return an empty dict if no config file is found to prevent NoneErrors\n",
        "\n",
        "    try:\n",
        "        with open(config_path_to_use, 'r') as f:\n",
        "            config_yaml_content = yaml.safe_load(f) # Use safe_load for security\n",
        "        logger.info(f\"Configuration loaded successfully from: {config_path_to_use}\")\n",
        "        # Ensure it returns a dict, even if YAML file is empty or just a single value\n",
        "        return config_yaml_content if isinstance(config_yaml_content, dict) else {}\n",
        "    except yaml.YAMLError as e_yaml_parse: # Catch specific YAML parsing errors\n",
        "        logger.error(f\"Error parsing YAML config file '{config_path_to_use}': {e_yaml_parse}\", exc_info=True)\n",
        "    except IOError as e_file_io: # Catch file I/O errors (e.g., permission denied)\n",
        "        logger.error(f\"Error reading YAML config file '{config_path_to_use}': {e_file_io}\", exc_info=True)\n",
        "    except Exception as e_general_load: # Catch any other unexpected errors during loading\n",
        "        logger.error(f\"An unexpected error occurred while loading YAML config '{config_path_to_use}': {e_general_load}\", exc_info=True)\n",
        "    return {}\n",
        "\n",
        "# Load the main configuration dictionary from config.yaml into a global variable 'CONFIG'\n",
        "CONFIG: dict = load_config_from_yaml() # Uses default filename \"config.yaml\"\n",
        "if not CONFIG: # Check if CONFIG is empty after attempting to load\n",
        "    logger.warning(\"Global CONFIG dictionary is empty (config.yaml likely not found or empty/invalid). \"\n",
        "                   \"The bot will rely heavily on default values defined in the script or environment variables, \"\n",
        "                   \"which might not be suitable for all operations, especially trading.\")\n",
        "\n",
        "def get_config_value(\n",
        "    yaml_keys_path: list[str] | None, # Path to navigate in YAML dict, e.g., ['trading_params', 'symbols']\n",
        "    env_var_key: str | None,          # Environment variable name to check as a fallback\n",
        "    default_val: any = None,          # Default value if not found in YAML or environment\n",
        "    expected_type: type | None = None # Expected data type (bool, int, float, list, dict) for casting\n",
        ") -> any:\n",
        "\n",
        "    global CONFIG # Use the global CONFIG dictionary loaded earlier\n",
        "\n",
        "    val_to_process = None\n",
        "    source_of_value = \"Default Value\" # Initial assumption\n",
        "    found_in_yaml = False\n",
        "\n",
        "    # 1. Check YAML Configuration first\n",
        "    if CONFIG and yaml_keys_path:\n",
        "        current_level = CONFIG\n",
        "        try:\n",
        "            for key_part in yaml_keys_path:\n",
        "                if isinstance(current_level, dict):\n",
        "                    current_level = current_level[key_part]\n",
        "                else:\n",
        "                    # Path is invalid if an intermediate key does not lead to a dictionary\n",
        "                    raise KeyError(f\"Path part '{key_part}' not found or not a dict in YAML structure.\")\n",
        "            val_to_process = current_level\n",
        "            source_of_value = \"YAML\"\n",
        "            found_in_yaml = True\n",
        "        except (KeyError, TypeError):\n",
        "            pass\n",
        "\n",
        "    # 2. If not found in YAML, check Environment Variable\n",
        "    if not found_in_yaml:\n",
        "        if env_var_key and os.getenv(env_var_key) is not None:\n",
        "            val_to_process = os.getenv(env_var_key)\n",
        "            source_of_value = \"Environment Variable\"\n",
        "        else:\n",
        "            # Not found in YAML and not in ENV (or no env_var_key provided)\n",
        "            val_to_process = default_val\n",
        "            source_of_value = \"Default Value\" # Explicitly set source if default is used here\n",
        "\n",
        "    # If val_to_process is None at this point, it means default_val was None and nothing was found.\n",
        "    if val_to_process is None:\n",
        "        return None\n",
        "\n",
        "    # Determine expected type if not explicitly provided\n",
        "    if expected_type is None and default_val is not None:\n",
        "        expected_type = type(default_val)\n",
        "\n",
        "    # Perform type conversion\n",
        "    try:\n",
        "        if expected_type is bool:\n",
        "            if isinstance(val_to_process, str): return val_to_process.lower() in ('true', '1', 't', 'yes', 'y')\n",
        "            return bool(val_to_process)\n",
        "        if expected_type is int: return int(float(val_to_process)) # float conversion for \"1.0\" -> 1\n",
        "        if expected_type is float: return float(val_to_process)\n",
        "        if expected_type is list:\n",
        "            if isinstance(val_to_process, list): return val_to_process\n",
        "            if isinstance(val_to_process, str):\n",
        "                try: # Try parsing as JSON list\n",
        "                    parsed_json = json.loads(val_to_process)\n",
        "                    if isinstance(parsed_json, list): return parsed_json\n",
        "                except json.JSONDecodeError: # If not JSON, try comma-separated\n",
        "                    if ',' in val_to_process: return [item.strip() for item in val_to_process.split(',') if item.strip()]\n",
        "            # If not already a list or parsable string, it's a mismatch for 'list' type\n",
        "            raise ValueError(f\"Cannot convert '{val_to_process}' to list.\")\n",
        "        if expected_type is dict:\n",
        "            if isinstance(val_to_process, dict): return val_to_process\n",
        "            if isinstance(val_to_process, str): # Try parsing as JSON dict\n",
        "                parsed_json = json.loads(val_to_process)\n",
        "                if isinstance(parsed_json, dict): return parsed_json\n",
        "            # If not already a dict or parsable JSON string, it's a mismatch\n",
        "            raise ValueError(f\"Cannot convert '{val_to_process}' to dict.\")\n",
        "\n",
        "        # If no specific type conversion matched but expected_type is set, check instance\n",
        "        if expected_type and not isinstance(val_to_process, expected_type):\n",
        "             # Attempt direct casting if types don't match but expected_type is known\n",
        "            try:\n",
        "                return expected_type(val_to_process)\n",
        "            except (ValueError, TypeError):\n",
        "                raise TypeError(f\"Value '{val_to_process}' (type {type(val_to_process)}) is not of expected type {expected_type} and direct cast failed.\")\n",
        "        return val_to_process # Return as is if no specific type or conversion needed/matched\n",
        "\n",
        "    except (ValueError, TypeError, json.JSONDecodeError) as e_cast:\n",
        "        if source_of_value != \"Default Value\":\n",
        "             logger.warning(f\"Config value '{val_to_process}' (from {source_of_value}) for '{yaml_keys_path or env_var_key}' \"\n",
        "                           f\"could not be cast to {expected_type or 'inferred type'}. Error: {e_cast}. Using default: {default_val}.\")\n",
        "        return default_val\n",
        "\n",
        "\n",
        "# --- TensorFlow Environment Checks ---\n",
        "try:\n",
        "    # Attempt to import google.colab. This will only succeed if running in a Colab environment.\n",
        "    import google.colab # type: ignore\n",
        "    IN_COLAB = True\n",
        "    logger.info(\"Running in Google Colab environment.\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    logger.info(\"Not running in Google Colab environment (or google.colab module not found).\")\n",
        "\n",
        "\n",
        "# Mixed Precision for TensorFlow (if enabled in config and a GPU is available)\n",
        "MIXED_PRECISION_ENABLED = get_config_value(\n",
        "    yaml_keys_path=['training_params', 'mixed_precision_enabled'],\n",
        "    env_var_key='MIXED_PRECISION_ENABLED_ENV',\n",
        "    default_val=False,\n",
        "    expected_type=bool\n",
        ")\n",
        "if MIXED_PRECISION_ENABLED:\n",
        "    physical_gpus_mp = tf.config.list_physical_devices('GPU')\n",
        "    if physical_gpus_mp:\n",
        "        try:\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "            logger.info(\"Mixed precision training policy ('mixed_float16') enabled for TensorFlow.\")\n",
        "        except Exception as e_mixed_precision:\n",
        "            logger.error(f\"Failed to enable mixed precision for TensorFlow: {e_mixed_precision}\", exc_info=True)\n",
        "    else:\n",
        "        logger.info(\"Mixed precision training configured to be enabled, but no GPU available or TensorFlow cannot access it. Mixed precision not activated.\")\n",
        "else:\n",
        "    logger.info(\"Mixed precision training is disabled by configuration.\")\n",
        "\n",
        "# XLA (Accelerated Linear Algebra) JIT Compilation for TensorFlow (if enabled in config)\n",
        "XLA_ENABLED = get_config_value(\n",
        "    yaml_keys_path=['training_params', 'xla_enabled'],\n",
        "    env_var_key='XLA_ENABLED_ENV',\n",
        "    default_val=False,\n",
        "    expected_type=bool\n",
        ")\n",
        "if XLA_ENABLED:\n",
        "    logger.info(\"XLA JIT compilation for model training will be attempted (set via model.compile(jit_compile=True)).\")\n",
        "else:\n",
        "    logger.info(\"XLA JIT compilation for model training is disabled by configuration.\")\n",
        "\n",
        "\n",
        "# TensorFlow GPU Setup and Information\n",
        "logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "physical_gpu_devices_tf = tf.config.list_physical_devices('GPU')\n",
        "if physical_gpu_devices_tf:\n",
        "    logger.info(f\"Found Physical GPU(s) for TensorFlow: {physical_gpu_devices_tf}\")\n",
        "    try:\n",
        "        for gpu_dev_tf in physical_gpu_devices_tf:\n",
        "            tf.config.experimental.set_memory_growth(gpu_dev_tf, True)\n",
        "        logger.info(\"GPU memory growth enabled for all detected physical GPUs for TensorFlow.\")\n",
        "    except RuntimeError as e_gpu_runtime_error:\n",
        "        # This error often means memory growth must be set before GPUs have been initialized\n",
        "        logger.warning(f\"Could not set GPU memory growth (GPU might already be initialized by TensorFlow): {e_gpu_runtime_error}\")\n",
        "    except Exception as e_gpu_general_error:\n",
        "        logger.error(f\"An unexpected error occurred during TensorFlow GPU setup: {e_gpu_general_error}\", exc_info=True)\n",
        "else:\n",
        "    logger.info(\"No Physical GPU available for TensorFlow, or TensorFlow cannot detect it. TensorFlow operations will use CPU.\")\n",
        "\n",
        "print(\"Cell 1: Initial Setup, Imports, and Configuration Loading - Complete.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block can be used for quick testing of Cell 1 functionalities.\n",
        "    print(\"\\n--- Cell 1 Standalone Test ---\")\n",
        "    print(f\"Logger Name: {logger.name}, Level: {logging.getLevelName(logger.level)}\")\n",
        "    print(f\"Base Project Path: {BASE_PROJECT_PATH}\")\n",
        "    print(f\"Config Loaded (sample): {list(CONFIG.keys()) if CONFIG else 'No Config Loaded'}\")\n",
        "\n",
        "    print(f\"TensorFlow GPU Available: {True if physical_gpu_devices_tf else False}\")\n",
        "    print(f\"Mixed Precision Enabled by Config: {MIXED_PRECISION_ENABLED}\")\n",
        "    if MIXED_PRECISION_ENABLED and physical_gpu_devices_tf : print(f\"  TF Mixed Precision Policy: {tf.keras.mixed_precision.global_policy().name}\")\n",
        "    print(f\"XLA JIT Enabled by Config: {XLA_ENABLED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYLreDhdtd7O",
        "outputId": "fccf2dcc-d789-4055-a28a-7fe7db229cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 2: Configuration Definitions and Global Variables\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-4-1352014288>.<cell line: 0>:189] - Initial symbols from config: 2. Validated and active symbols for session: 2 -> IRFC, IRB\n",
            "2025-06-17 17:49:04 - TradingBotLogger - INFO - [<ipython-input-4-1352014288>.<cell line: 0>:413] - Cell 2: Configuration Definitions and Global Variables - Complete.\n",
            "\n",
            "--- Cell 2 Standalone Test ---\n",
            "Historical Data Dir: /content/drive/MyDrive/main/data_historical\n",
            "Symbols List (from config): ['IRFC', 'IRB']\n",
            "Validated Symbols List: ['IRFC', 'IRB']\n",
            "Target Interval: 1minute\n",
            "Lookback Window: 60\n",
            "Auto Order Execution Enabled: False\n",
            "Max Daily Loss Fixed: 400.0, Margin Threshold: 20000.0, Margin Percent: 3.0%\n",
            "Capital Threshold for Multi Trade: 30000.0\n",
            "Max Trades Per Symbol: 2, Global Max Trades: 10\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 2: Configuration Definitions and Global Variables ---\n",
        "\n",
        "print(\"\\nInitializing Cell 2: Configuration Definitions and Global Variables\")\n",
        "\n",
        "# --- Standard Library Imports (ensure these are available if not already from Cell 1) ---\n",
        "import os\n",
        "import json\n",
        "import pytz\n",
        "from datetime import datetime, date as datetime_date, timedelta, time as datetime_time\n",
        "import collections\n",
        "import threading\n",
        "import uuid # For generating unique identifiers if needed\n",
        "from typing import List, Dict, Any, Optional # For type hinting\n",
        "\n",
        "# --- Ensure necessary variables and functions from Cell 0 and Cell 1 are available ---\n",
        "\n",
        "# Logger (from Cell 1)\n",
        "if 'logger' not in globals():\n",
        "    import logging as pylogging # Use alias to avoid conflict if logging is imported again\n",
        "    import sys as pysys\n",
        "    logger = pylogging.getLogger(\"TradingBotLogger_C2_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c2 = pylogging.StreamHandler(pysys.stdout)\n",
        "        _ch_c2.setFormatter(pylogging.Formatter('%(asctime)s - %(levelname)s - C2_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c2)\n",
        "        logger.setLevel(pylogging.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 2.\")\n",
        "\n",
        "# CONFIG dictionary (from Cell 1, loaded from config.yaml)\n",
        "if 'CONFIG' not in globals():\n",
        "    CONFIG: Dict[str, Any] = {} # Initialize as an empty dict if not found\n",
        "    logger.warning(\"Global 'CONFIG' dictionary (from Cell 1) not found. \"\n",
        "                   \"Cell 2 will rely on ENV vars or script defaults for YAML-expected values.\")\n",
        "\n",
        "# get_config_value function (from Cell 1)\n",
        "if 'get_config_value' not in globals():\n",
        "    def get_config_value(yaml_keys_path: list[str] | None,\n",
        "                         env_var_key: str | None,\n",
        "                         default_val: Any = None,\n",
        "                         expected_type: type | None = None) -> Any:\n",
        "        logger.critical(f\"CRITICAL: 'get_config_value' function (from Cell 1) not found. \"\n",
        "                        f\"Using simplified fallback for '{yaml_keys_path or env_var_key}'. This may lead to incorrect config loading.\")\n",
        "        val_to_use = os.getenv(env_var_key, default_val) if env_var_key else default_val\n",
        "        if expected_type is not None and val_to_use is not None:\n",
        "            try:\n",
        "                if expected_type is bool and isinstance(val_to_use, str):\n",
        "                    return val_to_use.lower() in ('true', '1', 't', 'yes', 'y')\n",
        "                return expected_type(val_to_use)\n",
        "            except (ValueError, TypeError):\n",
        "                logger.error(f\"Fallback get_config_value: Failed to cast '{val_to_use}' to {expected_type}. Returning default: {default_val}\")\n",
        "                return default_val\n",
        "        return val_to_use\n",
        "    logger.warning(\"Using a placeholder for 'get_config_value' in Cell 2. Config priority might not be as intended.\")\n",
        "\n",
        "# BASE_PROJECT_PATH (from Cell 1, derived from Cell 0 or ENV)\n",
        "if 'BASE_PROJECT_PATH' not in globals():\n",
        "    BASE_PROJECT_PATH = os.getcwd() # Fallback to current working directory\n",
        "    logger.critical(f\"CRITICAL: 'BASE_PROJECT_PATH' (from Cell 1) not found. Defaulting to CWD: {BASE_PROJECT_PATH}\")\n",
        "\n",
        "# Constants from Cell 0 (ensure these are available or have fallbacks)\n",
        "if 'MARKET_TIMEZONE_STR' not in globals(): MARKET_TIMEZONE_STR = \"Asia/Kolkata\"; logger.critical(\"CRITICAL: MARKET_TIMEZONE_STR (Cell 0) not found. Using default.\")\n",
        "if 'UPSTOX_HISTORY_INTERVAL_MAP' not in globals(): UPSTOX_HISTORY_INTERVAL_MAP = {\"1minute\": \"1minute\", \"day\": \"day\"}; logger.critical(\"CRITICAL: UPSTOX_HISTORY_INTERVAL_MAP (Cell 0) not found. Using minimal default.\")\n",
        "if 'UPSTOX_DATE_FORMAT' not in globals(): UPSTOX_DATE_FORMAT = \"%Y-%m-%d\"; logger.critical(\"CRITICAL: UPSTOX_DATE_FORMAT (Cell 0) not found. Using default.\")\n",
        "if 'CLASS_LABELS' not in globals(): CLASS_LABELS = {0: 'BUY', 1: 'HOLD', 2: 'SELL'}; logger.critical(\"CRITICAL: CLASS_LABELS (Cell 0) not found. Using default.\")\n",
        "\n",
        "# SDK related variables (availability checked in Cell 1)\n",
        "if 'UPSTOX_SDK_AVAILABLE' not in globals(): UPSTOX_SDK_AVAILABLE = False\n",
        "if 'upstox_client' not in globals(): upstox_client = None\n",
        "if 'UpstoxApiException' not in globals(): UpstoxApiException = Exception\n",
        "\n",
        "# --- Helper function for parsing integer lists from config ---\n",
        "def _parse_int_list_from_config(raw_val: Any, default_list: List[int], param_name_for_log: str = \"indicator periods\") -> List[int]:\n",
        "    \"\"\"\n",
        "    Parses a raw configuration value (either a list or a comma-separated string)\n",
        "    into a list of integers. Uses the provided default_list if parsing fails or raw_val is empty/None.\n",
        "    \"\"\"\n",
        "    parsed_list: List[int] = []\n",
        "    if isinstance(raw_val, list):\n",
        "        for item in raw_val:\n",
        "            try:\n",
        "                # Ensure items are converted to int, even if they are numeric strings in a list\n",
        "                parsed_list.append(int(item))\n",
        "            except (ValueError, TypeError):\n",
        "                logger.warning(f\"Invalid item '{item}' in list for {param_name_for_log}, skipping.\")\n",
        "    elif isinstance(raw_val, str):\n",
        "        items_str = raw_val.split(',')\n",
        "        for item_str in items_str:\n",
        "            item_stripped = item_str.strip()\n",
        "            if item_stripped: # Process only non-empty strings\n",
        "                try:\n",
        "                    parsed_list.append(int(item_stripped))\n",
        "                except ValueError:\n",
        "                    logger.warning(f\"Invalid integer string '{item_stripped}' in comma-separated list for {param_name_for_log}, skipping.\")\n",
        "    elif isinstance(raw_val, (int, float)): # Handle single number case\n",
        "        try:\n",
        "            parsed_list.append(int(raw_val))\n",
        "        except (ValueError, TypeError):\n",
        "             logger.warning(f\"Invalid single number item '{raw_val}' for {param_name_for_log}, skipping.\")\n",
        "\n",
        "\n",
        "    return parsed_list if parsed_list else default_list\n",
        "\n",
        "def _validate_configured_symbols(\n",
        "    symbols_to_check: List[str],\n",
        "    instrument_key_map: Dict[str, str],\n",
        "    logger_instance: Any\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Validates that each symbol in a list has a valid, configured instrument key.\n",
        "\n",
        "    Args:\n",
        "        symbols_to_check: The list of symbol names from the configuration.\n",
        "        instrument_key_map: The dictionary mapping symbols to their instrument keys.\n",
        "        logger_instance: The logger object for logging critical messages.\n",
        "\n",
        "    Returns:\n",
        "        A new list containing only the symbols that have a valid instrument key.\n",
        "    \"\"\"\n",
        "    validated_list: List[str] = []\n",
        "    for symbol_name in symbols_to_check:\n",
        "        key = instrument_key_map.get(symbol_name.upper())\n",
        "        # A key is considered valid if it exists, is not None, and is not a placeholder.\n",
        "        if key and \"INVALID_KEY_FOR_\" not in str(key).upper() and \"PLEASE_CONFIGURE\" not in str(key).upper():\n",
        "            validated_list.append(symbol_name)\n",
        "        else:\n",
        "            logger_instance.critical(\n",
        "                f\"CRITICAL: Symbol '{symbol_name}' is excluded from this session due to a \"\n",
        "                f\"missing or invalid instrument key in the configuration.\"\n",
        "            )\n",
        "    return validated_list\n",
        "\n",
        "\n",
        "# --- I. Directory Paths ---\n",
        "HISTORICAL_DATA_DIR: str = os.path.join(BASE_PROJECT_PATH, get_config_value(['directory_paths', 'historical_data_dir'], 'HISTORICAL_DATA_DIR_ENV', 'data_historical', str))\n",
        "MODELS_ARTEFACTS_DIR: str = os.path.join(BASE_PROJECT_PATH, get_config_value(['directory_paths', 'models_artefacts_dir'], 'MODELS_ARTEFACTS_DIR_ENV', 'models', str))\n",
        "TUNING_RESULTS_DIR: str = os.path.join(BASE_PROJECT_PATH, get_config_value(['directory_paths', 'tuning_results_dir'], 'TUNING_RESULTS_DIR_ENV', 'results_tuning', str))\n",
        "OTHER_FILES_DIR: str = os.path.join(BASE_PROJECT_PATH, get_config_value(['directory_paths', 'other_files_dir'], 'OTHER_FILES_DIR_ENV', 'other_files', str))\n",
        "\n",
        "# --- Corrected Directory Creation Loop ---\n",
        "for path_to_ensure in [HISTORICAL_DATA_DIR, MODELS_ARTEFACTS_DIR, TUNING_RESULTS_DIR, OTHER_FILES_DIR]:\n",
        "    try:\n",
        "        os.makedirs(path_to_ensure, exist_ok=True)\n",
        "    except OSError as e:\n",
        "        logger.error(f\"Could not create directory {path_to_ensure}: {e}. This may cause issues later.\")\n",
        "\n",
        "# --- II. API Keys and Tokens ---\n",
        "UPSTOX_API_KEY: Optional[str] = os.getenv('UPSTOX_API_KEY')\n",
        "UPSTOX_API_SECRET: Optional[str] = os.getenv('UPSTOX_API_SECRET')\n",
        "UPSTOX_REDIRECT_URI: Optional[str] = os.getenv('UPSTOX_REDIRECT_URI')\n",
        "if not all([UPSTOX_API_KEY, UPSTOX_API_SECRET, UPSTOX_REDIRECT_URI]):\n",
        "    logger.warning(\"One or more Upstox API credentials (KEY, SECRET, REDIRECT_URI) are missing from environment. Authentication will fail.\")\n",
        "\n",
        "UPSTOX_ACCESS_TOKEN_FILENAME: str = get_config_value(['upstox', 'access_token_file'], 'UPSTOX_ACCESS_TOKEN_FILENAME_ENV', \"upstox_access_token.json\", str)\n",
        "UPSTOX_ACCESS_TOKEN_FILE_PATH: str = os.path.join(OTHER_FILES_DIR, UPSTOX_ACCESS_TOKEN_FILENAME)\n",
        "UPSTOX_ACCESS_TOKEN_HARDCODED: Optional[str] = get_config_value(['upstox', 'access_token_hardcoded'], 'UPSTOX_ACCESS_TOKEN_HARDCODED_ENV', None, str)\n",
        "\n",
        "TELEGRAM_BOT_TOKEN: Optional[str] = os.getenv('TELEGRAM_BOT_TOKEN')\n",
        "TELEGRAM_CHAT_ID: Optional[str] = os.getenv('TELEGRAM_CHAT_ID')\n",
        "if not TELEGRAM_BOT_TOKEN: logger.warning(\"TELEGRAM_BOT_TOKEN not found. Telegram notifications will be disabled.\")\n",
        "if not TELEGRAM_CHAT_ID: logger.warning(\"TELEGRAM_CHAT_ID not found. Telegram notifications to the primary chat ID will be disabled.\")\n",
        "\n",
        "# --- III. Trading Parameters ---\n",
        "INITIAL_SYMBOLS_DEFAULT: List[str] = [\"IRFC\",\"IRB\"]\n",
        "SYMBOLS_LIST_RAW: Any = get_config_value(['trading_parameters', 'symbols'], 'SYMBOLS_LIST_ENV', INITIAL_SYMBOLS_DEFAULT)\n",
        "SYMBOLS_LIST: List[str] = []\n",
        "if isinstance(SYMBOLS_LIST_RAW, str): SYMBOLS_LIST = [s.strip().upper() for s in SYMBOLS_LIST_RAW.split(',') if s.strip()]\n",
        "elif isinstance(SYMBOLS_LIST_RAW, list): SYMBOLS_LIST = [str(s).strip().upper() for s in SYMBOLS_LIST_RAW if str(s).strip()]\n",
        "else: logger.error(f\"SYMBOLS_LIST invalid type: {type(SYMBOLS_LIST_RAW)}. Using default.\"); SYMBOLS_LIST = INITIAL_SYMBOLS_DEFAULT\n",
        "if not SYMBOLS_LIST: logger.critical(\"CRITICAL: SYMBOLS_LIST is empty. Using default.\"); SYMBOLS_LIST = INITIAL_SYMBOLS_DEFAULT\n",
        "\n",
        "_upstox_instrument_keys_default_map: Dict[str, str] = {s.upper(): f\"PLEASE_CONFIGURE_KEY_FOR_{s.upper()}\" for s in SYMBOLS_LIST}\n",
        "upstox_instrument_keys_config_val: Any = get_config_value(['trading_parameters', 'upstox_instrument_keys'], 'UPSTOX_INSTRUMENT_KEYS_JSON_ENV', _upstox_instrument_keys_default_map)\n",
        "UPSTOX_INSTRUMENT_KEYS: Dict[str, str] = {}\n",
        "if isinstance(upstox_instrument_keys_config_val, str):\n",
        "    try: loaded_keys = json.loads(upstox_instrument_keys_config_val)\n",
        "    except json.JSONDecodeError: logger.error(\"Could not parse UPSTOX_INSTRUMENT_KEYS_JSON_ENV. Using defaults.\"); loaded_keys = _upstox_instrument_keys_default_map\n",
        "elif isinstance(upstox_instrument_keys_config_val, dict): loaded_keys = upstox_instrument_keys_config_val\n",
        "else: logger.error(f\"UPSTOX_INSTRUMENT_KEYS invalid type: {type(upstox_instrument_keys_config_val)}. Using defaults.\"); loaded_keys = _upstox_instrument_keys_default_map\n",
        "if not isinstance(loaded_keys, dict): logger.error(\"Loaded instrument keys not a dict. Using defaults.\"); loaded_keys = _upstox_instrument_keys_default_map\n",
        "\n",
        "UPSTOX_INSTRUMENT_KEYS = {str(k).strip().upper(): str(v).strip() for k, v in loaded_keys.items()}\n",
        "\n",
        "# --- Symbol Validation ---\n",
        "VALIDATED_SYMBOLS_LIST = _validate_configured_symbols(\n",
        "    symbols_to_check=SYMBOLS_LIST,\n",
        "    instrument_key_map=UPSTOX_INSTRUMENT_KEYS,\n",
        "    logger_instance=logger\n",
        ")\n",
        "logger.info(\n",
        "    f\"Initial symbols from config: {len(SYMBOLS_LIST)}. \"\n",
        "    f\"Validated and active symbols for session: {len(VALIDATED_SYMBOLS_LIST)} -> {', '.join(VALIDATED_SYMBOLS_LIST)}\"\n",
        ")\n",
        "\n",
        "TARGET_INTERVAL: str = get_config_value(['trading_parameters', 'target_interval'], 'TARGET_INTERVAL_ENV', \"1minute\", str).lower()\n",
        "if TARGET_INTERVAL not in UPSTOX_HISTORY_INTERVAL_MAP: # type: ignore\n",
        "    logger.critical(f\"CRITICAL: TARGET_INTERVAL '{TARGET_INTERVAL}' invalid. Defaulting to '1minute'.\")\n",
        "    TARGET_INTERVAL = \"1minute\"\n",
        "\n",
        "# Defines the total window of TRADING days to maintain in historical data in the database.\n",
        "HISTORICAL_DATA_LOOKBACK_DAYS: int = get_config_value(\n",
        "    ['trading_parameters', 'historical_data_lookback_days'],\n",
        "    'HISTORICAL_DATA_LOOKBACK_DAYS_ENV',\n",
        "    880,\n",
        "    int\n",
        ")\n",
        "\n",
        "# Defines how many RECENT CALENDAR days of data to fetch from API to update/fill gaps.\n",
        "RECENT_DATA_API_FETCH_DAYS: int = get_config_value(\n",
        "    ['trading_parameters', 'recent_data_api_fetch_days'],\n",
        "    'RECENT_DATA_API_FETCH_DAYS_ENV',\n",
        "    30,\n",
        "    int\n",
        ")\n",
        "# --- IV. Model Parameters ---\n",
        "LOOKBACK_WINDOW: int = get_config_value(['model_params', 'lookback_window'], 'LOOKBACK_WINDOW_ENV', 60, int)\n",
        "CLASSIFICATION_PRICE_CHANGE_THRESHOLD: float = get_config_value(['model_params', 'classification_price_change_threshold'], 'CLASSIFICATION_PRICE_CHANGE_THRESHOLD_ENV', 0.0020, float)\n",
        "CLASSIFICATION_LOOKAHEAD_PERIODS: int = get_config_value(['model_params', 'classification_lookahead_periods'], 'CLASSIFICATION_LOOKAHEAD_PERIODS_ENV', 5, int)\n",
        "\n",
        "# --- V. Technical Indicator Parameters ---\n",
        "SMA_PERIODS: List[int] = _parse_int_list_from_config(get_config_value(['indicator_params','sma_periods'], 'SMA_PERIODS_CSV_ENV', [10,20,50]), [10,20,50], \"SMA\")\n",
        "EMA_PERIODS: List[int] = _parse_int_list_from_config(get_config_value(['indicator_params','ema_periods'], 'EMA_PERIODS_CSV_ENV', [10,20,50]), [10,20,50], \"EMA\")\n",
        "RSI_PERIOD: int = get_config_value(['indicator_params','rsi_period'], 'RSI_PERIOD_ENV', 14, int)\n",
        "MACD_FAST: int = get_config_value(['indicator_params','macd_fast'], 'MACD_FAST_ENV', 12, int)\n",
        "MACD_SLOW: int = get_config_value(['indicator_params','macd_slow'], 'MACD_SLOW_ENV', 26, int)\n",
        "MACD_SIGNAL: int = get_config_value(['indicator_params','macd_signal'], 'MACD_SIGNAL_ENV', 9, int)\n",
        "ATR_PERIOD: int = get_config_value(['indicator_params','atr_period'], 'ATR_PERIOD_ENV', 14, int)\n",
        "BB_WINDOW: int = get_config_value(['indicator_params','bb_window'], 'BB_WINDOW_ENV', 20, int)\n",
        "BB_NUM_STD: float = get_config_value(['indicator_params','bb_num_std'], 'BB_NUM_STD_ENV', 2.0, float)\n",
        "AVG_DAILY_RANGE_PERIOD: int = get_config_value(['indicator_params','avg_daily_range_period'], 'AVG_DAILY_RANGE_PERIOD_ENV', 10, int)\n",
        "\n",
        "# --- VI. Pattern Detection Parameters ---\n",
        "OB_LOOKBACK: int = get_config_value(['pattern_params','ob_lookback'],'OB_LOOKBACK_ENV', 3, int)\n",
        "OB_THRESH_MULT: float = get_config_value(['pattern_params','ob_thresh_mult'],'OB_THRESH_MULT_ENV', 1.0, float)\n",
        "OB_STRICT_REFINE: bool = get_config_value(['pattern_params','ob_strict_refine'],'OB_STRICT_REFINE_ENV', True, bool)\n",
        "ENGULFING_INC_DOJI: bool = get_config_value(['pattern_params','engulfing_inc_doji'],'ENGULFING_INC_DOJI_ENV', True, bool)\n",
        "LS_LOOKBACK: int = get_config_value(['pattern_params','ls_lookback'],'LS_LOOKBACK_ENV', 5, int)\n",
        "LS_WICK_RATIO: float = get_config_value(['pattern_params','ls_wick_ratio'],'LS_WICK_RATIO_ENV', 0.7, float)\n",
        "LS_BODY_CLOSE_THRESH_RATIO: float = get_config_value(['pattern_params','ls_body_close_thresh_ratio'],'LS_BODY_CLOSE_THRESH_RATIO_ENV', 0.4, float)\n",
        "INST_LOOKBACK: int = get_config_value(['pattern_params','inst_lookback'],'INST_LOOKBACK_ENV', 10, int)\n",
        "INST_VOL_THRESH: float = get_config_value(['pattern_params','inst_vol_thresh'],'INST_VOL_THRESH_ENV', 1.5, float)\n",
        "INST_RANGE_MULT: float = get_config_value(['pattern_params','inst_range_mult'],'INST_RANGE_MULT_ENV', 1.0, float)\n",
        "INST_WICK_MAX_RATIO: float = get_config_value(['pattern_params','inst_wick_max_ratio'],'INST_WICK_MAX_RATIO_ENV', 0.25, float)\n",
        "MC_LOOKBACK: int = get_config_value(['pattern_params','mc_lookback'],'MC_LOOKBACK_ENV', 10, int)\n",
        "MC_VOL_THRESH: float = get_config_value(['pattern_params','mc_vol_thresh'],'MC_VOL_THRESH_ENV', 1.5, float)\n",
        "MC_RANGE_THRESH: float = get_config_value(['pattern_params','mc_range_thresh'],'MC_RANGE_THRESH_ENV', 1.0, float)\n",
        "MC_TREND_THRESH_ABS: float = get_config_value(['pattern_params','mc_trend_thresh_abs'],'MC_TREND_THRESH_ABS_ENV', 0.03, float)\n",
        "MS_LOOKBACK: int = get_config_value(['pattern_params','ms_lookback'],'MS_LOOKBACK_ENV', 10, int)\n",
        "MS_VOL_THRESH: float = get_config_value(['pattern_params','ms_vol_thresh'],'MS_VOL_THRESH_ENV', 1.2, float)\n",
        "MS_PRICE_CHG_THRESH_PCT: float = get_config_value(['pattern_params','ms_price_chg_thresh_pct'],'MS_PRICE_CHG_THRESH_PCT_ENV', 0.0005, float)\n",
        "\n",
        "# --- VII. Training Parameters ---\n",
        "TRAIN_RATIO: float = get_config_value(['training_params','train_ratio'],'TRAIN_RATIO_ENV', 0.8, float)\n",
        "TEST_RATIO: float = get_config_value(['training_params','test_ratio'],'TEST_RATIO_ENV', 0.2, float)\n",
        "WALK_FORWARD_VALIDATION_ENABLED: bool = get_config_value(['training_params','walk_forward_validation_enabled'],'WALK_FORWARD_VALIDATION_ENABLED_ENV', True, bool)\n",
        "N_SPLITS_WALK_FORWARD: int = get_config_value(['training_params','n_splits_walk_forward'],'N_SPLITS_WALK_FORWARD_ENV', 5, int)\n",
        "EPOCHS: int = get_config_value(['training_params','epochs'],'EPOCHS_ENV', 100, int)\n",
        "BATCH_SIZE: int = get_config_value(['training_params','batch_size'],'BATCH_SIZE_ENV', 32, int)\n",
        "INITIAL_LEARNING_RATE: float = get_config_value(['training_params','initial_learning_rate'],'INITIAL_LEARNING_RATE_ENV', 5e-5, float)\n",
        "WEIGHT_DECAY: float = get_config_value(['training_params','weight_decay'],'WEIGHT_DECAY_ENV', 1e-6, float)\n",
        "L2_REG_STRENGTH: float = get_config_value(['training_params','l2_reg_strength'],'L2_REG_STRENGTH_ENV', 1e-6, float)\n",
        "DROPOUT_RATE: float = get_config_value(['training_params','dropout_rate'],'DROPOUT_RATE_ENV', 0.3, float)\n",
        "ES_MONITOR: str = get_config_value(['training_params','es_monitor'],'ES_MONITOR_ENV','val_loss', str)\n",
        "ES_PATIENCE: int = get_config_value(['training_params','es_patience'],'ES_PATIENCE_ENV', 20, int)\n",
        "ES_RESTORE_BEST: bool = get_config_value(['training_params','es_restore_best'],'ES_RESTORE_BEST_ENV', True, bool)\n",
        "RLP_MONITOR: str = get_config_value(['training_params','rlp_monitor'],'RLP_MONITOR_ENV','val_loss', str)\n",
        "RLP_FACTOR: float = get_config_value(['training_params','rlp_factor'],'RLP_FACTOR_ENV', 0.2, float)\n",
        "RLP_PATIENCE: int = get_config_value(['training_params','rlp_patience'],'RLP_PATIENCE_ENV', 7, int)\n",
        "RLP_MIN_LR: float = get_config_value(['training_params','rlp_min_lr'],'RLP_MIN_LR_ENV', 1e-7, float)\n",
        "USE_LIVE_LOGS_FOR_TRAINING_AUGMENTATION: bool = get_config_value(['training_params', 'use_live_logs_for_augmentation'], 'USE_LIVE_LOGS_FOR_AUGMENTATION_ENV', True, bool)\n",
        "LIVE_LOG_AUGMENTATION_SAMPLE_WEIGHT: float = get_config_value(['training_params', 'live_log_augmentation_sample_weight'], 'LIVE_LOG_AUGMENTATION_SAMPLE_WEIGHT_ENV', 1.5, float)\n",
        "AUGMENTATION_LOSS_ATR_MULTIPLIER: float = get_config_value(['training_params', 'augmentation_loss_atr_multiplier'], 'AUGMENTATION_LOSS_ATR_MULTIPLIER', 0.5, float)\n",
        "\n",
        "# --- VIII. Keras Tuner Parameters ---\n",
        "KERAS_TUNER_ENABLED: bool = get_config_value(['tuner_params','keras_tuner_enabled'],'KERAS_TUNER_ENABLED_ENV', False, bool)\n",
        "TUNER_PROJECT_NAME_BASE: str = get_config_value(['tuner_params','tuner_project_name_base'],'TUNER_PROJECT_NAME_BASE_ENV','adaptive_trading_model_tuning', str)\n",
        "TUNER_MAX_TRIALS: int = get_config_value(['tuner_params','tuner_max_trials'],'TUNER_MAX_TRIALS_ENV', 20, int)\n",
        "TUNER_EXEC_PER_TRIAL: int = get_config_value(['tuner_params','tuner_exec_per_trial'],'TUNER_EXEC_PER_TRIAL_ENV', 1, int)\n",
        "TUNER_OBJECTIVE_METRIC: str = get_config_value(['tuner_params','tuner_objective_metric'],'TUNER_OBJECTIVE_METRIC_ENV','val_accuracy', str)\n",
        "\n",
        "# --- IX. Strategy Parameters ---\n",
        "MC_DROPOUT_SAMPLES: int = get_config_value(['strategy_params', 'mc_dropout_samples'], 'MC_DROPOUT_SAMPLES_ENV', 20, int)\n",
        "CONFIDENCE_THRESHOLD_TRADE: float = get_config_value(['strategy_params', 'confidence_threshold_trade'], 'CONFIDENCE_THRESHOLD_TRADE_ENV', 0.98, float)\n",
        "SL_ATR_MULTIPLIER_DEFAULT: float = get_config_value(['strategy_params','sl_atr_multiplier_default'],'SL_ATR_MULTIPLIER_DEFAULT_ENV', 0.75, float)\n",
        "TP_ATR_MULTIPLIER_DEFAULT: float = get_config_value(['strategy_params','tp_atr_multiplier_default'],'TP_ATR_MULTIPLIER_DEFAULT_ENV', 1.5, float)\n",
        "BACKTEST_TRANSACTION_COST_PCT: float = get_config_value(['strategy_params','backtest_transaction_cost_pct'],'BACKTEST_TRANSACTION_COST_PCT_ENV', 0.0007, float)\n",
        "MARGIN_UTILIZATION_PERCENT: float = get_config_value(['strategy_params', 'margin_utilization_percent'], 'MARGIN_UTILIZATION_PERCENT_ENV', 0.92, float)\n",
        "UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER: float = get_config_value(['strategy_params', 'upstox_intraday_leverage_multiplier'], 'UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER_ENV', 5.0, float)\n",
        "CAPITAL_THRESHOLD_FOR_MULTI_TRADE: float = get_config_value(['strategy_params', 'capital_threshold_for_multi_trade'], 'CAPITAL_THRESHOLD_FOR_MULTI_TRADE_ENV', 30000.0, float)\n",
        "CONSECUTIVE_LOSS_DAYS_HALT_THRESHOLD: int = get_config_value(['strategy_params', 'consecutive_loss_days_halt_threshold'], 'CONSECUTIVE_LOSS_DAYS_HALT_THRESHOLD_ENV', 3, int)\n",
        "MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG: int = get_config_value(['strategy_params', 'min_trades_for_strategy_adaptation_config'], 'MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG_ENV', 10, int)\n",
        "CAPITAL_ALLOCATION_MODE: str = get_config_value(['strategy_params', 'capital_allocation_mode'], 'CAPITAL_ALLOCATION_MODE_ENV', \"EQUAL\", str).upper()\n",
        "\n",
        "# --- X. Live Trading Parameters ---\n",
        "AUTO_ORDER_EXECUTION_ENABLED: bool = get_config_value(['live_trading_params', 'auto_order_execution_enabled'], 'AUTO_ORDER_EXECUTION_ENABLED_ENV', False, bool)\n",
        "DEFAULT_ORDER_QUANTITY: int = get_config_value(['live_trading_params', 'default_order_quantity'], 'DEFAULT_ORDER_QUANTITY_ENV', 1, int)\n",
        "LIVE_PROCESSING_INTERVAL_SECONDS: int = get_config_value(['live_trading_params', 'live_processing_interval_seconds'], 'LIVE_PROCESSING_INTERVAL_SECONDS_ENV', 10, int) # Adjusted default for 5s/10s data\n",
        "LIVE_MONITORING_INTERVAL_SECONDS: int = get_config_value(['live_trading_params', 'live_monitoring_interval_seconds'], 'LIVE_MONITORING_INTERVAL_SECONDS_ENV', 5, int)\n",
        "MIN_ENTRY_TIME_AFTER_OPEN_STR: str = get_config_value(['live_trading_params', 'min_entry_time_after_open'], 'MIN_ENTRY_TIME_AFTER_OPEN_ENV', \"09:17:00\", str)\n",
        "NO_NEW_ENTRY_AFTER_TIME_STR: str = get_config_value(['live_trading_params', 'no_new_entry_after_time'], 'NO_NEW_ENTRY_AFTER_TIME_ENV', \"15:00:00\", str)\n",
        "SQUARE_OFF_ALL_START_TIME_STR: str = get_config_value(['live_trading_params', 'square_off_all_start_time'], 'SQUARE_OFF_ALL_START_TIME_ENV', \"15:10:00\", str)\n",
        "SQUARE_OFF_ALL_END_TIME_STR: str = get_config_value(['live_trading_params', 'square_off_all_end_time'], 'SQUARE_OFF_ALL_END_TIME_ENV', \"15:15:00\", str)\n",
        "EXIT_ORDER_TYPE: str = get_config_value(['live_trading_params', 'exit_order_type'], 'EXIT_ORDER_TYPE_ENV', \"MARKET\", str).upper()\n",
        "UPSTOX_PRODUCT_TYPE: str = get_config_value(['live_trading_params', 'upstox_product_type'], 'UPSTOX_PRODUCT_TYPE_ENV', \"I\", str).upper()\n",
        "UPSTOX_ORDER_VALIDITY: str = get_config_value(['live_trading_params', 'upstox_order_validity'], 'UPSTOX_ORDER_VALIDITY_ENV', \"DAY\", str).upper()\n",
        "MAX_ORDER_RETRY_ATTEMPTS: int = get_config_value(['live_trading_params', 'max_order_retry_attempts'], 'MAX_ORDER_RETRY_ATTEMPTS_ENV', 3, int)\n",
        "USE_REALTIME_WEBSOCKET_FEED: bool = get_config_value(['live_trading_params', 'use_realtime_websocket_feed'], 'USE_REALTIME_WEBSOCKET_FEED_ENV', True, bool)\n",
        "MAX_DAILY_LOSS_FIXED_CONFIG: float = get_config_value(['live_trading_params', 'max_daily_loss_fixed'], 'MAX_DAILY_LOSS_FIXED_ENV', 400.0, float)\n",
        "MAX_DAILY_LOSS_MARGIN_THRESHOLD_CONFIG: float = get_config_value(['live_trading_params', 'max_daily_loss_margin_threshold'], 'MAX_DAILY_LOSS_MARGIN_THRESHOLD_ENV', 20000.0, float)\n",
        "MAX_DAILY_LOSS_MARGIN_PERCENTAGE_CONFIG: float = get_config_value(['live_trading_params', 'max_daily_loss_margin_percentage'], 'MAX_DAILY_LOSS_MARGIN_PERCENTAGE_ENV', 0.025, float)\n",
        "MAX_TRADES_PER_DAY_GLOBAL: int = get_config_value(['live_trading_params', 'max_trades_per_day_global'], 'MAX_TRADES_PER_DAY_GLOBAL_ENV', 10, int)\n",
        "MAX_TRADES_PER_SYMBOL_PER_DAY: int = get_config_value(['live_trading_params', 'max_trades_per_symbol_per_day'], 'MAX_TRADES_PER_SYMBOL_PER_DAY_ENV', 2, int)\n",
        "\n",
        "# --- XI. Market Hours and Timezone ---\n",
        "try:\n",
        "    NSE_TZ = pytz.timezone(MARKET_TIMEZONE_STR)\n",
        "except pytz.exceptions.UnknownTimeZoneError:\n",
        "    logger.critical(f\"CRITICAL: Unknown timezone: '{MARKET_TIMEZONE_STR}'. Defaulting to UTC.\")\n",
        "    NSE_TZ = pytz.utc\n",
        "MARKET_OPEN_TIME_STR: str = get_config_value(['market_hours','open_time'],\"MARKET_OPEN_TIME_ENV\",\"09:15:00\", str)\n",
        "MARKET_CLOSE_TIME_STR: str = get_config_value(['market_hours','close_time'],\"MARKET_CLOSE_TIME_ENV\",\"15:30:00\", str)\n",
        "\n",
        "# --- XII. File Path Naming Templates ---\n",
        "MODEL_BASE_FILENAME: str = get_config_value(['file_paths','model_base_name'],'MODEL_BASE_FILENAME_ENV','trading_model', str)\n",
        "TUNER_PROJECT_NAME_TEMPLATE: str = f\"{TUNER_PROJECT_NAME_BASE}_\" + \"{symbol}\"\n",
        "\n",
        "# --- XIII. Ensemble Parameters ---\n",
        "ENSEMBLE_ENABLED: bool = get_config_value(['ensemble_params','ensemble_enabled'],'ENSEMBLE_ENABLED_ENV', False, bool)\n",
        "N_ENSEMBLE_MODELS_CONFIG: int = get_config_value(['ensemble_params','n_ensemble_models'],'N_ENSEMBLE_MODELS_ENV', 3, int)\n",
        "\n",
        "# --- XIV. Global State Variables ---\n",
        "# These are initialized as empty or default and will be populated by other cells or during runtime.\n",
        "upstox_api_client_global: Optional[Any] = None\n",
        "telegram_bot_global: Optional[Any] = None\n",
        "telegram_app_global: Optional[Any] = None\n",
        "telegram_initialized_successfully: bool = False\n",
        "\n",
        "data_store_by_symbol: Dict[str, Dict[str, Any]] = {}\n",
        "trained_models_by_symbol: Dict[str, List[Dict[str, Any]]] = {}\n",
        "best_hyperparameters_by_symbol: Dict[str, Any] = {}\n",
        "SYMBOLS_REQUIRING_RETRAINING: List[str] = []\n",
        "POLL_INTERVAL_SECONDS: int = 5\n",
        "\n",
        "live_states_by_symbol: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "strategy_performance_insights_by_symbol: Dict[str, Dict[str, Any]] = {}\n",
        "tick_aggregators_by_symbol: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "TICK_QUEUE_MAX_LEN: int = get_config_value(['live_trading_params','tick_queue_max_len'],'TICK_QUEUE_MAX_LEN_ENV', 50000, int)\n",
        "tick_queue_global: collections.deque = collections.deque(maxlen=TICK_QUEUE_MAX_LEN)\n",
        "tick_queue_lock_global: threading.Lock = threading.Lock()\n",
        "websocket_thread_global: Optional[threading.Thread] = None\n",
        "stop_websocket_flag_global: threading.Event = threading.Event()\n",
        "upstox_market_streamer_global: Optional[Any] = None\n",
        "\n",
        "portfolio_available_margin: float = 0.0\n",
        "live_trading_mode: str = \"NOT_SET\"\n",
        "capital_per_symbol_allowance: Dict[str, float] = {}\n",
        "global_trade_active_flag: bool = False\n",
        "portfolio_daily_pnl_achieved: float = 0.0\n",
        "portfolio_trades_today_count: int = 0\n",
        "calculated_max_portfolio_trades_today: int = 0\n",
        "is_trading_halted_for_day_global: bool = False\n",
        "calculated_max_daily_loss_global: float = MAX_DAILY_LOSS_FIXED_CONFIG\n",
        "can_place_new_order_today_global: bool = True\n",
        "last_daily_reset_date_global: Optional[datetime_date] = None\n",
        "MARKET_HOLIDAYS: List[datetime_date] = []\n",
        "selected_symbols_for_session: List[str] = []\n",
        "\n",
        "# --- XV. Upstox API Helper Functions (get_market_holidays_upstox) ---\n",
        "def get_market_holidays_upstox(year_to_fetch: Optional[int] = None) -> List[datetime_date]:\n",
        "    \"\"\"\n",
        "    Fetches market holidays from Upstox. Filters for the specified year if provided.\n",
        "    Updates the global MARKET_HOLIDAYS list if new, valid holidays for the target year are found.\n",
        "    Relies on `upstox_api_client_global` being initialized.\n",
        "    \"\"\"\n",
        "    global MARKET_HOLIDAYS, upstox_api_client_global, NSE_TZ, logger, UpstoxApiException, UPSTOX_SDK_AVAILABLE, UPSTOX_DATE_FORMAT, upstox_client\n",
        "\n",
        "    if not UPSTOX_SDK_AVAILABLE: logger.error(\"SDK not available. Cannot fetch holidays.\"); return MARKET_HOLIDAYS\n",
        "    if not upstox_api_client_global: logger.warning(\"API client not init. Cannot fetch holidays now.\"); return MARKET_HOLIDAYS\n",
        "    if upstox_client is None or not hasattr(upstox_client, 'MarketHolidaysAndTimingsApi'): logger.error(\"MarketHolidaysApi not found in SDK.\"); return MARKET_HOLIDAYS\n",
        "\n",
        "    current_year_holidays_fetched: List[datetime_date] = []\n",
        "    try:\n",
        "        holidays_api_instance = upstox_client.MarketHolidaysAndTimingsApi(upstox_api_client_global)\n",
        "    except Exception as e_init_api: logger.error(f\"Error init MarketHolidaysApi: {e_init_api}\", exc_info=True); return MARKET_HOLIDAYS\n",
        "\n",
        "    target_year = year_to_fetch if year_to_fetch is not None else datetime.now(NSE_TZ).year\n",
        "    logger.info(f\"Fetching market holidays from Upstox (filter year: {target_year}).\")\n",
        "\n",
        "    try:\n",
        "        api_response = holidays_api_instance.get_holidays()\n",
        "        if hasattr(api_response, 'status') and str(api_response.status).lower() == 'success' and hasattr(api_response, 'data') and api_response.data:\n",
        "            holiday_data_list = api_response.data\n",
        "            if not isinstance(holiday_data_list, list): logger.warning(f\"Holiday data not a list: {type(holiday_data_list)}\"); holiday_data_list = []\n",
        "            for holiday_obj in holiday_data_list:\n",
        "                if hasattr(holiday_obj, 'date') and isinstance(holiday_obj.date, str):\n",
        "                    try:\n",
        "                        parsed_date = datetime.strptime(holiday_obj.date, UPSTOX_DATE_FORMAT).date()\n",
        "                        if parsed_date.year == target_year: current_year_holidays_fetched.append(parsed_date)\n",
        "                    except (ValueError, TypeError) as e_parse: logger.warning(f\"Could not parse holiday date '{holiday_obj.date}': {e_parse}\")\n",
        "            if current_year_holidays_fetched:\n",
        "                existing_other_years = [h for h in MARKET_HOLIDAYS if h.year != target_year]\n",
        "                updated_holidays_for_year = sorted(list(set(current_year_holidays_fetched)))\n",
        "                MARKET_HOLIDAYS = sorted(list(set(existing_other_years + updated_holidays_for_year)))\n",
        "                logger.info(f\"Fetched/updated {len(updated_holidays_for_year)} holidays for {target_year}. Total known: {len(MARKET_HOLIDAYS)}.\")\n",
        "            elif not any(h.year == target_year for h in MARKET_HOLIDAYS): logger.info(f\"No holidays for {target_year} in API response.\")\n",
        "        else: logger.warning(f\"Failed to fetch holidays. Status: {getattr(api_response, 'status', 'N/A')}, Msg: {getattr(api_response, 'message', 'N/A')}\")\n",
        "    except UpstoxApiException as e_sdk_ex: # type: ignore\n",
        "        logger.error(f\"UpstoxApiException fetching holidays: Status {e_sdk_ex.status} - {e_sdk_ex.reason}\", exc_info=False)\n",
        "    except Exception as e_general: logger.error(f\"General error fetching holidays: {e_general}\", exc_info=True)\n",
        "    return MARKET_HOLIDAYS\n",
        "\n",
        "logger.info(\"Cell 2: Configuration Definitions and Global Variables - Complete.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n--- Cell 2 Standalone Test ---\")\n",
        "    print(f\"Historical Data Dir: {HISTORICAL_DATA_DIR}\")\n",
        "    print(f\"Symbols List (from config): {SYMBOLS_LIST}\")\n",
        "    # Display the new validated list\n",
        "    print(f\"Validated Symbols List: {VALIDATED_SYMBOLS_LIST}\")\n",
        "    print(f\"Target Interval: {TARGET_INTERVAL}\")\n",
        "    print(f\"Lookback Window: {LOOKBACK_WINDOW}\")\n",
        "    print(f\"Auto Order Execution Enabled: {AUTO_ORDER_EXECUTION_ENABLED}\")\n",
        "    print(f\"Max Daily Loss Fixed: {MAX_DAILY_LOSS_FIXED_CONFIG}, Margin Threshold: {MAX_DAILY_LOSS_MARGIN_THRESHOLD_CONFIG}, Margin Percent: {MAX_DAILY_LOSS_MARGIN_PERCENTAGE_CONFIG*100}%\")\n",
        "    print(f\"Capital Threshold for Multi Trade: {CAPITAL_THRESHOLD_FOR_MULTI_TRADE}\")\n",
        "    print(f\"Max Trades Per Symbol: {MAX_TRADES_PER_SYMBOL_PER_DAY}, Global Max Trades: {MAX_TRADES_PER_DAY_GLOBAL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5hhZ6mNtkyD",
        "outputId": "af124957-62d5-4c04-e023-e8e4976aac50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 3: Helper Functions (Vectorized Pattern Detection)\n",
            "2025-06-17 17:49:05 - TradingBotLogger - INFO - [<ipython-input-5-412161903>.<cell line: 0>:346] - Cell 3: Vectorized helper functions (Pattern Detection, SMC, Price Action) defined and refined.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 3: Helper Functions (Pattern Detection, SMC, Price Action) ---\n",
        "\n",
        "print(\"\\nInitializing Cell 3: Helper Functions (Vectorized Pattern Detection)\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging # Ensure logger is available\n",
        "from typing import Any, Optional, List # For type hinting\n",
        "\n",
        "# --- Ensure necessary variables from Cell 1 and Cell 2 are available ---\n",
        "# Logger (from Cell 1)\n",
        "if 'logger' not in globals():\n",
        "    import sys as pysys # Use alias\n",
        "    logger = logging.getLogger(\"TradingBotLogger_C3_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c3 = logging.StreamHandler(pysys.stdout)\n",
        "        _ch_c3.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - C3_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c3)\n",
        "        logger.setLevel(logging.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 3.\")\n",
        "\n",
        "# Define default values for pattern parameters if they are not found in globals.\n",
        "PATTERN_PARAMS_DEFAULTS_C3: dict[str, Any] = {\n",
        "    'OB_LOOKBACK': 3, 'OB_THRESH_MULT': 1.0, 'OB_STRICT_REFINE': True,\n",
        "    'ENGULFING_INC_DOJI': True,\n",
        "    'LS_LOOKBACK': 5, 'LS_WICK_RATIO': 0.7, 'LS_BODY_CLOSE_THRESH_RATIO': 0.4,\n",
        "    'INST_LOOKBACK': 10, 'INST_VOL_THRESH': 1.5, 'INST_RANGE_MULT': 1.0, 'INST_WICK_MAX_RATIO': 0.25,\n",
        "    'MC_LOOKBACK': 10, 'MC_VOL_THRESH': 1.5, 'MC_RANGE_THRESH': 1.0, 'MC_TREND_THRESH_ABS': 0.03,\n",
        "    'MS_LOOKBACK': 10, 'MS_VOL_THRESH': 1.2, 'MS_PRICE_CHG_THRESH_PCT': 0.0005\n",
        "}\n",
        "for param_name_c3, default_value_c3 in PATTERN_PARAMS_DEFAULTS_C3.items():\n",
        "    if param_name_c3 not in globals():\n",
        "        globals()[param_name_c3] = default_value_c3\n",
        "\n",
        "# Helper function to determine minimum periods for rolling calculations\n",
        "def _get_min_periods_c3(lookback: int, factor: float = 0.8) -> int:\n",
        "    \"\"\"\n",
        "    Calculates a minimum number of periods for rolling window operations.\n",
        "    Ensures that the minimum periods is at least 1.\n",
        "    \"\"\"\n",
        "    if not isinstance(lookback, int) or lookback <= 0:\n",
        "        return 1\n",
        "    calculated_min = int(lookback * factor)\n",
        "    return max(1, calculated_min)\n",
        "\n",
        "\n",
        "def find_potential_order_blocks(\n",
        "    df: pd.DataFrame,\n",
        "    lookback: Optional[int] = None,\n",
        "    thresh_mult: Optional[float] = None,\n",
        "    strict_refine: Optional[bool] = None,\n",
        "    copy_df: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies potential bullish and bearish order blocks (OB) using vectorized operations.\n",
        "    Adds 'Potential_Bullish_Ob' and 'Potential_Bearish_Ob' (boolean) columns.\n",
        "    \"\"\"\n",
        "    lookback_eff = lookback if lookback is not None else globals().get('OB_LOOKBACK', PATTERN_PARAMS_DEFAULTS_C3['OB_LOOKBACK'])\n",
        "    thresh_mult_eff = thresh_mult if thresh_mult is not None else globals().get('OB_THRESH_MULT', PATTERN_PARAMS_DEFAULTS_C3['OB_THRESH_MULT'])\n",
        "    strict_refine_eff = strict_refine if strict_refine is not None else globals().get('OB_STRICT_REFINE', PATTERN_PARAMS_DEFAULTS_C3['OB_STRICT_REFINE'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"OrderBlock: Missing OHLC (TitleCase). Returning original DataFrame.\")\n",
        "        data['Potential_Bullish_Ob'] = False; data['Potential_Bearish_Ob'] = False; return data\n",
        "    if len(data) < lookback_eff * 2 + 1:\n",
        "        data['Potential_Bullish_Ob'] = False; data['Potential_Bearish_Ob'] = False; return data\n",
        "\n",
        "    min_p_ob = _get_min_periods_c3(lookback_eff)\n",
        "    data['temp_ob_candle_range'] = data['High'] - data['Low']\n",
        "    data['temp_ob_avg_candle_range'] = data['temp_ob_candle_range'].shift(1).rolling(window=lookback_eff, min_periods=min_p_ob).mean()\n",
        "\n",
        "    # Vectorized forward-looking rolling window using the reverse-roll-reverse technique\n",
        "    future_max_highs = data['High'].iloc[::-1].shift(1).rolling(window=lookback_eff, min_periods=1).max().iloc[::-1]\n",
        "    future_min_lows = data['Low'].iloc[::-1].shift(1).rolling(window=lookback_eff, min_periods=1).min().iloc[::-1]\n",
        "    threshold_move = data['temp_ob_avg_candle_range'] * thresh_mult_eff\n",
        "\n",
        "    # --- Bullish Order Block Conditions ---\n",
        "    is_bearish_candle = data['Close'] < data['Open']\n",
        "    breaks_structure_up = future_max_highs > data['High']\n",
        "    is_strong_move_up = (future_max_highs - data['Low']) > threshold_move\n",
        "    bullish_ob_base = is_bearish_candle & breaks_structure_up & is_strong_move_up\n",
        "\n",
        "    if strict_refine_eff:\n",
        "        strict_low_not_broken = future_min_lows >= (data['Low'] - (data['temp_ob_avg_candle_range'] * 0.1))\n",
        "        data['Potential_Bullish_Ob'] = bullish_ob_base & strict_low_not_broken\n",
        "    else:\n",
        "        data['Potential_Bullish_Ob'] = bullish_ob_base\n",
        "\n",
        "    # --- Bearish Order Block Conditions ---\n",
        "    is_bullish_candle = data['Close'] > data['Open']\n",
        "    breaks_structure_down = future_min_lows < data['Low']\n",
        "    is_strong_move_down = (data['High'] - future_min_lows) > threshold_move\n",
        "    bearish_ob_base = is_bullish_candle & breaks_structure_down & is_strong_move_down\n",
        "\n",
        "    if strict_refine_eff:\n",
        "        strict_high_not_broken = future_max_highs <= (data['High'] + (data['temp_ob_avg_candle_range'] * 0.1))\n",
        "        data['Potential_Bearish_Ob'] = bearish_ob_base & strict_high_not_broken\n",
        "    else:\n",
        "        data['Potential_Bearish_Ob'] = bearish_ob_base\n",
        "\n",
        "    # Cleanup and fill NaNs\n",
        "    data['Potential_Bullish_Ob'] = data['Potential_Bullish_Ob'].fillna(False)\n",
        "    # - Corrected a copy-paste error. Was 'Potential_Bullish_Ob' twice.\n",
        "    data['Potential_Bearish_Ob'] = data['Potential_Bearish_Ob'].fillna(False)\n",
        "    return data.drop(columns=['temp_ob_candle_range', 'temp_ob_avg_candle_range'], errors='ignore')\n",
        "\n",
        "def find_engulfing_patterns(\n",
        "    df: pd.DataFrame,\n",
        "    copy_df: bool = True,\n",
        "    include_doji_in_prior: Optional[bool] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies bullish and bearish engulfing candlestick patterns using vectorized operations.\n",
        "    Adds 'Bullish_Engulfing' and 'Bearish_Engulfing' (boolean) columns.\n",
        "    \"\"\"\n",
        "    include_doji_eff = include_doji_in_prior if include_doji_in_prior is not None else globals().get('ENGULFING_INC_DOJI', PATTERN_PARAMS_DEFAULTS_C3['ENGULFING_INC_DOJI'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"Engulfing: Missing OHLC (TitleCase). Returning original DataFrame.\")\n",
        "        data['Bullish_Engulfing'] = False; data['Bearish_Engulfing'] = False; return data\n",
        "    if len(data) < 2:\n",
        "        data['Bullish_Engulfing'] = False; data['Bearish_Engulfing'] = False; return data\n",
        "\n",
        "    prev_o, prev_c = data['Open'].shift(1), data['Close'].shift(1)\n",
        "    prev_h, prev_l = data['High'].shift(1), data['Low'].shift(1)\n",
        "    curr_o, curr_c = data['Open'], data['Close']\n",
        "\n",
        "    prev_body = abs(prev_o - prev_c)\n",
        "    prev_range = prev_h - prev_l\n",
        "    is_prev_doji = (prev_body < prev_range * 0.15)\n",
        "\n",
        "    prev_is_eff_bearish = (prev_c < prev_o) | (include_doji_eff & is_prev_doji & (prev_c <= prev_o))\n",
        "    prev_is_eff_bullish = (prev_c > prev_o) | (include_doji_eff & is_prev_doji & (prev_c >= prev_o))\n",
        "    curr_is_bullish = curr_c > curr_o\n",
        "    curr_is_bearish = curr_c < curr_o\n",
        "\n",
        "    # Bullish Engulfing\n",
        "    data['Bullish_Engulfing'] = curr_is_bullish & prev_is_eff_bearish & (curr_c > prev_o) & (curr_o < prev_c)\n",
        "    # Bearish Engulfing\n",
        "    data['Bearish_Engulfing'] = curr_is_bearish & prev_is_eff_bullish & (curr_c < prev_o) & (curr_o > prev_c)\n",
        "\n",
        "    # - Replaced inplace=True with direct assignment to avoid FutureWarning.\n",
        "    data['Bullish_Engulfing'] = data['Bullish_Engulfing'].fillna(False)\n",
        "    data['Bearish_Engulfing'] = data['Bearish_Engulfing'].fillna(False)\n",
        "    return data\n",
        "\n",
        "def find_potential_liquidity_sweeps(\n",
        "    df: pd.DataFrame,\n",
        "    lookback: Optional[int] = None,\n",
        "    wick_ratio_thresh: Optional[float] = None,\n",
        "    body_close_thresh_ratio: Optional[float] = None,\n",
        "    copy_df: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies potential liquidity sweeps (stop hunts) using vectorized operations.\n",
        "    Adds 'Potential_Bearish_Sweep' and 'Potential_Bullish_Sweep' (boolean) columns.\n",
        "    \"\"\"\n",
        "    lookback_eff = lookback if lookback is not None else globals().get('LS_LOOKBACK', PATTERN_PARAMS_DEFAULTS_C3['LS_LOOKBACK'])\n",
        "    wick_ratio_eff = wick_ratio_thresh if wick_ratio_thresh is not None else globals().get('LS_WICK_RATIO', PATTERN_PARAMS_DEFAULTS_C3['LS_WICK_RATIO'])\n",
        "    body_close_eff = body_close_thresh_ratio if body_close_thresh_ratio is not None else globals().get('LS_BODY_CLOSE_THRESH_RATIO', PATTERN_PARAMS_DEFAULTS_C3['LS_BODY_CLOSE_THRESH_RATIO'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"LiquiditySweep: Missing OHLC (TitleCase). Returning original DataFrame.\")\n",
        "        data['Potential_Bearish_Sweep']=False; data['Potential_Bullish_Sweep']=False; return data\n",
        "    if len(data) < lookback_eff + 1:\n",
        "        data['Potential_Bearish_Sweep']=False; data['Potential_Bullish_Sweep']=False; return data\n",
        "\n",
        "    min_p_ls = _get_min_periods_c3(lookback_eff)\n",
        "    recent_high = data['High'].shift(1).rolling(window=lookback_eff, min_periods=min_p_ls).max()\n",
        "    recent_low = data['Low'].shift(1).rolling(window=lookback_eff, min_periods=min_p_ls).min()\n",
        "\n",
        "    curr_range = data['High'] - data['Low']\n",
        "    # - Replaced inplace=True with direct assignment to avoid potential warnings.\n",
        "    curr_range = curr_range.replace(0, 1e-9) # Avoid division by zero\n",
        "\n",
        "    # Bearish Sweep Conditions\n",
        "    upper_wick = data['High'] - np.maximum(data['Open'], data['Close'])\n",
        "    close_in_lower_body = data['Close'] < (data['Low'] + curr_range * body_close_eff)\n",
        "    bearish_sweep_cond = (data['High'] > recent_high) & ((upper_wick / curr_range) >= wick_ratio_eff) & close_in_lower_body\n",
        "    data['Potential_Bearish_Sweep'] = bearish_sweep_cond\n",
        "\n",
        "    # Bullish Sweep Conditions\n",
        "    lower_wick = np.minimum(data['Open'], data['Close']) - data['Low']\n",
        "    close_in_upper_body = data['Close'] > (data['High'] - curr_range * body_close_eff)\n",
        "    bullish_sweep_cond = (data['Low'] < recent_low) & ((lower_wick / curr_range) >= wick_ratio_eff) & close_in_upper_body\n",
        "    data['Potential_Bullish_Sweep'] = bullish_sweep_cond\n",
        "\n",
        "    # - Replaced inplace=True with direct assignment to avoid FutureWarning.\n",
        "    data['Potential_Bearish_Sweep'] = data['Potential_Bearish_Sweep'].fillna(False)\n",
        "    data['Potential_Bullish_Sweep'] = data['Potential_Bullish_Sweep'].fillna(False)\n",
        "    return data\n",
        "\n",
        "def find_institutional_trading_patterns(\n",
        "    df: pd.DataFrame,\n",
        "    lookback: Optional[int] = None,\n",
        "    vol_thresh_mult: Optional[float] = None,\n",
        "    range_thresh_mult: Optional[float] = None,\n",
        "    wick_to_body_max_ratio: Optional[float] = None,\n",
        "    copy_df: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies candles potentially indicative of institutional trading using vectorized operations.\n",
        "    Adds 'Inst_Buy_Signal' and 'Inst_Sell_Signal' (boolean) columns.\n",
        "    \"\"\"\n",
        "    lookback_eff = lookback if lookback is not None else globals().get('INST_LOOKBACK', PATTERN_PARAMS_DEFAULTS_C3['INST_LOOKBACK'])\n",
        "    vol_thresh_eff = vol_thresh_mult if vol_thresh_mult is not None else globals().get('INST_VOL_THRESH', PATTERN_PARAMS_DEFAULTS_C3['INST_VOL_THRESH'])\n",
        "    range_thresh_eff = range_thresh_mult if range_thresh_mult is not None else globals().get('INST_RANGE_MULT', PATTERN_PARAMS_DEFAULTS_C3['INST_RANGE_MULT'])\n",
        "    wick_ratio_eff = wick_to_body_max_ratio if wick_to_body_max_ratio is not None else globals().get('INST_WICK_MAX_RATIO', PATTERN_PARAMS_DEFAULTS_C3['INST_WICK_MAX_RATIO'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close', 'Volume'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"InstPattern: Missing OHLCV (TitleCase). Returning original DataFrame.\")\n",
        "        data['Inst_Buy_Signal']=False; data['Inst_Sell_Signal']=False; return data\n",
        "    if len(data) < lookback_eff + 1:\n",
        "        data['Inst_Buy_Signal']=False; data['Inst_Sell_Signal']=False; return data\n",
        "\n",
        "    min_p_inst = _get_min_periods_c3(lookback_eff)\n",
        "    avg_vol = data['Volume'].shift(1).rolling(window=lookback_eff, min_periods=min_p_inst).mean()\n",
        "    candle_range = data['High'] - data['Low']\n",
        "    avg_range = candle_range.shift(1).rolling(window=lookback_eff, min_periods=min_p_inst).mean()\n",
        "\n",
        "    is_vol_spike = data['Volume'] > (vol_thresh_eff * avg_vol)\n",
        "    is_strong_range = candle_range > (range_thresh_eff * avg_range)\n",
        "    # - Replaced replace with direct assignment.\n",
        "    body_size = abs(data['Close'] - data['Open']).replace(0, 1e-9)\n",
        "\n",
        "    common_cond = is_vol_spike & is_strong_range\n",
        "\n",
        "    # Bullish Institutional Signal\n",
        "    is_bullish_candle = data['Close'] > data['Open']\n",
        "    upper_wick = data['High'] - data['Close']\n",
        "    bullish_wick_cond = (upper_wick / body_size) <= wick_ratio_eff\n",
        "    data['Inst_Buy_Signal'] = common_cond & is_bullish_candle & bullish_wick_cond\n",
        "\n",
        "    # Bearish Institutional Signal\n",
        "    is_bearish_candle = data['Close'] < data['Open']\n",
        "    lower_wick = data['Close'] - data['Low']\n",
        "    bearish_wick_cond = (lower_wick / body_size) <= wick_ratio_eff\n",
        "    data['Inst_Sell_Signal'] = common_cond & is_bearish_candle & bearish_wick_cond\n",
        "\n",
        "    # - Replaced inplace=True with direct assignment to avoid FutureWarning.\n",
        "    data['Inst_Buy_Signal'] = data['Inst_Buy_Signal'].fillna(False)\n",
        "    data['Inst_Sell_Signal'] = data['Inst_Sell_Signal'].fillna(False)\n",
        "    return data\n",
        "\n",
        "def detect_market_character(\n",
        "    df: pd.DataFrame,\n",
        "    lookback: Optional[int] = None,\n",
        "    vol_thresh_mult: Optional[float] = None,\n",
        "    range_thresh_mult: Optional[float] = None,\n",
        "    trend_strength_thresh_abs: Optional[float] = None,\n",
        "    copy_df: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Classifies market character: \"Volatile\", \"Trending\", \"Calm\", \"Ranging\", or \"Undefined\".\n",
        "    Adds 'Market_Character' (string) column. This function was already vectorized.\n",
        "    \"\"\"\n",
        "    lookback_eff = lookback if lookback is not None else globals().get('MC_LOOKBACK', PATTERN_PARAMS_DEFAULTS_C3['MC_LOOKBACK'])\n",
        "    vol_thresh_eff = vol_thresh_mult if vol_thresh_mult is not None else globals().get('MC_VOL_THRESH', PATTERN_PARAMS_DEFAULTS_C3['MC_VOL_THRESH'])\n",
        "    range_thresh_eff = range_thresh_mult if range_thresh_mult is not None else globals().get('MC_RANGE_THRESH', PATTERN_PARAMS_DEFAULTS_C3['MC_RANGE_THRESH'])\n",
        "    trend_thresh_eff_abs = trend_strength_thresh_abs if trend_strength_thresh_abs is not None else globals().get('MC_TREND_THRESH_ABS', PATTERN_PARAMS_DEFAULTS_C3['MC_TREND_THRESH_ABS'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close', 'Volume'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"MarketChar: Missing OHLCV (TitleCase). Returning original DataFrame.\")\n",
        "        data['Market_Character'] = \"Undefined_Missing_Cols\"; return data\n",
        "    if len(data) < lookback_eff + 1:\n",
        "        data['Market_Character'] = \"Undefined_Short_Data\"; return data\n",
        "\n",
        "    min_p_mc = _get_min_periods_c3(lookback_eff)\n",
        "    data['temp_mc_avg_vol'] = data['Volume'].shift(1).rolling(window=lookback_eff, min_periods=min_p_mc).mean()\n",
        "    data['temp_mc_curr_range'] = data['High'] - data['Low']\n",
        "    data['temp_mc_avg_range'] = data['temp_mc_curr_range'].shift(1).rolling(window=lookback_eff, min_periods=min_p_mc).mean()\n",
        "    data['temp_mc_price_chg_abs'] = (data['Close'] - data['Close'].shift(lookback_eff)).abs()\n",
        "\n",
        "    valid_calcs = data['temp_mc_avg_range'].notna() & (data['temp_mc_avg_range'] > 1e-7) & \\\n",
        "                  data['temp_mc_avg_vol'].notna() & (data['temp_mc_avg_vol'] > 1e-7) & \\\n",
        "                  data['temp_mc_price_chg_abs'].notna() & data['temp_mc_curr_range'].notna()\n",
        "\n",
        "    cond_volatile = (data['Volume'] > vol_thresh_eff * data['temp_mc_avg_vol']) & \\\n",
        "                    (data['temp_mc_curr_range'] > range_thresh_eff * data['temp_mc_avg_range'])\n",
        "    cond_trending = (data['temp_mc_price_chg_abs'] >= trend_thresh_eff_abs * data['temp_mc_avg_range']) & \\\n",
        "                    (data['temp_mc_curr_range'] <= range_thresh_eff * data['temp_mc_avg_range'])\n",
        "    cond_calm = (data['temp_mc_curr_range'] <= range_thresh_eff * data['temp_mc_avg_range'] * 0.5) & \\\n",
        "                (data['Volume'] <= vol_thresh_eff * data['temp_mc_avg_vol'] * 0.75) & \\\n",
        "                (data['temp_mc_price_chg_abs'] < trend_thresh_eff_abs * data['temp_mc_avg_range'] * 0.5)\n",
        "\n",
        "    choices = [\"Volatile\", \"Trending\", \"Calm\"]\n",
        "    conditions = [cond_volatile & valid_calcs, cond_trending & valid_calcs, cond_calm & valid_calcs]\n",
        "    data['Market_Character'] = np.select(conditions, choices, default=\"Ranging\")\n",
        "    data.loc[~valid_calcs, 'Market_Character'] = \"Undefined\"\n",
        "\n",
        "    return data.drop(columns=['temp_mc_avg_vol','temp_mc_curr_range','temp_mc_avg_range','temp_mc_price_chg_abs'], errors='ignore')\n",
        "\n",
        "def detect_market_sentiment(\n",
        "    df: pd.DataFrame,\n",
        "    lookback: Optional[int] = None,\n",
        "    vol_thresh_mult: Optional[float] = None,\n",
        "    price_chg_thresh_pct: Optional[float] = None,\n",
        "    copy_df: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Determines market sentiment: \"Bullish\", \"Bearish\", \"Neutral\", or \"Unknown\".\n",
        "    Adds 'Market_Sentiment' (string) column. This function was already vectorized.\n",
        "    \"\"\"\n",
        "    lookback_eff = lookback if lookback is not None else globals().get('MS_LOOKBACK', PATTERN_PARAMS_DEFAULTS_C3['MS_LOOKBACK'])\n",
        "    vol_thresh_eff = vol_thresh_mult if vol_thresh_mult is not None else globals().get('MS_VOL_THRESH', PATTERN_PARAMS_DEFAULTS_C3['MS_VOL_THRESH'])\n",
        "    price_chg_thresh_eff_pct = price_chg_thresh_pct if price_chg_thresh_pct is not None else globals().get('MS_PRICE_CHG_THRESH_PCT', PATTERN_PARAMS_DEFAULTS_C3['MS_PRICE_CHG_THRESH_PCT'])\n",
        "\n",
        "    data = df.copy() if copy_df else df\n",
        "    required_cols = {'Close', 'Volume'}\n",
        "    if not required_cols.issubset(data.columns):\n",
        "        logger.warning(\"MarketSent: Missing Close/Volume (TitleCase). Returning original DataFrame.\")\n",
        "        data['Market_Sentiment'] = \"Unknown_Missing_Cols\"; return data\n",
        "    if len(data) < lookback_eff + 1:\n",
        "        data['Market_Sentiment'] = \"Unknown_Short_Data\"; return data\n",
        "\n",
        "    min_p_ms = _get_min_periods_c3(lookback_eff)\n",
        "    data['temp_ms_avg_vol'] = data['Volume'].shift(1).rolling(window=lookback_eff, min_periods=min_p_ms).mean()\n",
        "    data['temp_ms_price_chg_pct'] = data['Close'].pct_change(fill_method=None)\n",
        "    data['temp_ms_avg_price_chg_pct'] = data['temp_ms_price_chg_pct'].shift(1).rolling(window=lookback_eff, min_periods=min_p_ms).mean()\n",
        "\n",
        "    valid_calcs = data['temp_ms_avg_vol'].notna() & (data['temp_ms_avg_vol'] > 1e-7) & \\\n",
        "                  data['temp_ms_avg_price_chg_pct'].notna() & data['Volume'].notna()\n",
        "\n",
        "    cond_bullish = (data['temp_ms_avg_price_chg_pct'] > price_chg_thresh_eff_pct) & \\\n",
        "                   (data['Volume'] > vol_thresh_eff * data['temp_ms_avg_vol'])\n",
        "    cond_bearish = (data['temp_ms_avg_price_chg_pct'] < -price_chg_thresh_eff_pct) & \\\n",
        "                   (data['Volume'] > vol_thresh_eff * data['temp_ms_avg_vol'])\n",
        "\n",
        "    choices = [\"Bullish\", \"Bearish\"]\n",
        "    conditions = [cond_bullish & valid_calcs, cond_bearish & valid_calcs]\n",
        "    data['Market_Sentiment'] = np.select(conditions, choices, default=\"Neutral\")\n",
        "    data.loc[~valid_calcs, 'Market_Sentiment'] = \"Unknown\"\n",
        "\n",
        "    return data.drop(columns=['temp_ms_avg_vol','temp_ms_price_chg_pct','temp_ms_avg_price_chg_pct'], errors='ignore')\n",
        "\n",
        "logger.info(\"Cell 3: Vectorized helper functions (Pattern Detection, SMC, Price Action) defined and refined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCi31Z_PtuBr",
        "outputId": "8803f63b-3c81-47f7-e345-22b6ab5c29a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 4: Data Loading, Preprocessing, and Feature Engineering (Data Handling)\n",
            "2025-06-17 17:49:05 - TradingBotLogger - INFO - [<ipython-input-6-2115711219>.<cell line: 0>:506] - Cell 4: Data Loading, Preprocessing, Feature Engineering, and Sequencing functions defined (Data Handling).\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 4: Data Loading, Preprocessing, and Feature Engineering (Data Handling) ---\n",
        "\n",
        "print(\"\\nInitializing Cell 4: Data Loading, Preprocessing, and Feature Engineering (Data Handling)\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime, timedelta, date as datetime_date\n",
        "import pytz\n",
        "from typing import Union, List, Dict, Any, Optional, Tuple\n",
        "import sqlite3\n",
        "# --- External Libraries ---\n",
        "import ta\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Ensure necessary variables and functions from previous cells are available ---\n",
        "if 'logger' not in globals():\n",
        "    import logging as pylogging_c4; import sys as pysys_c4\n",
        "    logger = pylogging_c4.getLogger(\"TradingBotLogger_C4_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c4 = pylogging_c4.StreamHandler(pysys_c4.stdout)\n",
        "        _ch_c4.setFormatter(pylogging_c4.Formatter('%(asctime)s - %(levelname)s - C4_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c4); logger.setLevel(pylogging_c4.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 4.\")\n",
        "\n",
        "# Globals from Cell 0, 1, 2 (with defaults if not found)\n",
        "config_defaults_c4 = {\n",
        "    'HISTORICAL_DATA_DIR': \"./data_historical\",\n",
        "    'UPSTOX_INSTRUMENT_KEYS': {}, 'NSE_TZ': pytz.timezone(\"Asia/Kolkata\"),\n",
        "    'TARGET_INTERVAL': \"1minute\",\n",
        "    'HISTORICAL_DATA_LOOKBACK_DAYS': 880,\n",
        "    'RECENT_DATA_API_FETCH_DAYS': 30,\n",
        "    'UPSTOX_HISTORY_INTERVAL_MAP': {\"1minute\": \"1minute\", \"day\": \"day\"}, 'UPSTOX_DATE_FORMAT': \"%Y-%m-%d\",\n",
        "    'CLASS_LABELS': {0:'BUY',1:'HOLD',2:'SELL'}, 'LOOKBACK_WINDOW': 60,\n",
        "    'CLASSIFICATION_LOOKAHEAD_PERIODS': 5, 'CLASSIFICATION_PRICE_CHANGE_THRESHOLD': 0.0020,\n",
        "    'USE_LIVE_LOGS_FOR_TRAINING_AUGMENTATION': True, 'LIVE_LOG_AUGMENTATION_SAMPLE_WEIGHT': 1.5,\n",
        "    'AUGMENTATION_LOSS_ATR_MULTIPLIER': 0.5,\n",
        "    'strategy_performance_insights_by_symbol': {}, 'data_store_by_symbol': {},\n",
        "    'SMA_PERIODS': [10,20,50], 'EMA_PERIODS': [10,20,50], 'RSI_PERIOD': 14,\n",
        "    'MACD_FAST': 12, 'MACD_SLOW': 26, 'MACD_SIGNAL': 9, 'ATR_PERIOD': 14,\n",
        "    'BB_WINDOW': 20, 'BB_NUM_STD': 2.0, 'AVG_DAILY_RANGE_PERIOD': 10,\n",
        "    'UPSTOX_SDK_AVAILABLE': False, 'upstox_api_client_global': None,\n",
        "    'UpstoxApiException': Exception, 'upstox_client': None,\n",
        "}\n",
        "for param_c4, default_val_c4 in config_defaults_c4.items():\n",
        "    if param_c4 not in globals(): globals()[param_c4] = default_val_c4\n",
        "\n",
        "# Pattern functions from Cell 3 (with fallbacks)\n",
        "pattern_func_names_c3 = ['find_potential_order_blocks', 'find_engulfing_patterns', 'find_potential_liquidity_sweeps',\n",
        "                       'find_institutional_trading_patterns', 'detect_market_character', 'detect_market_sentiment', '_get_min_periods_c3']\n",
        "for func_name_c3 in pattern_func_names_c3:\n",
        "    if func_name_c3 not in globals():\n",
        "        if func_name_c3 == '_get_min_periods_c3': globals()[func_name_c3] = lambda lookback, factor=0.8: max(1, int(lookback * factor)) if isinstance(lookback, int) and lookback > 0 else 1\n",
        "        else: globals()[func_name_c3] = lambda df, **kwargs: df\n",
        "        logger.warning(f\"Function '{func_name_c3}' (from Cell 3) not found. Using placeholder for Cell 4.\")\n",
        "\n",
        "# --- - Database Management Functions ---\n",
        "\n",
        "def get_db_path(symbol_name: str) -> Optional[str]:\n",
        "    \"\"\"Constructs the full path for a symbol's SQLite database file.\"\"\"\n",
        "    global logger, HISTORICAL_DATA_DIR\n",
        "    if not symbol_name or not isinstance(symbol_name, str):\n",
        "        logger.error(\"get_db_path failed: Provided symbol_name is invalid or empty.\")\n",
        "        return None\n",
        "    db_filename = f\"{symbol_name.strip().upper()}.db\"\n",
        "    return os.path.join(HISTORICAL_DATA_DIR, db_filename)\n",
        "\n",
        "def initialize_database_for_symbol(symbol_name: str):\n",
        "    \"\"\"Ensures a database file and all required tables exist for a symbol.\"\"\"\n",
        "    global logger, get_db_path\n",
        "    db_path = get_db_path(symbol_name)\n",
        "    if not db_path:\n",
        "        logger.error(f\"Could not get DB path for {symbol_name}. DB initialization failed.\"); return\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            table_queries = {\n",
        "                \"historical_data\": \"CREATE TABLE IF NOT EXISTS historical_data (timestamp TEXT PRIMARY KEY, open REAL, high REAL, low REAL, close REAL, volume INTEGER);\",\n",
        "                \"trade_logs\": \"CREATE TABLE IF NOT EXISTS trade_logs (timestamp TEXT, symbol TEXT, type TEXT, action TEXT, price REAL, qty INTEGER, order_id TEXT, pnl_trade REAL, reason TEXT, sl_price REAL, tp_price REAL, atr_at_entry REAL, confidence REAL, daily_pnl_symbol REAL, daily_pnl_portfolio REAL, PRIMARY KEY (timestamp, order_id));\",\n",
        "                \"backtest_logs\": \"CREATE TABLE IF NOT EXISTS backtest_logs (EntryTime TEXT, ExitTime TEXT, Symbol TEXT, PositionType TEXT, EntryPrice REAL, ExitPrice REAL, Shares INTEGER, ExitReason TEXT, GrossPnL REAL, NetPnL REAL, EntryConfidence REAL, EquityAfterTrade REAL, Fold TEXT);\",\n",
        "                \"augmented_logs\": \"CREATE TABLE IF NOT EXISTS augmented_logs (original_signal_ts_utc TEXT, augmented_pnl REAL, augmented_entry_price REAL, augmented_exit_price REAL, augmented_exit_reason TEXT);\"\n",
        "            }\n",
        "            for table_name, query in table_queries.items(): cursor.execute(query)\n",
        "            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_historical_timestamp ON historical_data (timestamp);\")\n",
        "            cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_tradelog_timestamp ON trade_logs (timestamp);\")\n",
        "            conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        logger.error(f\"DB error initializing for {symbol_name} at {db_path}: {e}\", exc_info=True)\n",
        "\n",
        "def write_df_to_db(df: pd.DataFrame, table_name: str, symbol_name: str):\n",
        "    \"\"\"Writes a DataFrame to a specific table in the symbol's database, replacing the existing table.\"\"\"\n",
        "    global logger, get_db_path\n",
        "    if df.empty: return\n",
        "    db_path = get_db_path(symbol_name)\n",
        "    if not db_path: logger.error(f\"Could not get DB path for {symbol_name}. Write failed.\"); return\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
        "            logger.debug(f\"Wrote {len(df)} rows to table '{table_name}' for {symbol_name}.\")\n",
        "    except sqlite3.Error as e:\n",
        "        logger.error(f\"DB error writing to table '{table_name}' for {symbol_name}: {e}\", exc_info=True)\n",
        "\n",
        "def read_table_from_db(table_name: str, symbol_name: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Reads a full table from the symbol's database into a DataFrame.\"\"\"\n",
        "    global logger, get_db_path\n",
        "    db_path = get_db_path(symbol_name)\n",
        "    if not db_path: logger.error(f\"Could not get DB path for {symbol_name}. Read failed.\"); return None\n",
        "    if not os.path.exists(db_path): return pd.DataFrame()\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            df = pd.read_sql_query(f\"SELECT * FROM {table_name}\", conn)\n",
        "            logger.debug(f\"Read {len(df)} rows from table '{table_name}' for {symbol_name}.\")\n",
        "            return df\n",
        "    except (sqlite3.Error, pd.io.sql.DatabaseError) as e:\n",
        "        if \"no such table\" in str(e).lower():\n",
        "            logger.warning(f\"Table '{table_name}' not in DB for {symbol_name}. Returning empty DF.\")\n",
        "            return pd.DataFrame()\n",
        "        logger.error(f\"DB error reading table '{table_name}' for {symbol_name}: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def append_record_to_db(record_dict: Dict[str, Any], table_name: str, symbol_name: str):\n",
        "    \"\"\"Appends a single record (as a dict) to a table in the symbol's database.\"\"\"\n",
        "    global logger, get_db_path\n",
        "    db_path = get_db_path(symbol_name)\n",
        "    if not db_path: logger.error(f\"Could not get DB path for {symbol_name}. Append failed.\"); return\n",
        "    columns = ', '.join(record_dict.keys())\n",
        "    placeholders = ', '.join('?' * len(record_dict))\n",
        "    sql_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(sql_query, list(record_dict.values()))\n",
        "            conn.commit()\n",
        "    except sqlite3.Error as e:\n",
        "        logger.error(f\"DB error appending to table '{table_name}' for {symbol_name}: {e}\", exc_info=True)\n",
        "\n",
        "# --- Core Data Fetching and Management Functions ---\n",
        "\n",
        "async def get_upstox_historical_candles_robust(\n",
        "    instrument_key: str, interval_str_api: str, to_date_obj: datetime, from_date_obj: datetime,\n",
        "    max_retries: int = 3, retry_delay_seconds: int = 5\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Fetches historical candle data from Upstox API with pagination to handle long date ranges.\n",
        "    This version explicitly formats datetime objects into 'YYYY-MM-DD' strings before the API call.\n",
        "    \"\"\"\n",
        "    global upstox_api_client_global, logger, UpstoxApiException, UPSTOX_SDK_AVAILABLE, upstox_client, UPSTOX_DATE_FORMAT\n",
        "\n",
        "    if not UPSTOX_SDK_AVAILABLE or not upstox_api_client_global:\n",
        "        logger.error(f\"SDK/client not ready for {instrument_key}.\"); return None\n",
        "    try:\n",
        "        history_api = upstox_client.HistoryApi(upstox_api_client_global)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"HistoryApi init failed for {instrument_key}: {e}\", exc_info=True); return None\n",
        "\n",
        "    all_fetched_data = []\n",
        "    current_from_date = from_date_obj\n",
        "    logger.info(f\"Initiating paginated fetch for {instrument_key} from {from_date_obj.strftime(UPSTOX_DATE_FORMAT)} to {to_date_obj.strftime(UPSTOX_DATE_FORMAT)}.\")\n",
        "\n",
        "    # Loop through the date range in 60-day chunks\n",
        "    while current_from_date <= to_date_obj:\n",
        "        current_to_date = current_from_date + timedelta(days=60)\n",
        "        if current_to_date > to_date_obj:\n",
        "            current_to_date = to_date_obj\n",
        "\n",
        "        # --- - Ensure dates are formatted to 'YYYY-MM-DD' strings here ---\n",
        "        from_date_iso = current_from_date.strftime(UPSTOX_DATE_FORMAT)\n",
        "        to_date_iso = current_to_date.strftime(UPSTOX_DATE_FORMAT)\n",
        "\n",
        "        logger.info(f\"  Fetching chunk: {from_date_iso} to {to_date_iso}\")\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Use asyncio.to_thread to run the blocking SDK call with correctly formatted date strings\n",
        "                api_response = await asyncio.to_thread(history_api.get_historical_candle_data1, instrument_key, interval_str_api, to_date_iso, from_date_iso, \"2.0\")\n",
        "\n",
        "                if hasattr(api_response, 'status') and str(api_response.status).lower() == 'success':\n",
        "                    if hasattr(api_response, 'data') and api_response.data and hasattr(api_response.data, 'candles') and api_response.data.candles:\n",
        "                        df_chunk = pd.DataFrame(api_response.data.candles, columns=['timestamp_raw', 'open', 'high', 'low', 'close', 'volume', 'oi'])\n",
        "                        df_chunk['timestamp'] = pd.to_datetime(df_chunk['timestamp_raw'], errors='coerce', utc=True)\n",
        "                        all_fetched_data.append(df_chunk)\n",
        "                    break # Success, break from retry loop\n",
        "                else:\n",
        "                    logger.error(f\"Upstox API error on chunk {from_date_iso} (Att {attempt+1}): Status {getattr(api_response, 'status', 'N/A')}\")\n",
        "            except UpstoxApiException as e:\n",
        "                logger.error(f\"UpstoxApiException on chunk {from_date_iso} (Att {attempt+1}): {e.status}-{e.reason}\", exc_info=False)\n",
        "                if e.status in [401, 400, 403, 404]: return None\n",
        "                if e.status == 429: await asyncio.sleep(retry_delay_seconds * (attempt + 2))\n",
        "                else: await asyncio.sleep(retry_delay_seconds)\n",
        "            except Exception as e_gen:\n",
        "                logger.error(f\"General error on chunk {from_date_iso} (Att {attempt+1}): {e_gen}\", exc_info=True)\n",
        "                await asyncio.sleep(retry_delay_seconds)\n",
        "\n",
        "            if attempt == max_retries - 1:\n",
        "                logger.error(f\"Max retries for chunk {from_date_iso}. Aborting full fetch.\"); return None\n",
        "\n",
        "        # Move to the next chunk\n",
        "        current_from_date = current_to_date + timedelta(days=1)\n",
        "        await asyncio.sleep(0.5) # Be respectful to API rate limits between chunks\n",
        "\n",
        "    if not all_fetched_data:\n",
        "        logger.warning(f\"No data fetched for {instrument_key} in the entire range.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Combine all chunks and process the final DataFrame\n",
        "    final_df = pd.concat(all_fetched_data)\n",
        "    final_df.dropna(subset=['timestamp'], inplace=True)\n",
        "    if final_df.empty: return pd.DataFrame()\n",
        "\n",
        "    final_df.set_index('timestamp', inplace=True)\n",
        "    ohlcv_cols = ['open','high','low','close','volume']\n",
        "    for col in ohlcv_cols: final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "    final_df.dropna(subset=ohlcv_cols, how='any', inplace=True)\n",
        "    final_df.sort_index(inplace=True)\n",
        "    final_df = final_df[~final_df.index.duplicated(keep='last')] # Remove any overlaps\n",
        "\n",
        "    logger.info(f\"Paginated fetch complete for {instrument_key}. Total unique candles retrieved: {len(final_df)}.\")\n",
        "    return final_df[ohlcv_cols]\n",
        "\n",
        "\n",
        "# --- CORRECTED Historical Data Update Function ---\n",
        "async def update_historical_data_in_db(\n",
        "    symbol_name: str,\n",
        "    instrument_key: str,\n",
        "    target_interval_user_key: str,\n",
        "    lookback_trading_days: int,\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Populates and intelligently updates the 'historical_data' table for a symbol.\n",
        "    (This version contains the fix for the AttributeError).\n",
        "    \"\"\"\n",
        "    global logger, UPSTOX_HISTORY_INTERVAL_MAP, NSE_TZ, read_table_from_db, write_df_to_db, get_upstox_historical_candles_robust\n",
        "\n",
        "    # ... (function initialization remains the same) ...\n",
        "    api_interval_str = UPSTOX_HISTORY_INTERVAL_MAP.get(target_interval_user_key.lower())\n",
        "    if not api_interval_str:\n",
        "        logger.error(f\"Invalid interval '{target_interval_user_key}' for {symbol_name}.\")\n",
        "        return False\n",
        "    now_nse = datetime.now(NSE_TZ)\n",
        "    df_historical = read_table_from_db('historical_data', symbol_name)\n",
        "    df_current_data = pd.DataFrame()\n",
        "\n",
        "    if df_historical is not None and not df_historical.empty:\n",
        "        df_historical['timestamp'] = pd.to_datetime(df_historical['timestamp'], utc=True, errors='coerce')\n",
        "        df_historical.dropna(subset=['timestamp'], inplace=True)\n",
        "        df_historical.set_index('timestamp', inplace=True)\n",
        "        df_historical.sort_index(inplace=True)\n",
        "        df_current_data = df_historical\n",
        "\n",
        "    num_days_available = len(df_current_data.index.normalize().unique()) if not df_current_data.empty else 0\n",
        "\n",
        "    if df_current_data.empty:\n",
        "        logger.warning(f\"'historical_data' is empty for {symbol_name}. Attempting one-time population from 'candles' archive.\")\n",
        "        df_candles_source = read_table_from_db('candles', symbol_name)\n",
        "        if df_candles_source is None or df_candles_source.empty:\n",
        "            logger.error(f\"CRITICAL: Initial population failed. 'candles' source table is also empty for {symbol_name}.\")\n",
        "            return False\n",
        "\n",
        "        df_current_data = df_candles_source.copy()\n",
        "        df_current_data['timestamp'] = pd.to_datetime(df_current_data['timestamp'], utc=True, errors='coerce')\n",
        "\n",
        "        # --- - Separate the inplace operations ---\n",
        "        df_current_data.set_index('timestamp', inplace=True)\n",
        "        df_current_data.sort_index(inplace=True)\n",
        "\n",
        "        logger.info(f\"Populated with {len(df_current_data)} records from 'candles' archive.\")\n",
        "\n",
        "    elif num_days_available < lookback_trading_days:\n",
        "        # (Data shortfall logic remains the same)\n",
        "        logger.warning(f\"Data shortfall for {symbol_name}: Found {num_days_available} days, require {lookback_trading_days}. Back-filling from 'candles' archive.\")\n",
        "        df_candles_source = read_table_from_db('candles', symbol_name)\n",
        "        if df_candles_source is not None and not df_candles_source.empty:\n",
        "            df_candles_source['timestamp'] = pd.to_datetime(df_candles_source['timestamp'], utc=True, errors='coerce')\n",
        "            df_candles_source.set_index('timestamp', inplace=True)\n",
        "            df_current_data = pd.concat([df_current_data, df_candles_source])\n",
        "            df_current_data = df_current_data[~df_current_data.index.duplicated(keep='first')].sort_index()\n",
        "            logger.info(f\"Back-filled from archive. Data now contains {len(df_current_data.index.normalize().unique())} trading days.\")\n",
        "\n",
        "    # (Rest of the function remains the same)\n",
        "    last_known_timestamp = df_current_data.index.max() if not df_current_data.empty else None\n",
        "    df_to_write = df_current_data.copy()\n",
        "\n",
        "    if last_known_timestamp is None:\n",
        "        logger.warning(f\"No local data for {symbol_name}. Performing initial bulk fetch from API.\")\n",
        "        calendar_days_to_fetch = int(lookback_trading_days * 1.5)\n",
        "        fetch_from_date = datetime.combine(now_nse.date() - timedelta(days=calendar_days_to_fetch), datetime.min.time(), tzinfo=NSE_TZ)\n",
        "    else:\n",
        "        fetch_from_date = last_known_timestamp\n",
        "\n",
        "    df_new_from_api = await get_upstox_historical_candles_robust(\n",
        "        instrument_key, api_interval_str, now_nse, fetch_from_date\n",
        "    )\n",
        "\n",
        "    if df_new_from_api is not None and not df_new_from_api.empty:\n",
        "        df_new_from_api.index = pd.to_datetime(df_new_from_api.index, utc=True, errors='coerce')\n",
        "        if last_known_timestamp:\n",
        "            df_new_from_api = df_new_from_api[df_new_from_api.index > last_known_timestamp]\n",
        "\n",
        "        if not df_new_from_api.empty:\n",
        "            df_merged = pd.concat([df_to_write, df_new_from_api])\n",
        "            df_to_write = df_merged.sort_index()\n",
        "            logger.info(f\"API Update: Fetched and merged {len(df_new_from_api)} new records for {symbol_name}.\")\n",
        "\n",
        "    if df_to_write.empty:\n",
        "        logger.error(f\"No data available for {symbol_name} after all update attempts.\")\n",
        "        return False\n",
        "\n",
        "    trading_dates = df_to_write.index.tz_convert(NSE_TZ).normalize().unique()\n",
        "    if len(trading_dates) > lookback_trading_days:\n",
        "        cutoff_date = trading_dates[-lookback_trading_days]\n",
        "        df_final = df_to_write[df_to_write.index.tz_convert(NSE_TZ).normalize() >= cutoff_date]\n",
        "    else:\n",
        "        df_final = df_to_write\n",
        "\n",
        "    df_out = df_final.reset_index()\n",
        "    df_out['timestamp'] = pd.to_datetime(df_out['timestamp']).dt.tz_convert(NSE_TZ).dt.strftime('%Y-%m-%dT%H:%M:%S%z')\n",
        "\n",
        "    write_df_to_db(df_out, 'historical_data', symbol_name)\n",
        "\n",
        "    final_trading_days = len(df_final.index.normalize().unique())\n",
        "    logger.info(f\"‚úÖ Database update complete for {symbol_name}. 'historical_data' now has {len(df_final)} candles over {final_trading_days} trading days.\")\n",
        "    return True\n",
        "\n",
        "\n",
        "# --- - Refactored functions using the database ---\n",
        "\n",
        "def process_symbol_trade_logs_for_learning(symbol_name: str) -> List[tuple]:\n",
        "    \"\"\"Processes trade logs from the database to extract performance insights and learning experiences.\"\"\"\n",
        "    global logger, strategy_performance_insights_by_symbol, CLASS_LABELS, read_table_from_db\n",
        "    symbol_upper = symbol_name.upper()\n",
        "    default_insights = {'message': 'No trade logs in DB.', 'total_completed_trade_cycles': 0, 'win_rate': 0.0, 'total_net_pnl_from_logs': 0.0}\n",
        "    try:\n",
        "        logs_df = read_table_from_db('trade_logs', symbol_upper)\n",
        "        if logs_df is None or logs_df.empty:\n",
        "            strategy_performance_insights_by_symbol[symbol_upper] = default_insights; return []\n",
        "        logs_df['timestamp'] = pd.to_datetime(logs_df['timestamp'], errors='coerce', utc=True)\n",
        "        for col in ['pnl_trade', 'price', 'atr_at_entry']: logs_df[col] = pd.to_numeric(logs_df[col], errors='coerce')\n",
        "        logs_df.dropna(subset=['timestamp', 'action', 'type', 'price'], inplace=True)\n",
        "        logs_df.sort_values(by='timestamp', inplace=True)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing DB trade logs for {symbol_upper}: {e}\", exc_info=True)\n",
        "        strategy_performance_insights_by_symbol[symbol_upper] = default_insights; return []\n",
        "\n",
        "    completed_trades, open_trade = [], None\n",
        "    for _, row in logs_df.iterrows():\n",
        "        action, trade_type = str(row.get('action', '')).upper(), str(row.get('type', '')).upper()\n",
        "        if action == 'ENTRY':\n",
        "            open_trade = {'ts': row['timestamp'], 'type': trade_type, 'entry_p': row['price'], 'pnl': row['pnl_trade'], 'atr': row['atr_at_entry']}\n",
        "        elif \"EXIT_\" in action and open_trade and open_trade['type'] == trade_type:\n",
        "            open_trade['pnl'] += row['pnl_trade']; open_trade['exit_p'] = row['price']; open_trade['exit_r'] = row.get('reason', action.replace(\"EXIT_\", \"\"))\n",
        "            completed_trades.append(open_trade.copy()); open_trade = None\n",
        "\n",
        "    insights = {'symbol': symbol_upper, 'total_completed_trade_cycles': len(completed_trades)}\n",
        "    if completed_trades:\n",
        "        df_cycles = pd.DataFrame(completed_trades)\n",
        "        wins = df_cycles[df_cycles['pnl'] > 0]; losses = df_cycles[df_cycles['pnl'] <= 0]\n",
        "        insights.update({'win_rate': len(wins) / len(df_cycles) if len(df_cycles) > 0 else 0.0, 'total_net_pnl_from_logs': df_cycles['pnl'].sum()})\n",
        "    strategy_performance_insights_by_symbol[symbol_upper] = insights\n",
        "\n",
        "    experiences = []\n",
        "    for trade in completed_trades:\n",
        "        action_label_str = 'BUY' if trade['type'] == 'LONG' else 'SELL'\n",
        "        if pd.notna(trade['ts']):\n",
        "            experiences.append((trade['ts'], action_label_str, trade['pnl'], trade['entry_p'], trade.get('exit_p'), trade.get('exit_r'), trade['atr']))\n",
        "    return experiences\n",
        "\n",
        "def augment_data_with_live_trade_experiences(\n",
        "    historical_df: pd.DataFrame, symbol_name: str, experiences: list, feature_cols: list[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Augments training data with live trade experiences, saving logs to the database.\"\"\"\n",
        "\n",
        "    global logger, CLASS_LABELS, LIVE_LOG_AUGMENTATION_SAMPLE_WEIGHT, USE_LIVE_LOGS_FOR_TRAINING_AUGMENTATION, AUGMENTATION_LOSS_ATR_MULTIPLIER, write_df_to_db, get_config_value\n",
        "\n",
        "    MAX_AUGMENT_PCT = get_config_value(\n",
        "        ['training_params', 'max_augmentation_percentage'], 'MAX_AUGMENTATION_PERCENTAGE_ENV', 0.05, float\n",
        "    )\n",
        "\n",
        "    if not USE_LIVE_LOGS_FOR_TRAINING_AUGMENTATION or not experiences:\n",
        "        df_to_return = historical_df.copy(); df_to_return['is_augmented'] = 0.0; return df_to_return\n",
        "\n",
        "    logger.info(f\"Augmenting dataset for {symbol_name} with {len(experiences)} live experiences...\")\n",
        "    new_rows, label_to_int = [], {v: k for k, v in CLASS_LABELS.items()}\n",
        "    df_augment_from = historical_df.copy().sort_index()\n",
        "\n",
        "    max_new_rows = int(len(df_augment_from) * MAX_AUGMENT_PCT)\n",
        "\n",
        "    for ts, action, pnl, entry_p, exit_p, exit_r, atr in experiences:\n",
        "        if len(new_rows) >= max_new_rows:\n",
        "            logger.warning(f\"Augmentation limit ({max_new_rows} samples, or {MAX_AUGMENT_PCT:.1%}) reached for {symbol_name}. Halting augmentation for this run.\")\n",
        "            break\n",
        "\n",
        "        ts_aware = pytz.utc.localize(ts) if ts.tzinfo is None else ts.astimezone(pytz.utc)\n",
        "        match_idx = df_augment_from.index.asof(ts_aware)\n",
        "        if pd.isna(match_idx): continue\n",
        "        orig_sample = df_augment_from.loc[match_idx].copy()\n",
        "        new_target = None\n",
        "        if pnl > 0: new_target = label_to_int.get(action)\n",
        "        else:\n",
        "            if atr > 0 and abs(exit_p - entry_p) >= atr * AUGMENTATION_LOSS_ATR_MULTIPLIER:\n",
        "                if action == 'BUY': new_target = label_to_int.get('SELL')\n",
        "                elif action == 'SELL': new_target = label_to_int.get('BUY')\n",
        "        if new_target is not None:\n",
        "            new_data = orig_sample.to_dict()\n",
        "            new_data.update({'target_raw': new_target, 'is_augmented': 1.0, 'original_signal_ts_utc': ts_aware.isoformat(), 'augmented_pnl': pnl, 'augmented_entry_price': entry_p, 'augmented_exit_price': exit_p, 'augmented_exit_reason': exit_r})\n",
        "            for _ in range(int(max(1, LIVE_LOG_AUGMENTATION_SAMPLE_WEIGHT))):\n",
        "\n",
        "                if len(new_rows) >= max_new_rows:\n",
        "                    break\n",
        "                new_rows.append(pd.Series(new_data, name=match_idx + pd.Timedelta(microseconds=len(new_rows)+1)))\n",
        "\n",
        "    if new_rows:\n",
        "        df_new_aug = pd.DataFrame(new_rows)\n",
        "        all_cols = df_augment_from.columns.union(df_new_aug.columns, sort=False).tolist()\n",
        "        df_augment_from['is_augmented'] = 0.0\n",
        "        combined = pd.concat([df_augment_from.reindex(columns=all_cols), df_new_aug.reindex(columns=all_cols)]).sort_index()\n",
        "        db_log_cols = ['original_signal_ts_utc', 'augmented_pnl', 'augmented_entry_price', 'augmented_exit_price', 'augmented_exit_reason']\n",
        "        write_df_to_db(df_new_aug[db_log_cols].copy(), 'augmented_logs', symbol_name)\n",
        "        logger.info(f\"Dataset for {symbol_name} augmented. New total rows: {len(combined)}\")\n",
        "        return combined\n",
        "    else: return df_augment_from\n",
        "\n",
        "# --- load_and_preprocess_data_for_symbol (Main Orchestrator) ---\n",
        "async def load_and_preprocess_data_for_symbol(\n",
        "    symbol_to_process: str, target_interval_key: Optional[str] = None\n",
        ") -> Tuple[Optional[pd.DataFrame], Optional[List[str]]]:\n",
        "    \"\"\"Main data pipeline orchestrator using the corrected database-driven workflow.\"\"\"\n",
        "    global logger, UPSTOX_INSTRUMENT_KEYS, CLASS_LABELS, data_store_by_symbol, HISTORICAL_DATA_LOOKBACK_DAYS, RECENT_DATA_API_FETCH_DAYS, update_historical_data_in_db, read_table_from_db, process_symbol_trade_logs_for_learning, augment_data_with_live_trade_experiences, SMA_PERIODS, EMA_PERIODS, RSI_PERIOD, MACD_FAST, MACD_SLOW, MACD_SIGNAL, ATR_PERIOD, BB_WINDOW, BB_NUM_STD, CLASSIFICATION_LOOKAHEAD_PERIODS, CLASSIFICATION_PRICE_CHANGE_THRESHOLD, find_potential_order_blocks, find_engulfing_patterns\n",
        "    logger.info(f\"--- Starting Data Pipeline for Symbol: {symbol_to_process} ---\")\n",
        "    symbol_upper = symbol_to_process.upper()\n",
        "    instrument_key = UPSTOX_INSTRUMENT_KEYS.get(symbol_upper)\n",
        "    if not instrument_key or \"INVALID_KEY\" in instrument_key: logger.error(f\"No valid instrument key for {symbol_upper}.\"); return None, None\n",
        "    eff_interval = target_interval_key or globals().get('TARGET_INTERVAL', '1minute')\n",
        "\n",
        "    # --- FUNCTION CALL ---\n",
        "    # The `RECENT_DATA_API_FETCH_DAYS` argument is no longer needed.\n",
        "    if not await update_historical_data_in_db(symbol_upper, instrument_key, eff_interval, HISTORICAL_DATA_LOOKBACK_DAYS):\n",
        "        logger.error(f\"Database update failed for {symbol_upper}.\"); return None, None\n",
        "\n",
        "    df_raw = read_table_from_db('historical_data', symbol_upper)\n",
        "    if df_raw is None or df_raw.empty: logger.error(f\"No data in DB for {symbol_upper}.\"); return None, None\n",
        "    df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'], errors='coerce', utc=True)\n",
        "    df_raw.set_index('timestamp', inplace=True); df_raw.sort_index(inplace=True)\n",
        "\n",
        "    logger.info(f\"Engineering features for {symbol_upper}...\")\n",
        "    df_features = df_raw.copy()\n",
        "    close, high, low = df_features['close'], df_features['high'], df_features['low']\n",
        "    for p in SMA_PERIODS: df_features[f'sma_{p}'] = ta.trend.SMAIndicator(close, p, True).sma_indicator()\n",
        "    for p in EMA_PERIODS: df_features[f'ema_{p}'] = ta.trend.EMAIndicator(close, p, True).ema_indicator()\n",
        "    df_features['rsi'] = ta.momentum.RSIIndicator(close, RSI_PERIOD, True).rsi()\n",
        "    macd_i = ta.trend.MACD(close, MACD_SLOW, MACD_FAST, MACD_SIGNAL, True); df_features.update({'macd':macd_i.macd(), 'macd_signal':macd_i.macd_signal(), 'macd_diff':macd_i.macd_diff()})\n",
        "    df_features['atr'] = ta.volatility.AverageTrueRange(high, low, close, ATR_PERIOD, True).average_true_range().replace(0, 1e-7)\n",
        "    bb_i = ta.volatility.BollingerBands(close, BB_WINDOW, BB_NUM_STD, True); df_features.update({'bb_mavg':bb_i.bollinger_mavg(), 'bb_hband':bb_i.bollinger_hband(), 'bb_lband':bb_i.bollinger_lband()})\n",
        "    df_patterns = df_features.rename(columns=str.capitalize)\n",
        "    df_patterns = find_potential_order_blocks(df_patterns, copy_df=False)\n",
        "    df_patterns = find_engulfing_patterns(df_patterns, copy_df=False)\n",
        "    df_features['pattern_bullish_ob'] = df_patterns['Potential_Bullish_Ob'].astype(int)\n",
        "    df_features['pattern_bearish_ob'] = df_patterns['Potential_Bearish_Ob'].astype(int)\n",
        "    df_features['pattern_bullish_engulfing'] = df_patterns['Bullish_Engulfing'].astype(int)\n",
        "    df_features['pattern_bearish_engulfing'] = df_patterns['Bearish_Engulfing'].astype(int)\n",
        "\n",
        "    feature_columns = sorted([c for c in df_features.columns if c not in ['open','high','low','close','volume','oi','timestamp_raw']])\n",
        "    data_store_by_symbol.setdefault(symbol_upper, {})['feature_columns'] = feature_columns\n",
        "\n",
        "    trade_experiences = process_symbol_trade_logs_for_learning(symbol_upper)\n",
        "    df_final = augment_data_with_live_trade_experiences(df_features, symbol_upper, trade_experiences, feature_columns)\n",
        "    if 'is_augmented' not in df_final.columns: df_final['is_augmented'] = 0.0\n",
        "\n",
        "    label_to_int = {v: k for k, v in CLASS_LABELS.items()}\n",
        "    future_max = df_final['high'].shift(-CLASSIFICATION_LOOKAHEAD_PERIODS).rolling(CLASSIFICATION_LOOKAHEAD_PERIODS).max()\n",
        "    future_min = df_final['low'].shift(-CLASSIFICATION_LOOKAHEAD_PERIODS).rolling(CLASSIFICATION_LOOKAHEAD_PERIODS).min()\n",
        "    buy_trigger = df_final['close'] * (1 + CLASSIFICATION_PRICE_CHANGE_THRESHOLD)\n",
        "    sell_trigger = df_final['close'] * (1 - CLASSIFICATION_PRICE_CHANGE_THRESHOLD)\n",
        "    hist_target = pd.Series(label_to_int['HOLD'], index=df_final.index, dtype=int)\n",
        "    hist_target.loc[future_max >= buy_trigger] = label_to_int['BUY']\n",
        "    hist_target.loc[future_min <= sell_trigger] = label_to_int['SELL']\n",
        "    df_final['target_raw'] = hist_target\n",
        "    df_final.loc[df_final['is_augmented'] == 1.0, 'target_raw'] = df_final['target_raw'] # Keep augmented targets\n",
        "    df_final.dropna(subset=['target_raw'], inplace=True)\n",
        "    df_final[feature_columns] = df_final[feature_columns].ffill().bfill()\n",
        "    df_final.dropna(subset=feature_columns, inplace=True)\n",
        "\n",
        "    logger.info(f\"--- Data Pipeline End for Symbol: {symbol_upper}. Final shape: {df_final.shape} ---\")\n",
        "    return df_final, feature_columns\n",
        "\n",
        "# --- create_sequences_tf_data_classification (No changes needed) ---\n",
        "def create_sequences_tf_data_classification(\n",
        "    data_df: pd.DataFrame, feature_cols: list[str], target_col_name_encoded: str,\n",
        "    lookback_window_size: int, batch_proc_size: int, shuffle_data: bool = False\n",
        ") -> Optional[tf.data.Dataset]:\n",
        "    \"\"\"Creates a tf.data.Dataset of sequences from a DataFrame for classification.\"\"\"\n",
        "    global logger, tf\n",
        "    if data_df.empty or len(data_df) < lookback_window_size: return None\n",
        "    if target_col_name_encoded not in data_df.columns: return None\n",
        "    try:\n",
        "        features = data_df[feature_cols].astype(np.float32).values\n",
        "        targets = data_df[target_col_name_encoded].astype(np.int32).values\n",
        "        dataset = tf.keras.utils.timeseries_dataset_from_array(\n",
        "            data=features, targets=targets, sequence_length=lookback_window_size,\n",
        "            sequence_stride=1, shuffle=shuffle_data, batch_size=batch_proc_size)\n",
        "        return dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    except Exception as e: logger.error(f\"Error creating TF Dataset: {e}\", exc_info=True); return None\n",
        "\n",
        "logger.info(\"Cell 4: Data Loading, Preprocessing, Feature Engineering, and Sequencing functions defined (Data Handling).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8F4m5X7t0Bk",
        "outputId": "572956c9-5f84-4998-f534-54a7352cdbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 5: Model Definition (Improved TCN-BiLSTM-Attention Hybrid)\n",
            "2025-06-17 17:49:05 - TradingBotLogger - INFO - [<ipython-input-7-1234645139>.<cell line: 0>:181] - Cell 5: Model Definition functions updated to use a more robust TCN-BiLSTM-Attention architecture.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 5: Model Definition (Improved TCN-BiLSTM-Attention Hybrid) ---\n",
        "# This cell defines the architecture of the deep learning model used for trading predictions.\n",
        "\n",
        "print(\"\\nInitializing Cell 5: Model Definition (Improved TCN-BiLSTM-Attention Hybrid)\")\n",
        "\n",
        "# --- TensorFlow and Keras Imports (these are available from Cell 1) ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, Bidirectional, LSTM, MultiHeadAttention,\n",
        "    GlobalAveragePooling1D, Dense, Dropout, Add, Activation,\n",
        "    LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "import keras_tuner as kt\n",
        "from typing import Tuple, Optional, Any\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# --- Ensure necessary variables from Cell 1 and Cell 2 are available ---\n",
        "if 'logger' not in globals():\n",
        "    logger = logging.getLogger(\"TradingBotLogger_C5_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c5 = logging.StreamHandler(sys.stdout)\n",
        "        _ch_c5.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - C5_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c5)\n",
        "        logger.setLevel(logging.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 5.\")\n",
        "\n",
        "# Default values for model parameters if not loaded from config\n",
        "MODEL_CONFIG_DEFAULTS_C5: dict[str, Any] = {\n",
        "    'L2_REG_STRENGTH': 1e-5, 'DROPOUT_RATE': 0.35, 'INITIAL_LEARNING_RATE': 5e-5,\n",
        "    'WEIGHT_DECAY': 1e-6, 'XLA_ENABLED': False,\n",
        "    'LOOKBACK_WINDOW': 60,\n",
        "    'CLASS_LABELS': {0: 'BUY', 1: 'HOLD', 2: 'SELL'}\n",
        "}\n",
        "for param_c5, default_value_c5 in MODEL_CONFIG_DEFAULTS_C5.items():\n",
        "    if param_c5 not in globals():\n",
        "        globals()[param_c5] = default_value_c5\n",
        "\n",
        "def tcn_block(\n",
        "    inputs: tf.Tensor,\n",
        "    filters: int,\n",
        "    kernel_size: int,\n",
        "    dilation_rate: int,\n",
        "    dropout_rate: float,\n",
        "    l2_strength: float,\n",
        "    use_layernorm: bool = True,\n",
        "    block_name: str = \"tcn_block\"\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Defines a Temporal Convolutional Network (TCN) block with a residual connection.\n",
        "    This block consists of two dilated causal convolutional layers with normalization and activation.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(block_name):\n",
        "        residual_connection = inputs\n",
        "\n",
        "        # First convolutional layer in the block\n",
        "        x = Conv1D(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate,\n",
        "                   padding='causal', kernel_regularizer=l2(l2_strength), name=f\"{block_name}_conv1\")(inputs)\n",
        "        if use_layernorm:\n",
        "            x = LayerNormalization(name=f\"{block_name}_ln1\")(x)\n",
        "        x = Activation('relu', name=f\"{block_name}_relu1\")(x)\n",
        "        x = Dropout(dropout_rate, name=f\"{block_name}_dropout1\")(x)\n",
        "\n",
        "        # Second convolutional layer in the block\n",
        "        x = Conv1D(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate,\n",
        "                   padding='causal', kernel_regularizer=l2(l2_strength), name=f\"{block_name}_conv2\")(x)\n",
        "        if use_layernorm:\n",
        "            x = LayerNormalization(name=f\"{block_name}_ln2\")(x)\n",
        "        x = Activation('relu', name=f\"{block_name}_relu2\")(x)\n",
        "        x = Dropout(dropout_rate, name=f\"{block_name}_dropout2\")(x)\n",
        "\n",
        "        # Add residual connection. Project the residual if channel dimensions don't match.\n",
        "        projected_residual = residual_connection\n",
        "        if residual_connection.shape[-1] != filters:\n",
        "            projected_residual = Conv1D(filters=filters, kernel_size=1, padding='same',\n",
        "                                        kernel_regularizer=l2(l2_strength),\n",
        "                                        name=f\"{block_name}_residual_projection\")(residual_connection)\n",
        "\n",
        "        x = Add(name=f\"{block_name}_add_residual\")([x, projected_residual])\n",
        "        return x\n",
        "\n",
        "def build_advanced_model(\n",
        "    hp: Optional[kt.HyperParameters],\n",
        "    input_shape: Tuple[int, int],\n",
        "    num_classes: int,\n",
        "    cfg_l2_strength: float,\n",
        "    cfg_dropout_rate: float,\n",
        "    cfg_initial_learning_rate: float,\n",
        "    cfg_weight_decay: float,\n",
        "    cfg_xla_enabled: bool,\n",
        "    cfg_keras_tuner_enabled: bool\n",
        ") -> Model:\n",
        "    \"\"\"\n",
        "    Builds a powerful hybrid model using a TCN block for feature extraction\n",
        "    followed by a BiLSTM and Attention layer for temporal processing. This replaces\n",
        "    the previous, more complex architecture to reduce overfitting risk.\n",
        "    \"\"\"\n",
        "    global logger\n",
        "    is_tuning_active = cfg_keras_tuner_enabled and (hp is not None)\n",
        "    inputs_layer = Input(shape=input_shape, name=\"input_sequence_layer\")\n",
        "\n",
        "    # --- Hyperparameters ---\n",
        "    current_l2_strength = hp.Float('l2_reg_strength', min_value=1e-7, max_value=1e-4, sampling='log', default=cfg_l2_strength) if is_tuning_active else cfg_l2_strength\n",
        "    current_dropout_rate = hp.Float('dropout_rate', min_value=0.15, max_value=0.5, step=0.05, default=cfg_dropout_rate) if is_tuning_active else cfg_dropout_rate\n",
        "    tcn_filters = hp.Int('tcn_filters', min_value=32, max_value=96, step=16, default=64) if is_tuning_active else 64\n",
        "    bilstm_units = hp.Int('bilstm_units', min_value=48, max_value=128, step=16, default=64) if is_tuning_active else 64\n",
        "    attention_heads = hp.Int('attention_heads', min_value=2, max_value=8, step=2, default=4) if is_tuning_active else 4\n",
        "\n",
        "    # --- 1. TCN Block for multi-scale feature extraction ---\n",
        "    tcn_output = tcn_block(\n",
        "        inputs=inputs_layer,\n",
        "        filters=tcn_filters,\n",
        "        kernel_size=3,\n",
        "        dilation_rate=2,\n",
        "        dropout_rate=current_dropout_rate,\n",
        "        l2_strength=current_l2_strength,\n",
        "        block_name=\"tcn_feature_extractor\"\n",
        "    )\n",
        "\n",
        "    # --- 2. Bidirectional LSTM Block ---\n",
        "    lstm_output = Bidirectional(LSTM(units=bilstm_units, return_sequences=True, kernel_regularizer=l2(current_l2_strength)), name=\"bilstm_layer\")(tcn_output)\n",
        "    lstm_output = LayerNormalization(name=\"bilstm_layer_norm\")(lstm_output)\n",
        "    lstm_output = Dropout(current_dropout_rate, name=\"bilstm_dropout\")(lstm_output)\n",
        "\n",
        "    # --- 3. Multi-Head Attention Block ---\n",
        "    attention_key_dim = max(1, bilstm_units // attention_heads)\n",
        "    attention_output = MultiHeadAttention(num_heads=attention_heads, key_dim=attention_key_dim, dropout=current_dropout_rate, name=\"multihead_attention\")(query=lstm_output, value=lstm_output, key=lstm_output)\n",
        "    pooled_output = GlobalAveragePooling1D(name=\"global_average_pooling\")(attention_output)\n",
        "\n",
        "    # --- 4. Output Classification Block ---\n",
        "    dense_output = Dense(units=bilstm_units // 2, activation='relu', kernel_regularizer=l2(current_l2_strength), name=\"output_dense_layer\")(pooled_output)\n",
        "    final_dropout = Dropout(current_dropout_rate, name=\"output_dropout\")(dense_output)\n",
        "    outputs_layer = Dense(units=num_classes, activation='softmax', name='classification_softmax_output', dtype='float32')(final_dropout)\n",
        "\n",
        "    model = Model(inputs=inputs_layer, outputs=outputs_layer, name=\"TCN_BiLSTM_Attention_Model\")\n",
        "\n",
        "    # --- Optimizer ---\n",
        "    current_learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log', default=cfg_initial_learning_rate) if is_tuning_active else cfg_initial_learning_rate\n",
        "    current_weight_decay = hp.Float('weight_decay', min_value=1e-7, max_value=1e-4, sampling='log', default=cfg_weight_decay) if is_tuning_active else cfg_weight_decay\n",
        "    optimizer = AdamW(learning_rate=current_learning_rate, weight_decay=current_weight_decay)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'],\n",
        "                  jit_compile=cfg_xla_enabled)\n",
        "\n",
        "    logger.info(f\"TCN-BiLSTM-Attention model built. Input: {input_shape}, Output classes: {num_classes}.\")\n",
        "    return model\n",
        "\n",
        "def model_builder_for_tuner_adv(\n",
        "    hp: kt.HyperParameters,\n",
        "    # These fixed configurations are passed via a lambda from KerasTuner setup in Cell 6\n",
        "    cfg_input_shape: Tuple[int, int],\n",
        "    cfg_num_classes: int,\n",
        "    cfg_l2_strength: float,\n",
        "    cfg_dropout_rate: float,\n",
        "    cfg_initial_learning_rate: float,\n",
        "    cfg_weight_decay: float,\n",
        "    cfg_xla_enabled: bool\n",
        ") -> Model:\n",
        "    \"\"\"\n",
        "    Wrapper function for KerasTuner to build the advanced model.\n",
        "    It passes the KerasTuner 'hp' object and fixed configurations to the main model builder.\n",
        "    \"\"\"\n",
        "    # This now calls the refactored, more robust model builder\n",
        "    return build_advanced_model(\n",
        "        hp=hp,\n",
        "        input_shape=cfg_input_shape,\n",
        "        num_classes=cfg_num_classes,\n",
        "        cfg_l2_strength=cfg_l2_strength,\n",
        "        cfg_dropout_rate=cfg_dropout_rate,\n",
        "        cfg_initial_learning_rate=cfg_initial_learning_rate,\n",
        "        cfg_weight_decay=cfg_weight_decay,\n",
        "        cfg_xla_enabled=cfg_xla_enabled,\n",
        "        cfg_keras_tuner_enabled=True\n",
        "    )\n",
        "\n",
        "logger.info(\"Cell 5: Model Definition functions updated to use a more robust TCN-BiLSTM-Attention architecture.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZPWI8iIt6Y7",
        "outputId": "83e96ef3-2c7f-41ff-f9af-5f55f6f2fb23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 6: Model Training and Hyperparameter Tuning Pipeline\n",
            "2025-06-17 17:49:05 - TradingBotLogger - WARNING - [<ipython-input-8-1363183348>.<cell line: 0>:39] - send_telegram_message (Cell 8) placeholder used.\n",
            "2025-06-17 17:49:05 - TradingBotLogger - WARNING - [<ipython-input-8-1363183348>.<cell line: 0>:45] - get_symbol_specific_file_path_template placeholder used.\n",
            "2025-06-17 17:49:05 - TradingBotLogger - INFO - [<ipython-input-8-1363183348>.<cell line: 0>:479] - Cell 6: Fully enhanced model training and tuning pipeline functions are defined and ready.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 6: Model Training and Hyperparameter Tuning Pipeline ---\n",
        "\n",
        "print(\"\\nInitializing Cell 6: Model Training and Hyperparameter Tuning Pipeline\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "import joblib\n",
        "import asyncio\n",
        "from typing import Union, List, Dict, Any, Optional, Tuple\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model as keras_load_model, Model as KerasModel\n",
        "import keras_tuner as kt\n",
        "\n",
        "# --- Ensure necessary variables and functions from previous cells are available ---\n",
        "if 'logger' not in globals():\n",
        "    import logging as pylogging_c6; import sys as pysys_c6\n",
        "    logger = pylogging_c6.getLogger(\"TradingBotLogger_C6_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c6 = pylogging_c6.StreamHandler(pysys_c6.stdout)\n",
        "        _ch_c6.setFormatter(pylogging_c6.Formatter('%(asctime)s - %(levelname)s - C6_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c6); logger.setLevel(pylogging_c6.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 6.\")\n",
        "\n",
        "if 'send_telegram_message' not in globals():\n",
        "    async def send_telegram_message(msg_text_tg: str, chat_id_override_tg: Optional[str] = None) -> bool:\n",
        "        logger.info(f\"Telegram (mock_C6): {msg_text_tg}\"); return True\n",
        "    logger.warning(\"send_telegram_message (Cell 8) placeholder used.\")\n",
        "\n",
        "if 'get_symbol_specific_file_path_template' not in globals():\n",
        "    def get_symbol_specific_file_path_template(base_dir: str, base_fn: str, sym: str, s_type: str, f_id: str, ext: str) -> str:\n",
        "        filename = f\"{base_fn}_{sym.upper()}_{s_type}_{str(f_id).replace(' ', '_')}.{ext}\"\n",
        "        return os.path.join(base_dir, filename)\n",
        "    logger.warning(\"get_symbol_specific_file_path_template placeholder used.\")\n",
        "\n",
        "# --- Configuration Defaults ---\n",
        "config_defaults_c6 = {\n",
        "    'MODELS_ARTEFACTS_DIR': \"./models\", 'MODEL_BASE_FILENAME': \"trading_model\",\n",
        "    'TUNING_RESULTS_DIR': \"./results_tuning\", 'TUNER_PROJECT_NAME_TEMPLATE': \"tuner_project_{symbol}\",\n",
        "    'CLASS_LABELS': {0:'BUY',1:'HOLD',2:'SELL'}, 'LOOKBACK_WINDOW': 60,\n",
        "    'BATCH_SIZE': 32, 'EPOCHS': 5,\n",
        "    'CONFIDENCE_THRESHOLD_TRADE': 0.86,\n",
        "    'SL_ATR_MULTIPLIER_DEFAULT': 0.75, 'TP_ATR_MULTIPLIER_DEFAULT': 1.5,\n",
        "    'BACKTEST_TRANSACTION_COST_PCT': 0.0007,\n",
        "    'SIMULATION_INITIAL_CAPITAL': 100000,\n",
        "    'RISK_FREE_RATE': 0.0,\n",
        "    'strategy_performance_insights_by_symbol': {}, 'live_states_by_symbol': {}, 'data_store_by_symbol': {},\n",
        "    'trained_models_by_symbol': {}, 'best_hyperparameters_by_symbol': {},\n",
        "    'KERAS_TUNER_ENABLED': True, 'TUNER_MAX_TRIALS': 8, 'TUNER_EXEC_PER_TRIAL': 1,\n",
        "    'TUNER_OBJECTIVE_METRIC': 'val_sharpe_ratio',\n",
        "    'ES_MONITOR': 'val_sharpe_ratio',\n",
        "    'ES_PATIENCE': 8, 'ES_RESTORE_BEST': True,\n",
        "    'RLP_MONITOR': 'val_sharpe_ratio', # - Changed from 'val_loss'\n",
        "    'RLP_FACTOR': 0.2, 'RLP_PATIENCE': 7, 'RLP_MIN_LR': 1e-7,\n",
        "    'L2_REG_STRENGTH': 1e-6, 'DROPOUT_RATE': 0.25, 'INITIAL_LEARNING_RATE': 5e-5, 'WEIGHT_DECAY': 1e-6,\n",
        "    'XLA_ENABLED': True,\n",
        "    'WALK_FORWARD_VALIDATION_ENABLED': True, 'N_SPLITS_WALK_FORWARD': 5,\n",
        "    'TEST_RATIO': 0.2,\n",
        "    'ENSEMBLE_ENABLED': True, 'N_ENSEMBLE_MODELS_CONFIG': 3,\n",
        "    'TARGET_INTERVAL': '1minute', 'HISTORICAL_DATA_LOOKBACK_DAYS': 880,\n",
        "    'UPSTOX_INSTRUMENT_KEYS': {}, 'MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG': 10,\n",
        "}\n",
        "for param_c6, default_val_c6 in config_defaults_c6.items():\n",
        "    if param_c6 not in globals(): globals()[param_c6] = default_val_c6\n",
        "\n",
        "# --- Function Placeholders ---\n",
        "if 'create_sequences_tf_data_classification' not in globals():\n",
        "    globals()['create_sequences_tf_data_classification'] = lambda *args, **kwargs: None; logger.warning(\"create_sequences_tf_data_classification (Cell 4) placeholder used.\")\n",
        "if 'load_and_preprocess_data_for_symbol' not in globals():\n",
        "    globals()['load_and_preprocess_data_for_symbol'] = lambda *args, **kwargs: (None, None); logger.warning(\"load_and_preprocess_data_for_symbol (Cell 4) placeholder used.\")\n",
        "if 'build_advanced_model' not in globals():\n",
        "    globals()['build_advanced_model'] = lambda *args, **kwargs: None; logger.warning(\"build_advanced_model (Cell 5) placeholder used.\")\n",
        "if 'model_builder_for_tuner_adv' not in globals():\n",
        "    globals()['model_builder_for_tuner_adv'] = lambda *args, **kwargs: None; logger.warning(\"model_builder_for_tuner_adv (Cell 5) placeholder used.\")\n",
        "\n",
        "class TradingMetricsCallback(Callback):\n",
        "    \"\"\"\n",
        "    A custom Keras callback to perform a trading simulation on validation data at each epoch end.\n",
        "    Calculates advanced metrics like Sharpe Ratio, enabling optimization on risk-adjusted returns.\n",
        "    \"\"\"\n",
        "    def __init__(self, val_df_unscaled_for_sim: pd.DataFrame, symbol_name: str, lookback_window: int,\n",
        "                 feature_columns: List[str], scaler_obj: Any, label_encoder_obj: Any, batch_size_cb: int,\n",
        "                 class_labels: Dict[int, str], confidence_thresh: float, sl_atr_multiplier: float,\n",
        "                 tp_atr_multiplier: float, transaction_cost_pct: float, initial_capital: float, risk_free_rate: float, slippage_pct: float):\n",
        "\n",
        "        super().__init__()\n",
        "        self.val_df_unscaled_for_sim = val_df_unscaled_for_sim\n",
        "        self.symbol_name = symbol_name.upper()\n",
        "        self.lookback = lookback_window\n",
        "        self.feature_columns = feature_columns\n",
        "        self.scaler_obj = scaler_obj\n",
        "        self.label_encoder_obj = label_encoder_obj\n",
        "        self.batch_size_cb = batch_size_cb\n",
        "        self.class_labels = class_labels\n",
        "        self.conf_thresh = confidence_thresh\n",
        "        self.sl_mult = sl_atr_multiplier\n",
        "        self.tp_mult = tp_atr_multiplier\n",
        "        self.transaction_cost_pct = transaction_cost_pct\n",
        "        self.initial_capital = initial_capital\n",
        "        # --- - Store slippage_pct ---\n",
        "        self.slippage_pct = slippage_pct\n",
        "        self.risk_free_rate_per_period = (1 + risk_free_rate)**(1/252) - 1\n",
        "        self.active = False\n",
        "        self.sim_data_unscaled_ready = pd.DataFrame()\n",
        "        # This attribute will hold the history of trades within the KerasTuner search\n",
        "        self.intra_simulation_history = []\n",
        "\n",
        "\n",
        "        if not self.val_df_unscaled_for_sim.empty and len(self.val_df_unscaled_for_sim) >= self.lookback:\n",
        "            self.sim_data_unscaled_ready = self.val_df_unscaled_for_sim.iloc[self.lookback - 1:].copy()\n",
        "            if 'atr' not in self.sim_data_unscaled_ready.columns or self.sim_data_unscaled_ready['atr'].isnull().all():\n",
        "                self.sim_data_unscaled_ready['atr'] = self.sim_data_unscaled_ready['close'] * 0.015\n",
        "            self.sim_data_unscaled_ready['atr'] = self.sim_data_unscaled_ready['atr'].ffill().bfill()\n",
        "            self.sim_data_unscaled_ready.loc[self.sim_data_unscaled_ready['atr'].abs() < 1e-9, 'atr'] = 1e-7\n",
        "            self.sim_data_unscaled_ready.dropna(subset=['open','high','low','close','atr'], inplace=True)\n",
        "            if not self.sim_data_unscaled_ready.empty: self.active = True\n",
        "\n",
        "        if self.active: logger.info(f\"TradingMetricsCallback {self.symbol_name} init. Sim on {len(self.sim_data_unscaled_ready)} candles.\")\n",
        "        else: logger.warning(f\"TradingMetricsCallback {self.symbol_name}: Val data insufficient. Callback inactive.\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        # Initialize logs with default values.\n",
        "        logs.update({'val_win_rate': 0.0, 'val_profit_factor': 0.0, 'val_total_pnl': 0.0, 'val_sharpe_ratio': -1.0, 'val_max_drawdown': 1.0, 'val_trade_count': 0})\n",
        "        if not self.active: return\n",
        "\n",
        "        try:\n",
        "            # (The adaptation logic for SL/TP multipliers remains the same)\n",
        "            epoch_sl_mult = self.sl_mult\n",
        "            epoch_tp_mult = self.tp_mult\n",
        "\n",
        "            if self.intra_simulation_history:\n",
        "                sim_trades_df = pd.DataFrame(self.intra_simulation_history)\n",
        "                win_rate = len(sim_trades_df[sim_trades_df['pnl'] > 0]) / len(sim_trades_df) if not sim_trades_df.empty else 0\n",
        "                if win_rate < 0.4: epoch_sl_mult *= 1.05\n",
        "                elif win_rate > 0.6: epoch_sl_mult *= 0.95\n",
        "                if win_rate > 0.55: epoch_tp_mult *= 1.05\n",
        "                elif win_rate < 0.45: epoch_tp_mult *= 0.95\n",
        "                epoch_sl_mult = max(0.25, min(epoch_sl_mult, 2.0))\n",
        "                epoch_tp_mult = max(1.0, min(epoch_tp_mult, 5.0))\n",
        "\n",
        "            # (The data preparation logic remains the same)\n",
        "            temp_val_df_scaled_for_predict = self.val_df_unscaled_for_sim.copy()\n",
        "            temp_val_df_scaled_for_predict[self.feature_columns] = self.scaler_obj.transform(temp_val_df_scaled_for_predict[self.feature_columns])\n",
        "            temp_val_df_scaled_for_predict['target_encoded_cb'] = self.label_encoder_obj.transform(temp_val_df_scaled_for_predict['target_raw'])\n",
        "            val_dataset = create_sequences_tf_data_classification(temp_val_df_scaled_for_predict, self.feature_columns, 'target_encoded_cb', self.lookback, self.batch_size_cb, False)\n",
        "            if val_dataset is None: logger.error(f\"MetricsCB {self.symbol_name}: Failed to create TF dataset.\"); return\n",
        "\n",
        "            predictions_probs = self.model.predict(val_dataset, verbose=0)\n",
        "            sim_df_epoch = self.sim_data_unscaled_ready.iloc[:len(predictions_probs)].copy()\n",
        "            sim_df_epoch['predicted_label_idx'] = np.argmax(predictions_probs, axis=1)\n",
        "            sim_df_epoch['predicted_confidence'] = np.max(predictions_probs, axis=1)\n",
        "\n",
        "            equity_curve = [self.initial_capital]\n",
        "            trade_returns = []\n",
        "            current_epoch_trades = []\n",
        "            current_pos, entry_price, trades_count, winning_trades, gross_profit, gross_loss = 'None', 0.0, 0, 0, 0.0, 0.0\n",
        "\n",
        "            # --- SIMULATION LOOP ---\n",
        "            for i in range(len(sim_df_epoch)):\n",
        "                row = sim_df_epoch.iloc[i]\n",
        "                atr_val = row['atr']\n",
        "\n",
        "                # --- Exit Logic ---\n",
        "                if current_pos != 'None':\n",
        "                    exit_now, pnl, exit_reason = False, 0.0, None\n",
        "                    exit_price_ideal = 0.0\n",
        "\n",
        "                    sl_price = entry_price - (atr_val * epoch_sl_mult) if current_pos == 'Long' else entry_price + (atr_val * epoch_sl_mult)\n",
        "                    tp_price = entry_price + (atr_val * epoch_tp_mult) if current_pos == 'Long' else entry_price - (atr_val * epoch_tp_mult)\n",
        "\n",
        "                    sl_hit = (current_pos == 'Long' and row['low'] <= sl_price) or (current_pos == 'Short' and row['high'] >= sl_price)\n",
        "                    tp_hit = (current_pos == 'Long' and row['high'] >= tp_price) or (current_pos == 'Short' and row['low'] <= tp_price)\n",
        "\n",
        "                    if sl_hit:\n",
        "                        exit_now, exit_price_ideal, exit_reason = True, sl_price, \"SL_HIT\"\n",
        "                    elif tp_hit:\n",
        "                        exit_now, exit_price_ideal, exit_reason = True, tp_price, \"TP_HIT\"\n",
        "\n",
        "                    if exit_now:\n",
        "\n",
        "                        if current_pos == 'Long': # Selling to exit, price is lower\n",
        "                            exit_price_actual = exit_price_ideal * (1 - self.slippage_pct)\n",
        "                        else: # Buying to exit, price is higher\n",
        "                            exit_price_actual = exit_price_ideal * (1 + self.slippage_pct)\n",
        "\n",
        "                        pnl = (exit_price_actual - entry_price) if current_pos == 'Long' else (entry_price - exit_price_actual)\n",
        "                        pnl -= (abs(entry_price) + abs(exit_price_actual)) * self.transaction_cost_pct\n",
        "\n",
        "                        trade_returns.append(pnl / equity_curve[-1])\n",
        "                        equity_curve.append(equity_curve[-1] + pnl)\n",
        "                        if pnl > 0: winning_trades += 1; gross_profit += pnl\n",
        "                        else: gross_loss += abs(pnl)\n",
        "                        current_pos = 'None'\n",
        "                        current_epoch_trades.append({'pnl': pnl})\n",
        "\n",
        "                # --- Entry Logic ---\n",
        "                if current_pos == 'None' and row['predicted_confidence'] >= self.conf_thresh:\n",
        "                    signal = self.class_labels.get(row['predicted_label_idx'])\n",
        "                    if signal == 'BUY' or signal == 'SELL':\n",
        "                        trades_count += 1\n",
        "                        current_pos = 'Long' if signal == 'BUY' else 'Short'\n",
        "                        entry_price_ideal = row['close']\n",
        "\n",
        "                        if current_pos == 'Long':\n",
        "                            entry_price = entry_price_ideal * (1 + self.slippage_pct)\n",
        "                        else:\n",
        "                            entry_price = entry_price_ideal * (1 - self.slippage_pct)\n",
        "\n",
        "            # (Update logs and history logic remains the same)\n",
        "            self.intra_simulation_history = current_epoch_trades\n",
        "            # ... (rest of the metric calculation and logging)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"TradingMetricsCallback {self.symbol_name} Error: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "async def adapt_strategy_parameters_for_symbol(symbol_name: str):\n",
        "    \"\"\"Dynamically adapts SL/TP multipliers based on historical trade performance using a smoothed, dampened approach.\"\"\"\n",
        "    global logger, strategy_performance_insights_by_symbol, live_states_by_symbol, SL_ATR_MULTIPLIER_DEFAULT, TP_ATR_MULTIPLIER_DEFAULT, send_telegram_message, MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG, get_config_value\n",
        "\n",
        "    DAMPENING_FACTOR = get_config_value(\n",
        "        ['strategy_params', 'strategy_adaptation_dampening_factor'], 'STRATEGY_ADAPT_DAMPENING_FACTOR_ENV', 0.1, float\n",
        "    )\n",
        "    SL_MIN = get_config_value(['strategy_params', 'sl_atr_multiplier_min'], 'SL_MIN_ENV', 0.2, float)\n",
        "    SL_MAX = get_config_value(['strategy_params', 'sl_atr_multiplier_max'], 'SL_MAX_ENV', 2.0, float)\n",
        "    TP_MIN = get_config_value(['strategy_params', 'tp_atr_multiplier_min'], 'TP_MIN_ENV', 1.0, float)\n",
        "    TP_MAX = get_config_value(['strategy_params', 'tp_atr_multiplier_max'], 'TP_MAX_ENV', 5.0, float)\n",
        "\n",
        "    sym_upper = symbol_name.upper(); logger.info(f\"Attempting to adapt strategy for {sym_upper}\")\n",
        "    insights = strategy_performance_insights_by_symbol.get(sym_upper, {})\n",
        "    if sym_upper not in live_states_by_symbol: live_states_by_symbol[sym_upper] = {}\n",
        "    current_state = live_states_by_symbol[sym_upper]\n",
        "\n",
        "    original_sl = current_state.get('current_sl_atr_multiplier', SL_ATR_MULTIPLIER_DEFAULT)\n",
        "    original_tp = current_state.get('current_tp_atr_multiplier', TP_ATR_MULTIPLIER_DEFAULT)\n",
        "\n",
        "    target_sl, target_tp = original_sl, original_tp\n",
        "    adapted_sl, adapted_tp = original_sl, original_tp\n",
        "\n",
        "    total_trades = insights.get('total_completed_trade_cycles', 0)\n",
        "\n",
        "    if total_trades >= MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG:\n",
        "        logger.info(f\"Adapting strategy for {sym_upper} based on {total_trades} trades.\")\n",
        "        wr, slf, tpf = insights.get('win_rate',0.5), insights.get('sl_hit_frequency',0.5), insights.get('tp_hit_frequency',0.2)\n",
        "\n",
        "        if wr < 0.4 and slf > 0.6: target_sl *= 1.10\n",
        "        elif wr > 0.6 and slf < 0.3: target_sl *= 0.90\n",
        "\n",
        "        if wr > 0.5 and tpf < 0.35 and original_tp > 1.2: target_tp *= 0.95\n",
        "        elif wr > 0.65 and tpf > 0.55: target_tp *= 1.05\n",
        "\n",
        "        adapted_sl = (original_sl * (1 - DAMPENING_FACTOR)) + (target_sl * DAMPENING_FACTOR)\n",
        "        adapted_tp = (original_tp * (1 - DAMPENING_FACTOR)) + (target_tp * DAMPENING_FACTOR)\n",
        "\n",
        "        adapted_sl = max(SL_MIN, min(adapted_sl, SL_MAX))\n",
        "        adapted_tp = max(TP_MIN, min(adapted_tp, TP_MAX))\n",
        "\n",
        "        if abs(adapted_sl - original_sl) > 0.01 or abs(adapted_tp - original_tp) > 0.01:\n",
        "            msg = (f\"üìà Strategy ADAPTED for {sym_upper}:\\n  SLM: {original_sl:.2f} -> {adapted_sl:.2f}\\n  TPM: {original_tp:.2f} -> {adapted_tp:.2f}\\n\"\n",
        "                   f\"  Based on: {total_trades} trades, WR:{wr:.2%}\")\n",
        "            logger.info(msg); await send_telegram_message(msg)\n",
        "    else:\n",
        "        logger.info(f\"Not enough trades for {sym_upper} ({total_trades}/{MIN_TRADES_FOR_STRATEGY_ADAPTATION_CONFIG}) to adapt.\")\n",
        "\n",
        "    current_state['current_sl_atr_multiplier'] = adapted_sl\n",
        "    current_state['current_tp_atr_multiplier'] = adapted_tp\n",
        "\n",
        "\n",
        "def run_hyperparameter_tuning_for_symbol_sync(\n",
        "    symbol_name: str, train_df_unscaled: pd.DataFrame, val_df_unscaled: pd.DataFrame,\n",
        "    feature_columns_sym: list[str], input_shape_sym: tuple, num_classes_sym: int\n",
        ") -> Optional[kt.HyperParameters]:\n",
        "    \"\"\"\n",
        "    Synchronous function to run KerasTuner search, designed to be called in a separate thread.\n",
        "    This function now uses the advanced TradingMetricsCallback to optimize for risk-adjusted returns.\n",
        "    \"\"\"\n",
        "    global logger, TUNER_OBJECTIVE_METRIC, EPOCHS, ES_PATIENCE, BATCH_SIZE, LOOKBACK_WINDOW, CLASS_LABELS, CONFIDENCE_THRESHOLD_TRADE, SL_ATR_MULTIPLIER_DEFAULT, TP_ATR_MULTIPLIER_DEFAULT, BACKTEST_TRANSACTION_COST_PCT, live_states_by_symbol, SIMULATION_INITIAL_CAPITAL, RISK_FREE_RATE, L2_REG_STRENGTH, DROPOUT_RATE, INITIAL_LEARNING_RATE, WEIGHT_DECAY, XLA_ENABLED, model_builder_for_tuner_adv, create_sequences_tf_data_classification, TUNING_RESULTS_DIR, TUNER_PROJECT_NAME_TEMPLATE, TUNER_MAX_TRIALS\n",
        "\n",
        "    logger.info(f\"--- Starting Sync Hyperparameter Tuning for {symbol_name} (Objective: {TUNER_OBJECTIVE_METRIC}) ---\")\n",
        "    scaler_tune, label_encoder_tune = MinMaxScaler(), LabelEncoder()\n",
        "    train_df_scaled = train_df_unscaled.copy(); train_df_scaled[feature_columns_sym] = scaler_tune.fit_transform(train_df_unscaled[feature_columns_sym])\n",
        "    val_df_scaled = val_df_unscaled.copy(); val_df_scaled[feature_columns_sym] = scaler_tune.transform(val_df_unscaled[feature_columns_sym])\n",
        "    train_df_scaled['target_encoded'] = label_encoder_tune.fit_transform(train_df_unscaled['target_raw'])\n",
        "    val_df_scaled['target_encoded'] = label_encoder_tune.transform(val_df_unscaled['target_raw'])\n",
        "    train_ds_tune = create_sequences_tf_data_classification(train_df_scaled, feature_columns_sym, 'target_encoded', input_shape_sym[0], BATCH_SIZE, True)\n",
        "    val_ds_tune_for_tuner_search = create_sequences_tf_data_classification(val_df_scaled, feature_columns_sym, 'target_encoded', input_shape_sym[0], BATCH_SIZE, False)\n",
        "    if not train_ds_tune or not val_ds_tune_for_tuner_search: logger.error(f\"Tuning {symbol_name}: Dataset creation failed.\"); return None\n",
        "\n",
        "    tuner_dir = os.path.join(TUNING_RESULTS_DIR, f\"tuner_{symbol_name.upper()}\"); os.makedirs(tuner_dir, exist_ok=True)\n",
        "    objective_direction = 'max' if any(metric in TUNER_OBJECTIVE_METRIC for metric in ['sharpe', 'profit', 'pnl', 'acc', 'win_rate']) else 'min'\n",
        "    tuner = kt.Hyperband(hypermodel=lambda hp: model_builder_for_tuner_adv(hp, input_shape_sym, num_classes_sym, L2_REG_STRENGTH, DROPOUT_RATE, INITIAL_LEARNING_RATE, WEIGHT_DECAY, XLA_ENABLED),\n",
        "                         objective=kt.Objective(TUNER_OBJECTIVE_METRIC, direction=objective_direction), max_epochs=EPOCHS, factor=3, directory=tuner_dir, project_name=TUNER_PROJECT_NAME_TEMPLATE.format(symbol=symbol_name.upper()), overwrite=True)\n",
        "\n",
        "    class_weights_dict = dict(zip(np.unique(train_df_scaled['target_encoded']), class_weight.compute_class_weight('balanced', classes=np.unique(train_df_scaled['target_encoded']), y=train_df_scaled['target_encoded'])))\n",
        "\n",
        "    sl_mult_tune = live_states_by_symbol.get(symbol_name.upper(), {}).get('current_sl_atr_multiplier', SL_ATR_MULTIPLIER_DEFAULT)\n",
        "    tp_mult_tune = live_states_by_symbol.get(symbol_name.upper(), {}).get('current_tp_atr_multiplier', TP_ATR_MULTIPLIER_DEFAULT)\n",
        "    metrics_cb_tune = TradingMetricsCallback(\n",
        "        val_df_unscaled, symbol_name, LOOKBACK_WINDOW, feature_columns_sym,\n",
        "        scaler_tune, label_encoder_tune, BATCH_SIZE, CLASS_LABELS,\n",
        "        CONFIDENCE_THRESHOLD_TRADE, sl_mult_tune, tp_mult_tune,\n",
        "        BACKTEST_TRANSACTION_COST_PCT, SIMULATION_INITIAL_CAPITAL, RISK_FREE_RATE,\n",
        "        slippage_pct=SLIPPAGE_PCT\n",
        "    )\n",
        "    tuner_callbacks = [metrics_cb_tune, EarlyStopping(monitor=TUNER_OBJECTIVE_METRIC, patience=max(3, ES_PATIENCE // 2), mode=objective_direction, restore_best_weights=True, verbose=1)]\n",
        "\n",
        "    try:\n",
        "        tuner.search(train_ds_tune, epochs=EPOCHS, validation_data=val_ds_tune_for_tuner_search, callbacks=tuner_callbacks, class_weight=class_weights_dict, verbose=1)\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        logger.info(f\"Best HPs for {symbol_name}:\")\n",
        "        for hp_name, hp_value in best_hps.values.items(): logger.info(f\"  - {hp_name}: {hp_value}\")\n",
        "        return best_hps\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during KerasTuner search for {symbol_name}: {e}\", exc_info=True); return None\n",
        "\n",
        "\n",
        "def train_single_model_instance(\n",
        "    symbol_name: str, train_df_unscaled: pd.DataFrame, val_df_unscaled: pd.DataFrame,\n",
        "    feature_columns_sym: list[str], input_shape_sym: tuple, num_classes_sym: int,\n",
        "    model_save_path: str, scaler_save_path: str, encoder_save_path: str,\n",
        "    hps_for_model: Optional[kt.HyperParameters] = None\n",
        ") -> Optional[KerasModel]:\n",
        "    \"\"\"Trains, evaluates, and saves a single model instance, now with unified callback monitoring.\"\"\"\n",
        "    global logger, BATCH_SIZE, EPOCHS, ES_MONITOR, ES_PATIENCE, ES_RESTORE_BEST, RLP_MONITOR, RLP_FACTOR, RLP_PATIENCE, RLP_MIN_LR, CLASS_LABELS, CONFIDENCE_THRESHOLD_TRADE, SL_ATR_MULTIPLIER_DEFAULT, TP_ATR_MULTIPLIER_DEFAULT, BACKTEST_TRANSACTION_COST_PCT, live_states_by_symbol, LOOKBACK_WINDOW, L2_REG_STRENGTH, DROPOUT_RATE, INITIAL_LEARNING_RATE, WEIGHT_DECAY, XLA_ENABLED, build_advanced_model, create_sequences_tf_data_classification, SIMULATION_INITIAL_CAPITAL, RISK_FREE_RATE\n",
        "\n",
        "    scaler = MinMaxScaler(); label_encoder = LabelEncoder()\n",
        "    train_df_scaled = train_df_unscaled.copy(); train_df_scaled[feature_columns_sym] = scaler.fit_transform(train_df_unscaled[feature_columns_sym])\n",
        "    val_df_scaled = val_df_unscaled.copy(); val_df_scaled[feature_columns_sym] = scaler.transform(val_df_unscaled[feature_columns_sym])\n",
        "    train_df_scaled['target_encoded'] = label_encoder.fit_transform(train_df_unscaled['target_raw'])\n",
        "    val_df_scaled['target_encoded'] = label_encoder.transform(val_df_unscaled['target_raw'])\n",
        "    train_ds = create_sequences_tf_data_classification(train_df_scaled, feature_columns_sym, 'target_encoded', input_shape_sym[0], BATCH_SIZE, True)\n",
        "    val_ds_for_fit = create_sequences_tf_data_classification(val_df_scaled, feature_columns_sym, 'target_encoded', input_shape_sym[0], BATCH_SIZE, False)\n",
        "\n",
        "    model_instance = build_advanced_model(hp=hps_for_model, input_shape=input_shape_sym, num_classes=num_classes_sym, cfg_l2_strength=L2_REG_STRENGTH, cfg_dropout_rate=DROPOUT_RATE, cfg_initial_learning_rate=INITIAL_LEARNING_RATE, cfg_weight_decay=WEIGHT_DECAY, cfg_xla_enabled=XLA_ENABLED, cfg_keras_tuner_enabled=(hps_for_model is not None))\n",
        "    class_weights_dict = dict(zip(np.unique(train_df_scaled['target_encoded']), class_weight.compute_class_weight('balanced', classes=np.unique(train_df_scaled['target_encoded']), y=train_df_scaled['target_encoded'])))\n",
        "\n",
        "    cb_monitor = ES_MONITOR\n",
        "    cb_mode = 'max' if any(metric in cb_monitor for metric in ['sharpe', 'profit', 'pnl', 'acc', 'win_rate']) else 'min'\n",
        "\n",
        "    # --- - Determine RLP mode based on its specific monitor metric ---\n",
        "    rlp_monitor_metric = RLP_MONITOR\n",
        "    rlp_cb_mode = 'max' if any(metric in rlp_monitor_metric for metric in ['sharpe', 'profit', 'pnl', 'acc', 'win_rate']) else 'min'\n",
        "\n",
        "    sl_mult = live_states_by_symbol.get(symbol_name.upper(), {}).get('current_sl_atr_multiplier', SL_ATR_MULTIPLIER_DEFAULT)\n",
        "    tp_mult = live_states_by_symbol.get(symbol_name.upper(), {}).get('current_tp_atr_multiplier', TP_ATR_MULTIPLIER_DEFAULT)\n",
        "    metrics_cb = TradingMetricsCallback(\n",
        "        val_df_unscaled, symbol_name, LOOKBACK_WINDOW, feature_columns_sym,\n",
        "        scaler, label_encoder, BATCH_SIZE, CLASS_LABELS, CONFIDENCE_THRESHOLD_TRADE,\n",
        "        sl_mult, tp_mult, BACKTEST_TRANSACTION_COST_PCT, SIMULATION_INITIAL_CAPITAL,\n",
        "        RISK_FREE_RATE,\n",
        "        # --- - Add the missing slippage_pct argument here as well ---\n",
        "        slippage_pct=SLIPPAGE_PCT\n",
        "    )\n",
        "    callbacks = [\n",
        "        metrics_cb,\n",
        "        ModelCheckpoint(filepath=model_save_path, monitor=cb_monitor, save_best_only=True, mode=cb_mode, verbose=1),\n",
        "        EarlyStopping(monitor=cb_monitor, patience=ES_PATIENCE, restore_best_weights=ES_RESTORE_BEST, mode=cb_mode, verbose=1),\n",
        "        # --- - RLP now monitors the unified metric with the correct mode ---\n",
        "        ReduceLROnPlateau(monitor=rlp_monitor_metric, factor=RLP_FACTOR, patience=RLP_PATIENCE, min_lr=RLP_MIN_LR, mode=rlp_cb_mode, verbose=1)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        history = model_instance.fit(train_ds, epochs=EPOCHS, validation_data=val_ds_for_fit, callbacks=callbacks, class_weight=class_weights_dict, verbose=1)\n",
        "        best_epoch = np.argmax(history.history[cb_monitor]) if cb_mode == 'max' else np.argmin(history.history[cb_monitor])\n",
        "        best_score = history.history[cb_monitor][best_epoch]\n",
        "        logger.info(f\"Training finished for {os.path.basename(model_save_path)}. Best {cb_monitor}: {best_score:.4f} at epoch {best_epoch+1}.\")\n",
        "        final_model = model_instance\n",
        "        if ES_RESTORE_BEST and os.path.exists(model_save_path): final_model = keras_load_model(model_save_path)\n",
        "        joblib.dump(scaler, scaler_save_path); joblib.dump(label_encoder, encoder_save_path)\n",
        "        return final_model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during model.fit for {symbol_name}: {e}\", exc_info=True); return None\n",
        "\n",
        "\n",
        "async def run_standalone_tuning_pipeline(symbols_to_tune: List[str]):\n",
        "    \"\"\"Orchestrates the hyperparameter tuning pipeline for a list of symbols.\"\"\"\n",
        "    global KERAS_TUNER_ENABLED, logger\n",
        "    if not KERAS_TUNER_ENABLED: logger.info(\"KerasTuner disabled.\"); return\n",
        "    logger.info(f\"--- Starting Standalone Hyperparameter Tuning for: {', '.join(symbols_to_tune)} ---\")\n",
        "    for symbol_name in symbols_to_tune:\n",
        "        await _run_pipeline_for_single_symbol(symbol_name, mode='tune')\n",
        "\n",
        "\n",
        "async def run_adv_training_pipeline(symbols_to_train: List[str]):\n",
        "    \"\"\"Orchestrates the advanced model training pipeline for a list of symbols.\"\"\"\n",
        "    logger.info(f\"--- Starting Advanced Model Training Pipeline for: {', '.join(symbols_to_train)} ---\")\n",
        "    for symbol_name in symbols_to_train:\n",
        "        await _run_pipeline_for_single_symbol(symbol_name, mode='train')\n",
        "\n",
        "\n",
        "async def _run_pipeline_for_single_symbol(symbol_name: str, mode: str):\n",
        "    \"\"\"A unified helper to run the data prep and execution for either tuning or training.\"\"\"\n",
        "    global logger, data_store_by_symbol, best_hyperparameters_by_symbol, TARGET_INTERVAL, LOOKBACK_WINDOW, CLASS_LABELS, TEST_RATIO, MODELS_ARTEFACTS_DIR, MODEL_BASE_FILENAME, WALK_FORWARD_VALIDATION_ENABLED, N_SPLITS_WALK_FORWARD, ENSEMBLE_ENABLED, N_ENSEMBLE_MODELS_CONFIG, load_and_preprocess_data_for_symbol, adapt_strategy_parameters_for_symbol, send_telegram_message\n",
        "\n",
        "    try:\n",
        "        sym_upper = symbol_name.upper()\n",
        "        logger.info(f\"--- [{mode.upper()}] Processing Symbol: {sym_upper} ---\")\n",
        "        await adapt_strategy_parameters_for_symbol(sym_upper)\n",
        "\n",
        "        processed_df, feature_cols = await load_and_preprocess_data_for_symbol(\n",
        "            symbol_name,\n",
        "            TARGET_INTERVAL,\n",
        "        )\n",
        "        if processed_df is None or not feature_cols:\n",
        "            logger.error(f\"Data prep failed for {sym_upper}. Skipping {mode}.\")\n",
        "            await send_telegram_message(f\"‚ö†Ô∏è Data prep failed for {sym_upper}, {mode} skipped.\")\n",
        "            return\n",
        "\n",
        "        data_store_by_symbol[sym_upper] = {'feature_columns': feature_cols, 'processed_ohlcv_df': processed_df}\n",
        "        input_shape, num_classes = (LOOKBACK_WINDOW, len(feature_cols)), len(CLASS_LABELS)\n",
        "\n",
        "        if mode == 'tune':\n",
        "            train_df, val_df = train_test_split(processed_df, test_size=TEST_RATIO, shuffle=False)\n",
        "            tuned_hps = await asyncio.to_thread(run_hyperparameter_tuning_for_symbol_sync, symbol_name, train_df, val_df, feature_cols, input_shape, num_classes)\n",
        "            if tuned_hps:\n",
        "                best_hyperparameters_by_symbol[sym_upper] = tuned_hps\n",
        "                hp_path = os.path.join(MODELS_ARTEFACTS_DIR, f\"{MODEL_BASE_FILENAME}_{sym_upper}_best_hyperparameters.json\")\n",
        "                with open(hp_path, 'w') as f: json.dump(tuned_hps.get_config(), f, indent=4)\n",
        "                logger.info(f\"Best HPs for {sym_upper} saved.\"); await send_telegram_message(f\"‚öôÔ∏è Tuning complete for {sym_upper}. HPs saved.\")\n",
        "            else:\n",
        "                logger.warning(f\"Tuning failed for {sym_upper}.\"); await send_telegram_message(f\"‚ö†Ô∏è Tuning FAILED for {sym_upper}.\")\n",
        "\n",
        "        elif mode == 'train':\n",
        "            hps = best_hyperparameters_by_symbol.get(sym_upper)\n",
        "            if not hps: # Attempt to load from file if not in memory\n",
        "                hp_path = os.path.join(MODELS_ARTEFACTS_DIR, f\"{MODEL_BASE_FILENAME}_{sym_upper}_best_hyperparameters.json\")\n",
        "                if os.path.exists(hp_path):\n",
        "                    with open(hp_path, 'r') as f: hps = kt.HyperParameters.from_config(json.load(f))\n",
        "                    best_hyperparameters_by_symbol[sym_upper] = hps; logger.info(f\"Loaded HPs for {sym_upper} from file.\")\n",
        "                else:\n",
        "                    logger.info(f\"No tuned HPs for {sym_upper}. Using default model params.\")\n",
        "\n",
        "            symbol_artefacts = []\n",
        "            if WALK_FORWARD_VALIDATION_ENABLED and N_SPLITS_WALK_FORWARD > 0:\n",
        "                tscv = TimeSeriesSplit(n_splits=N_SPLITS_WALK_FORWARD)\n",
        "                for fold_num, (train_idx, val_idx) in enumerate(tscv.split(processed_df)):\n",
        "                    m,s,e = [get_symbol_specific_file_path_template(MODELS_ARTEFACTS_DIR, MODEL_BASE_FILENAME, sym_upper, t, f\"fold{fold_num+1}\", x) for t,x in [(\"model\",\"keras\"),(\"scaler\",\"pkl\"),(\"encoder\",\"pkl\")]]\n",
        "                    model = await asyncio.to_thread(train_single_model_instance, sym_upper, processed_df.iloc[train_idx], processed_df.iloc[val_idx], feature_cols, input_shape, num_classes, m,s,e, hps)\n",
        "                    if model: symbol_artefacts.append({'model_path':m,'scaler_path':s,'encoder_path':e,'fold_num_or_id':f\"fold{fold_num+1}\",'model_object':model})\n",
        "                if not ENSEMBLE_ENABLED and symbol_artefacts: symbol_artefacts = [symbol_artefacts[-1]]\n",
        "            else: # Standard Split / Ensemble\n",
        "                train_df, val_df = train_test_split(processed_df, test_size=TEST_RATIO, shuffle=False)\n",
        "                num_models = N_ENSEMBLE_MODELS_CONFIG if ENSEMBLE_ENABLED and N_ENSEMBLE_MODELS_CONFIG > 0 else 1\n",
        "                for i in range(1, num_models + 1):\n",
        "                    mem_id = f\"member{i}\" if num_models > 1 else \"main\"\n",
        "                    m,s,e = [get_symbol_specific_file_path_template(MODELS_ARTEFACTS_DIR, MODEL_BASE_FILENAME, sym_upper, t, mem_id, x) for t,x in [(\"model\",\"keras\"),(\"scaler\",\"pkl\"),(\"encoder\",\"pkl\")]]\n",
        "                    model = await asyncio.to_thread(train_single_model_instance, sym_upper, train_df, val_df, feature_cols, input_shape, num_classes, m,s,e, hps)\n",
        "                    if model: symbol_artefacts.append({'model_path':m,'scaler_path':s,'encoder_path':e,'fold_num_or_id':mem_id,'model_object':model})\n",
        "\n",
        "            if symbol_artefacts:\n",
        "                trained_models_by_symbol[sym_upper] = symbol_artefacts\n",
        "                logger.info(f\"Successfully trained {len(symbol_artefacts)} model(s) for {sym_upper}.\")\n",
        "                await send_telegram_message(f\"‚úÖ Model training complete for {sym_upper} ({len(symbol_artefacts)} instance(s)).\")\n",
        "                if sym_upper in live_states_by_symbol and live_states_by_symbol[sym_upper].get('is_halted_for_performance'):\n",
        "                    live_states_by_symbol[sym_upper]['is_halted_for_performance'] = False\n",
        "                    await send_telegram_message(f\"üëç SYMBOL RE-ENABLED: {sym_upper} has been successfully retrained.\")\n",
        "            else:\n",
        "                logger.error(f\"--- Training FAILED for {sym_upper}. No models were trained. ---\")\n",
        "                await send_telegram_message(f\"‚ùå Model training FAILED for {sym_upper}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unhandled exception in pipeline for {symbol_name} (mode: {mode}): {e}\", exc_info=True)\n",
        "        await send_telegram_message(f\"üö® Critical error in {mode} pipeline for {symbol_name}.\")\n",
        "    finally:\n",
        "        await asyncio.sleep(1) # Small delay to prevent resource exhaustion\n",
        "\n",
        "logger.info(\"Cell 6: Fully enhanced model training and tuning pipeline functions are defined and ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCe-zogVuBL5",
        "outputId": "1096f7d2-c153-45dc-c76d-1cd67d5faca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 7: Backtesting Pipeline (Enhanced with Advanced Metrics)\n",
            "2025-06-17 17:49:05 - TradingBotLogger - INFO - [<ipython-input-9-3915582482>.<cell line: 0>:380] - Cell 7: Backtesting Pipeline functions enhanced with Sortino and Calmar ratios.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 7: Backtesting Pipeline (Symbol-Specific & Adaptive) ---\n",
        "\n",
        "print(\"\\nInitializing Cell 7: Backtesting Pipeline (Enhanced with Advanced Metrics)\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "import joblib\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import time\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model as keras_load_model, Model as KerasModel\n",
        "\n",
        "# --- Matplotlib for plotting (if available, checked from Cell 1) ---\n",
        "if 'MATPLOTLIB_AVAILABLE' not in globals(): MATPLOTLIB_AVAILABLE = False; plt = None\n",
        "elif 'plt' not in globals() and MATPLOTLIB_AVAILABLE:\n",
        "    try:\n",
        "        import matplotlib\n",
        "        import matplotlib.pyplot as plt\n",
        "        if hasattr(matplotlib, 'use'):\n",
        "            matplotlib.use('Agg')\n",
        "    except ImportError:\n",
        "        MATPLOTLIB_AVAILABLE = False; plt = None\n",
        "\n",
        "# --- Ensure necessary variables and functions from previous cells are available ---\n",
        "if 'logger' not in globals():\n",
        "    logger = logging.getLogger(\"TradingBotLogger_C7_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c7 = logging.StreamHandler(sys.stdout)\n",
        "        _ch_c7.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - C7_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c7); logger.setLevel(logging.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 7.\")\n",
        "\n",
        "if 'get_symbol_specific_file_path_template' not in globals():\n",
        "    def get_symbol_specific_file_path_template(base_dir: str, base_fn: str, sym: str, s_type: str, f_id: str, ext: str) -> str:\n",
        "        filename = f\"{base_fn}_{sym.upper()}_{s_type}_{str(f_id).replace(' ', '_')}.{ext}\"\n",
        "        return os.path.join(base_dir, filename)\n",
        "    logger.warning(\"get_symbol_specific_file_path_template not found globally. Using fallback for Cell 7.\")\n",
        "\n",
        "# Default values for backtesting parameters\n",
        "config_defaults_c7 = {\n",
        "    'LOOKBACK_WINDOW': 60, 'BATCH_SIZE': 32, 'CLASS_LABELS': {0:'BUY',1:'HOLD',2:'SELL'},\n",
        "    'MC_DROPOUT_SAMPLES': 20, 'CONFIDENCE_THRESHOLD_TRADE': 0.90,\n",
        "    'TP_ATR_MULTIPLIER_DEFAULT': 1.5, 'SL_ATR_MULTIPLIER_DEFAULT': 0.75,\n",
        "    'BACKTEST_TRANSACTION_COST_PCT': 0.0007, 'MARGIN_UTILIZATION_PERCENT': 0.92,\n",
        "    ## --- - Add SLIPPAGE_PCT to the default configuration for the backtester. ---\n",
        "    'SLIPPAGE_PCT': 0.0005, # Represents 0.05% slippage on trades\n",
        "    'UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER': 5.0,\n",
        "    'MODELS_ARTEFACTS_DIR': './models', 'MODEL_BASE_FILENAME': 'trading_model',\n",
        "    'WALK_FORWARD_VALIDATION_ENABLED': True, 'N_SPLITS_WALK_FORWARD': 5, 'TEST_RATIO': 0.2,\n",
        "    'ENSEMBLE_ENABLED': False, 'N_ENSEMBLE_MODELS_CONFIG': 3,\n",
        "    'RISK_FREE_RATE': 0.07,\n",
        "    'data_store_by_symbol': {}, 'trained_models_by_symbol': {}, 'live_states_by_symbol': {},\n",
        "}\n",
        "for param_c7, default_val_c7 in config_defaults_c7.items():\n",
        "    if param_c7 not in globals(): globals()[param_c7] = default_val_c7\n",
        "\n",
        "if 'create_sequences_tf_data_classification' not in globals():\n",
        "    globals()['create_sequences_tf_data_classification'] = lambda *args, **kwargs: None; logger.warning(\"create_sequences_tf_data_classification (Cell 4) used.\")\n",
        "\n",
        "def calculate_max_drawdown(equity_curve_series: pd.Series) -> float:\n",
        "    \"\"\"Calculates the maximum drawdown from an equity curve pandas Series.\"\"\"\n",
        "    if equity_curve_series.empty or len(equity_curve_series) < 2: return 0.0\n",
        "    equity_curve_filled = equity_curve_series.ffill().bfill()\n",
        "    if equity_curve_filled.nunique() <= 1: return 0.0\n",
        "    running_max = equity_curve_filled.cummax()\n",
        "    drawdown_values = (equity_curve_filled - running_max) / running_max.replace(0, np.nan)\n",
        "    max_dd = drawdown_values.min()\n",
        "    return abs(max_dd) if pd.notna(max_dd) and max_dd < 0 else 0.0\n",
        "\n",
        "def calculate_dynamic_order_quantity_backtest(\n",
        "    stock_price: float, current_equity: float, leverage_multiplier: float,\n",
        "    margin_util_pct: float, position_size_pct_of_equity: float = 0.92\n",
        ") -> int:\n",
        "    \"\"\" Calculates order quantity for backtesting based on current equity and risk parameters. \"\"\"\n",
        "    if stock_price <= 0 or current_equity <= 0: return 0\n",
        "    capital_for_this_trade = current_equity * position_size_pct_of_equity\n",
        "    actual_margin_to_use = capital_for_this_trade * margin_util_pct\n",
        "    effective_buying_power = actual_margin_to_use * leverage_multiplier\n",
        "    quantity = int(np.floor(effective_buying_power / stock_price))\n",
        "    return max(0, quantity)\n",
        "\n",
        "def _simulate_trades_on_data(\n",
        "    sim_df_unscaled: pd.DataFrame, pred_probs_for_sim_df: np.ndarray, start_equity: float,\n",
        "    fold_id_logging: str, symbol_name_sim: str, class_labels_sim: Dict[int, str],\n",
        "    lookback_sim: int, confidence_thresh_sim: float, sl_atr_mult_sim: float,\n",
        "    tp_atr_mult_sim: float, transaction_cost_pct_sim: float, leverage_mult_sim: float,\n",
        "    margin_util_sim: float,\n",
        "    ## --- - Add slippage_pct_sim to the function signature. ---\n",
        "    slippage_pct_sim: float\n",
        ") -> Tuple[List[Dict[str, Any]], pd.Series, Optional[pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Helper function to simulate trades on a given segment of data.\n",
        "    Returns: list of trade dicts, equity curve Series, and the simulation df with predictions.\n",
        "    \"\"\"\n",
        "    local_trades_list: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Align simulation data with predictions\n",
        "    sim_df_aligned = sim_df_unscaled.iloc[lookback_sim - 1 : lookback_sim - 1 + len(pred_probs_for_sim_df)].copy()\n",
        "    if len(sim_df_aligned) != len(pred_probs_for_sim_df):\n",
        "        logger.warning(f\"BT Sim {symbol_name_sim} {fold_id_logging}: Prediction/data length mismatch. Truncating.\")\n",
        "        min_len = min(len(sim_df_aligned), len(pred_probs_for_sim_df))\n",
        "        sim_df_aligned = sim_df_aligned.head(min_len)\n",
        "        pred_probs_for_sim_df = pred_probs_for_sim_df[:min_len]\n",
        "\n",
        "    if sim_df_aligned.empty: return [], pd.Series([start_equity]), None\n",
        "\n",
        "    # Add predictions to the simulation dataframe\n",
        "    sim_df_aligned['predicted_signal_idx'] = np.argmax(pred_probs_for_sim_df, axis=1)\n",
        "    sim_df_aligned['predicted_signal'] = sim_df_aligned['predicted_signal_idx'].map(lambda x: class_labels_sim.get(x, \"UNKNOWN\"))\n",
        "    sim_df_aligned['prediction_confidence'] = np.max(pred_probs_for_sim_df, axis=1)\n",
        "\n",
        "    # Initialize equity curve\n",
        "    equity_curve = pd.Series(index=sim_df_aligned.index, dtype=float)\n",
        "    current_equity = start_equity\n",
        "\n",
        "    current_pos, entry_p, current_sl, current_tp, shares_held, entry_ts, entry_conf = 'None', 0.0, 0.0, 0.0, 0, None, 0.0\n",
        "\n",
        "    for idx, (s_dt, row) in enumerate(sim_df_aligned.iterrows()):\n",
        "        s_h, s_l, s_c, s_atr = row['high'], row['low'], row['close'], row['atr']\n",
        "        if pd.isna(s_atr) or s_atr <= 1e-7: s_atr = s_c * 0.015 + 1e-7\n",
        "\n",
        "        # --- Exit Logic (CORRECTED PESSIMISTIC VERSION) ---\n",
        "        if current_pos != 'None':\n",
        "            exit_trig, exit_rsn, exit_prc_sim = False, None, s_c\n",
        "\n",
        "            # --- - PESSIMISTIC EXIT LOGIC ---\n",
        "            # For a given candle, we must check if both SL and TP could have been hit.\n",
        "            # If so, we pessimistically assume the SL was hit first.\n",
        "\n",
        "            sl_was_hit = False\n",
        "            tp_was_hit = False\n",
        "\n",
        "            if current_pos == 'Long':\n",
        "                if s_l <= current_sl: sl_was_hit = True\n",
        "                if s_h >= current_tp: tp_was_hit = True\n",
        "            elif current_pos == 'Short':\n",
        "                if s_h >= current_sl: sl_was_hit = True\n",
        "                if s_l <= current_tp: tp_was_hit = True\n",
        "\n",
        "            # Now, determine the outcome based on a pessimistic priority: SL > TP\n",
        "            if sl_was_hit:\n",
        "                # If the SL was hit (regardless of the TP), we exit at the SL price.\n",
        "                exit_trig, exit_rsn, exit_prc_sim = True, \"SL_HIT\", current_sl\n",
        "            elif tp_was_hit:\n",
        "                # If only the TP was hit, we exit at the TP price.\n",
        "                exit_trig, exit_rsn, exit_prc_sim = True, \"TP_HIT\", current_tp\n",
        "\n",
        "            # Check for End-of-Day exit if no SL/TP was triggered\n",
        "            if not exit_trig and idx == len(sim_df_aligned) - 1:\n",
        "                exit_trig, exit_rsn = True, f\"EOD_{fold_id_logging}\"\n",
        "\n",
        "            if exit_trig:\n",
        "                # --- (P&L calculation logic remains the same) ---\n",
        "\n",
        "                ## the simulated exit price before calculating P&L.\n",
        "                if current_pos == 'Long': # Exiting by selling, price is worse (lower)\n",
        "                    exit_prc_sim = exit_prc_sim * (1 - slippage_pct_sim)\n",
        "                else: # Exiting a short by buying, price is worse (higher)\n",
        "                    exit_prc_sim = exit_prc_sim * (1 + slippage_pct_sim)\n",
        "\n",
        "                pnl_gross = (exit_prc_sim - entry_p if current_pos == 'Long' else entry_p - exit_prc_sim) * shares_held\n",
        "                cost_trade = (abs(entry_p * shares_held) + abs(exit_prc_sim * shares_held)) * transaction_cost_pct_sim\n",
        "                pnl_net = pnl_gross - cost_trade\n",
        "                current_equity += pnl_net\n",
        "\n",
        "                local_trades_list.append({'EntryTime': entry_ts, 'ExitTime': s_dt, 'Symbol': symbol_name_sim, 'PositionType': current_pos,'EntryPrice': entry_p, 'ExitPrice': exit_prc_sim, 'Shares': shares_held, 'ExitReason': exit_rsn,'GrossPnL': pnl_gross, 'NetPnL': pnl_net, 'EntryConfidence': entry_conf,'EquityAfterTrade': current_equity, 'Fold': fold_id_logging})\n",
        "                current_pos, shares_held = 'None', 0\n",
        "\n",
        "        # --- Entry Logic ---\n",
        "        if current_pos == 'None' and idx < len(sim_df_aligned) - 1 and row['prediction_confidence'] >= confidence_thresh_sim:\n",
        "            action_to_take = 'Long' if row['predicted_signal'] == 'BUY' else ('Short' if row['predicted_signal'] == 'SELL' else None)\n",
        "            if action_to_take:\n",
        "                entry_p_raw = s_c\n",
        "\n",
        "                if action_to_take == 'Long': # Buying, price is worse (higher)\n",
        "                    entry_p = entry_p_raw * (1 + slippage_pct_sim)\n",
        "                else: # Selling short, price is worse (lower)\n",
        "                    entry_p = entry_p_raw * (1 - slippage_pct_sim)\n",
        "\n",
        "                shares_held = calculate_dynamic_order_quantity_backtest(entry_p, current_equity, leverage_mult_sim, margin_util_sim)\n",
        "                if shares_held > 0:\n",
        "                    current_pos, entry_ts, entry_conf = action_to_take, s_dt, row['prediction_confidence']\n",
        "                    sl_dist, tp_dist = s_atr * sl_atr_mult_sim, s_atr * tp_atr_mult_sim\n",
        "                    current_sl, current_tp = (entry_p - sl_dist, entry_p + tp_dist) if current_pos == 'Long' else (entry_p + sl_dist, entry_p - tp_dist)\n",
        "\n",
        "        equity_curve.at[s_dt] = current_equity\n",
        "\n",
        "    return local_trades_list, equity_curve, sim_df_aligned\n",
        "\n",
        "\n",
        "def run_backtest_for_single_symbol(symbol_name: str, initial_capital: float = 50000.0) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Runs backtesting for a single symbol, now saving the detailed trade log\n",
        "    to the symbol's SQLite database instead of a CSV file.\n",
        "    \"\"\"\n",
        "    # --- - Added write_df_to_db to the list of global variables ---\n",
        "    global logger, data_store_by_symbol, trained_models_by_symbol, live_states_by_symbol, CLASS_LABELS, LOOKBACK_WINDOW, BATCH_SIZE, MODELS_ARTEFACTS_DIR, MC_DROPOUT_SAMPLES, CONFIDENCE_THRESHOLD_TRADE, TP_ATR_MULTIPLIER_DEFAULT, SL_ATR_MULTIPLIER_DEFAULT, BACKTEST_TRANSACTION_COST_PCT, MARGIN_UTILIZATION_PERCENT, UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER, ENSEMBLE_ENABLED, N_ENSEMBLE_MODELS_CONFIG, WALK_FORWARD_VALIDATION_ENABLED, N_SPLITS_WALK_FORWARD, TEST_RATIO, MATPLOTLIB_AVAILABLE, plt, RISK_FREE_RATE, SLIPPAGE_PCT, write_df_to_db, create_sequences_tf_data_classification, _simulate_trades_on_data, calculate_max_drawdown\n",
        "\n",
        "    logger.info(f\"--- Starting Backtest for Symbol: {symbol_name} with Initial Capital: ‚Çπ{initial_capital:,.2f} ---\")\n",
        "    symbol_upper = symbol_name.upper()\n",
        "\n",
        "    # --- Data and Model loading ---\n",
        "    if symbol_upper not in data_store_by_symbol or not data_store_by_symbol.get(symbol_upper, {}).get('feature_columns'):\n",
        "        logger.error(f\"BT {symbol_upper}: Processed data/features not found. Preprocess (Cell 4) first.\"); return None\n",
        "    full_symbol_df = data_store_by_symbol[symbol_upper]['processed_ohlcv_df'].copy()\n",
        "    feature_columns_sym = data_store_by_symbol[symbol_upper]['feature_columns']\n",
        "    if 'atr' not in full_symbol_df.columns: full_symbol_df['atr'] = full_symbol_df['close'] * 0.015\n",
        "    full_symbol_df['atr'].fillna(method='ffill', inplace=True); full_symbol_df['atr'].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    if symbol_upper not in trained_models_by_symbol or not trained_models_by_symbol[symbol_upper]:\n",
        "        logger.error(f\"BT {symbol_upper}: No trained models found. Train (Cell 6) first.\"); return None\n",
        "    symbol_model_infos = trained_models_by_symbol[symbol_upper]\n",
        "\n",
        "    all_trades_list: List[Dict[str, Any]] = []\n",
        "    full_equity_curve = pd.Series(dtype=float)\n",
        "\n",
        "    # --- Simulation Logic ---\n",
        "    is_wfv = WALK_FORWARD_VALIDATION_ENABLED and any('fold' in str(info.get('fold_num_or_id','')).lower() for info in symbol_model_infos)\n",
        "\n",
        "    if is_wfv:\n",
        "        logger.info(f\"BT {symbol_upper}: WFV backtest ({N_SPLITS_WALK_FORWARD} folds).\")\n",
        "        tscv = TimeSeriesSplit(n_splits=N_SPLITS_WALK_FORWARD)\n",
        "        current_equity_fold_start = initial_capital\n",
        "\n",
        "        for fold_idx, (train_indices, val_indices) in enumerate(tscv.split(full_symbol_df)):\n",
        "            fold_id = f\"fold{fold_idx + 1}\"\n",
        "            logger.info(f\"--- BT {symbol_upper} - WFV {fold_id} ---\")\n",
        "            model_info = next((info for info in symbol_model_infos if str(info.get('fold_num_or_id','')) == fold_id), None)\n",
        "            if not model_info:\n",
        "                logger.error(f\"BT {symbol_upper}: Model for {fold_id} not found. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            val_df_unscaled = full_symbol_df.iloc[val_indices]\n",
        "            if val_df_unscaled.empty:\n",
        "                logger.warning(f\"BT {symbol_upper} {fold_id}: Validation fold is empty. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                model = keras_load_model(model_info['model_path'])\n",
        "                scaler = joblib.load(model_info['scaler_path'])\n",
        "                encoder = joblib.load(model_info['encoder_path'])\n",
        "\n",
        "                val_df_scaled = val_df_unscaled.copy()\n",
        "                val_df_scaled[feature_columns_sym] = scaler.transform(val_df_unscaled[feature_columns_sym])\n",
        "                val_df_scaled['target_encoded'] = encoder.transform(val_df_unscaled['target_raw'])\n",
        "                val_ds = create_sequences_tf_data_classification(val_df_scaled, feature_columns_sym, 'target_encoded', LOOKBACK_WINDOW, BATCH_SIZE, False)\n",
        "\n",
        "                if val_ds:\n",
        "                    preds = model.predict(val_ds, verbose=0)\n",
        "                    sl_mult = live_states_by_symbol.get(symbol_upper, {}).get('current_sl_atr_multiplier', SL_ATR_MULTIPLIER_DEFAULT)\n",
        "                    tp_mult = live_states_by_symbol.get(symbol_upper, {}).get('current_tp_atr_multiplier', TP_ATR_MULTIPLIER_DEFAULT)\n",
        "\n",
        "                    trades, equity_pts, _ = _simulate_trades_on_data(val_df_unscaled, preds, current_equity_fold_start, fold_id, symbol_upper, CLASS_LABELS, LOOKBACK_WINDOW, CONFIDENCE_THRESHOLD_TRADE, sl_mult, tp_mult, BACKTEST_TRANSACTION_COST_PCT, UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER, MARGIN_UTILIZATION_PERCENT, SLIPPAGE_PCT)\n",
        "\n",
        "                    if trades: all_trades_list.extend(trades)\n",
        "                    if not equity_pts.empty:\n",
        "                        if full_equity_curve.empty:\n",
        "                            full_equity_curve = equity_pts\n",
        "                        else:\n",
        "                            full_equity_curve = pd.concat([full_equity_curve, equity_pts])\n",
        "                        current_equity_fold_start = equity_pts.iloc[-1]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during WFV backtest for {symbol_upper} fold {fold_id}: {e}\", exc_info=True)\n",
        "\n",
        "    else: # Standard Split\n",
        "        logger.info(f\"BT {symbol_upper}: Standard backtest on hold-out set.\")\n",
        "        _, test_df_unscaled = train_test_split(full_symbol_df, test_size=TEST_RATIO, shuffle=False)\n",
        "        if not test_df_unscaled.empty:\n",
        "            model_info = symbol_model_infos[-1]\n",
        "            try:\n",
        "                model = keras_load_model(model_info['model_path'])\n",
        "                scaler = joblib.load(model_info['scaler_path'])\n",
        "                encoder = joblib.load(model_info['encoder_path'])\n",
        "                test_df_scaled = test_df_unscaled.copy()\n",
        "                test_df_scaled[feature_columns_sym] = scaler.transform(test_df_unscaled[feature_columns_sym])\n",
        "                test_df_scaled['target_encoded'] = encoder.transform(test_df_unscaled['target_raw'])\n",
        "                test_ds = create_sequences_tf_data_classification(test_df_scaled, feature_columns_sym, 'target_encoded', LOOKBACK_WINDOW, BATCH_SIZE, False)\n",
        "                if test_ds:\n",
        "                    preds = model.predict(test_ds, verbose=0)\n",
        "                    sl_mult = live_states_by_symbol.get(symbol_upper, {}).get('current_sl_atr_multiplier', SL_ATR_MULTIPLIER_DEFAULT)\n",
        "                    tp_mult = live_states_by_symbol.get(symbol_upper, {}).get('current_tp_atr_multiplier', TP_ATR_MULTIPLIER_DEFAULT)\n",
        "                    trades, equity_pts, _ = _simulate_trades_on_data(test_df_unscaled, preds, initial_capital, \"StandardTest\", symbol_upper, CLASS_LABELS, LOOKBACK_WINDOW, CONFIDENCE_THRESHOLD_TRADE, sl_mult, tp_mult, BACKTEST_TRANSACTION_COST_PCT, UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER, MARGIN_UTILIZATION_PERCENT, SLIPPAGE_PCT)\n",
        "                    if trades: all_trades_list.extend(trades)\n",
        "                    if not equity_pts.empty: full_equity_curve = equity_pts\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during standard backtest for {symbol_upper}: {e}\", exc_info=True)\n",
        "\n",
        "    # --- Performance Metrics & Reporting ---\n",
        "    final_metrics = {'symbol': symbol_upper, 'initial_capital': initial_capital, 'final_equity': initial_capital, 'total_trades': 0}\n",
        "    if not all_trades_list:\n",
        "        logger.info(f\"No trades executed for {symbol_upper}.\")\n",
        "    else:\n",
        "        trades_df = pd.DataFrame(all_trades_list)\n",
        "        num_trades, final_equity, total_pnl = len(trades_df), full_equity_curve.iloc[-1] if not full_equity_curve.empty else initial_capital, trades_df['NetPnL'].sum()\n",
        "        wins, losses = trades_df[trades_df['NetPnL'] > 0], trades_df[trades_df['NetPnL'] <= 0]\n",
        "        win_rate = len(wins) / num_trades if num_trades > 0 else 0.0\n",
        "        avg_win, avg_loss = wins['NetPnL'].mean() if not wins.empty else 0.0, losses['NetPnL'].mean() if not losses.empty else 0.0\n",
        "        profit_factor = abs(wins['NetPnL'].sum() / losses['NetPnL'].sum()) if not losses.empty and losses['NetPnL'].sum() != 0 else float('inf')\n",
        "\n",
        "        sharpe_ratio, sortino_ratio, calmar_ratio, max_dd = 0.0, 0.0, 0.0, 0.0\n",
        "        if not full_equity_curve.empty:\n",
        "            daily_returns = full_equity_curve.resample('D').last().ffill().pct_change().dropna()\n",
        "            if len(daily_returns) > 1:\n",
        "                annualized_return, annualized_std = ((1 + daily_returns.mean())**252 - 1), (daily_returns.std() * np.sqrt(252))\n",
        "                sharpe_ratio = (annualized_return - RISK_FREE_RATE) / annualized_std if annualized_std > 0 else 0.0\n",
        "                negative_daily_returns = daily_returns[daily_returns < 0]\n",
        "                annualized_downside_std = negative_daily_returns.std() * np.sqrt(252) if not negative_daily_returns.empty else 0\n",
        "                sortino_ratio = (annualized_return - RISK_FREE_RATE) / annualized_downside_std if annualized_downside_std > 0 else float('inf')\n",
        "                max_dd = calculate_max_drawdown(full_equity_curve)\n",
        "                calmar_ratio = annualized_return / max_dd if max_dd > 0 else float('inf')\n",
        "\n",
        "        final_metrics.update({'final_equity': final_equity, 'total_net_pnl': total_pnl, 'total_trades': num_trades, 'win_rate': win_rate, 'avg_win_pnl': avg_win, 'avg_loss_pnl': avg_loss, 'profit_factor': profit_factor, 'max_drawdown': max_dd, 'sharpe_ratio': sharpe_ratio, 'sortino_ratio': sortino_ratio, 'calmar_ratio': calmar_ratio})\n",
        "\n",
        "        logger.info(f\"\\n--- Backtest Results for {symbol_upper} (Costs: {BACKTEST_TRANSACTION_COST_PCT*100:.3f}%) ---\")\n",
        "        for k, v in final_metrics.items():\n",
        "            if isinstance(v, float) and k in ['win_rate', 'max_drawdown']: logger.info(f\"  {k.replace('_',' ').title():<18}: {v:.2%}\")\n",
        "            elif isinstance(v, float): logger.info(f\"  {k.replace('_',' ').title():<18}: {v:,.2f}\")\n",
        "            else: logger.info(f\"  {k.replace('_',' ').title():<18}: {v}\")\n",
        "\n",
        "        # --- - Save trade log to database instead of CSV ---\n",
        "        try:\n",
        "            # Ensure timestamp columns are in a string format compatible with SQLite\n",
        "            trades_df['EntryTime'] = pd.to_datetime(trades_df['EntryTime']).dt.isoformat()\n",
        "            trades_df['ExitTime'] = pd.to_datetime(trades_df['ExitTime']).dt.isoformat()\n",
        "\n",
        "            write_df_to_db(trades_df, 'backtest_logs', symbol_upper)\n",
        "            logger.info(f\"‚úÖ Backtest trade log saved to database for {symbol_upper}.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to save backtest trade log to database for {symbol_upper}: {e}\", exc_info=True)\n",
        "\n",
        "    # Plotting logic\n",
        "    if MATPLOTLIB_AVAILABLE and plt and not full_equity_curve.empty:\n",
        "        try:\n",
        "            plt.figure(figsize=(14, 7))\n",
        "            plt.plot(full_equity_curve.index, full_equity_curve.values, marker='.', linestyle='-', markersize=4, label='Equity')\n",
        "            plt.title(f'Equity Curve - {symbol_upper} (Initial Cap ‚Çπ{initial_capital:,.0f})')\n",
        "            plt.ylabel(\"Equity (‚Çπ)\"); plt.xlabel(\"Date\"); plt.grid(True); plt.tight_layout()\n",
        "            plot_path = os.path.join(MODELS_ARTEFACTS_DIR, f\"equity_curve_backtest_{symbol_upper}.png\")\n",
        "            os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
        "            plt.savefig(plot_path); plt.close()\n",
        "            logger.info(f\"Equity curve plot saved to: {plot_path}\")\n",
        "        except Exception as e: logger.error(f\"Error plotting equity curve for {symbol_upper}: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"--- Backtest Complete for Symbol: {symbol_upper} ---\")\n",
        "    return final_metrics\n",
        "\n",
        "def run_adv_backtesting_pipeline(symbols_to_backtest: list[str], initial_capital_per_symbol: float = 50000.0):\n",
        "    \"\"\" Orchestrates the backtesting pipeline for a list of specified symbols. Synchronous. \"\"\"\n",
        "    global logger\n",
        "    if not symbols_to_backtest: logger.info(\"No symbols for backtesting.\"); return\n",
        "    logger.info(f\"--- Starting Advanced Backtesting Pipeline for: {', '.join(symbols_to_backtest)} ---\")\n",
        "    all_metrics_list = []\n",
        "    for symbol_name_bt in symbols_to_backtest:\n",
        "        try:\n",
        "            metrics = run_backtest_for_single_symbol(symbol_name_bt, initial_capital_per_symbol)\n",
        "            if metrics: all_metrics_list.append(metrics)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unhandled exception during backtesting for {symbol_name_bt}: {e}\", exc_info=True)\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    if all_metrics_list:\n",
        "        logger.info(\"\\n--- === Overall Backtesting Summary (Per Symbol) === ---\")\n",
        "        summary_df = pd.DataFrame(all_metrics_list).set_index('symbol')\n",
        "        float_cols = summary_df.select_dtypes(include=np.number).columns\n",
        "        formatters = {col: ('{:.2%}'.format if 'rate' in col or 'drawdown' in col else '{:,.2f}'.format) for col in float_cols}\n",
        "        try: logger.info(f\"\\n{summary_df.to_string(formatters=formatters)}\")\n",
        "        except Exception as e: logger.error(f\"Error formatting summary: {e}. Printing raw.\"); logger.info(f\"\\n{summary_df.to_string()}\")\n",
        "    else:\n",
        "        logger.info(\"No symbols successfully backtested or no trades generated.\")\n",
        "\n",
        "    logger.info(f\"--- Advanced Backtesting Pipeline Complete ---\")\n",
        "\n",
        "logger.info(\"Cell 7: Backtesting Pipeline functions enhanced with Sortino and Calmar ratios.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73kUCySuGvE",
        "outputId": "ac43cbf0-e003-4f7b-ce53-9ad10ee686be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 8: Live Trading Loop (Enhanced with State Persistence, Caching, Rate Limiting & Supervisor Architecture)\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.<cell line: 0>:136] - Initialized API Rate Limiter: Capacity=10, Refill Rate=3/sec.\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.<cell line: 0>:1923] - Cell 8: Live Trading Loop functions have been fixed and refactored into a resilient supervisor/worker architecture.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 8: Live Trading Loop, API Interactions, and Realtime Processing ---\n",
        "\n",
        "print(\"\\nInitializing Cell 8: Live Trading Loop (Enhanced with State Persistence, Caching, Rate Limiting & Supervisor Architecture)\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import asyncio\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, time as datetime_time, date as datetime_date, timedelta\n",
        "import pytz\n",
        "import joblib\n",
        "import uuid\n",
        "import threading\n",
        "from typing import Union, List, Dict, Any, Optional, Tuple\n",
        "import collections\n",
        "import upstox_client\n",
        "import upstox_client.api.portfolio_api\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "from tensorflow.keras.models import load_model as keras_load_model, Model as KerasModel\n",
        "import tensorflow as tf\n",
        "import websocket\n",
        "\n",
        "# --- Ensure necessary variables and functions from previous cells are available ---\n",
        "# Logger (from Cell 1)\n",
        "if 'logger' not in globals():\n",
        "    import logging as pylogging_c8; import sys as pysys_c8 # Use alias\n",
        "    logger = pylogging_c8.getLogger(\"TradingBotLogger_C8_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c8 = pylogging_c8.StreamHandler(pysys_c8.stdout)\n",
        "        _ch_c8.setFormatter(pylogging_c8.Formatter('%(asctime)s - %(levelname)s - C8_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c8); logger.setLevel(pylogging_c8.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 8.\")\n",
        "\n",
        "# Define STATE_FILE_PATH constant from other_files_dir\n",
        "STATE_FILE_PATH = os.path.join(globals().get('OTHER_FILES_DIR', 'other_files'), 'live_bot_state.json')\n",
        "\n",
        "# send_telegram_message (defined in this cell, but provide fallback for early ref if needed)\n",
        "if 'send_telegram_message' not in globals():\n",
        "    async def send_telegram_message(msg_text_tg: str, chat_id_override_tg: Optional[str] = None) -> bool:\n",
        "        logger.info(f\"Telegram (mock_C8_early): {msg_text_tg}\")\n",
        "        return True\n",
        "\n",
        "# Helper from Cell 3 for min_periods in TA\n",
        "if '_get_min_periods_c3' not in globals():\n",
        "    globals()['_get_min_periods_c3'] = lambda lookback, factor=0.8: max(1, int(lookback * factor)) if isinstance(lookback, int) and lookback > 0 else 1\n",
        "    logger.warning(\"_get_min_periods_c3 (Cell 3) placeholder used.\")\n",
        "\n",
        "# Technical Analysis library (from Cell 1)\n",
        "if 'ta' not in globals():\n",
        "    try: import ta\n",
        "    except ImportError: logger.critical(\"CRITICAL: 'ta' library not imported. Live features will fail.\"); ta = None # type: ignore\n",
        "\n",
        "# Globals from Cell 0, 1, 2, 4, 5, 6 (with defaults)\n",
        "config_defaults_c8 = {\n",
        "    'SL_ATR_MULTIPLIER_DEFAULT': 0.75, 'TP_ATR_MULTIPLIER_DEFAULT': 1.5,\n",
        "    'MAX_DAILY_LOSS_FIXED_CONFIG': 400.0, 'MAX_DAILY_LOSS_MARGIN_THRESHOLD_CONFIG': 20000.0,\n",
        "    'MAX_DAILY_LOSS_MARGIN_PERCENTAGE_CONFIG': 0.025,\n",
        "    'CONSECUTIVE_LOSS_DAYS_HALT_THRESHOLD': 3,\n",
        "    'UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER': 5.0, 'MARGIN_UTILIZATION_PERCENT': 0.92,\n",
        "    'MAX_ORDER_RETRY_ATTEMPTS': 3, 'UPSTOX_PRODUCT_TYPE': \"I\", 'UPSTOX_ORDER_VALIDITY': \"DAY\",\n",
        "    'EXIT_ORDER_TYPE': \"MARKET\", # For SL/TP/EOD exits\n",
        "    'ENTRY_ORDER_TYPE_DEFAULT': \"LIMIT\",\n",
        "    'ENTRY_LIMIT_PRICE_BUFFER_PCT': 0.0005,\n",
        "    'MAX_TRADES_PER_SYMBOL_PER_DAY': 2, 'MAX_TRADES_PER_DAY_GLOBAL': 10,\n",
        "    'LOOKBACK_WINDOW': 60, 'CLASS_LABELS': {0:'BUY',1:'HOLD',2:'SELL'}, 'UPSTOX_INSTRUMENT_KEYS': {},\n",
        "    'NSE_TZ': pytz.timezone(\"Asia/Kolkata\"), 'MARKET_OPEN_TIME_STR': \"09:15:00\",\n",
        "    'MARKET_CLOSE_TIME_STR': \"15:30:00\", 'MIN_ENTRY_TIME_AFTER_OPEN_STR': \"09:20:00\",\n",
        "    'NO_NEW_ENTRY_AFTER_TIME_STR': \"14:20:00\", 'SQUARE_OFF_ALL_START_TIME_STR': \"14:45:00\",\n",
        "    'SQUARE_OFF_ALL_END_TIME_STR': \"14:55:00\",\n",
        "    'LIVE_PROCESSING_INTERVAL_SECONDS': 10, # Process signals every 10s\n",
        "    'LIVE_MONITORING_INTERVAL_SECONDS': 1,  # Monitor SL/TP every 1s\n",
        "    'LIVE_AGGREGATION_INTERVAL_SECONDS': 10, # Aggregate ticks to 10s micro-candles\n",
        "    'MC_DROPOUT_SAMPLES': 20, 'CONFIDENCE_THRESHOLD_TRADE': 0.98,\n",
        "    'CAPITAL_THRESHOLD_FOR_MULTI_TRADE': 30000.0, 'USE_REALTIME_WEBSOCKET_FEED': True,\n",
        "    'BACKTEST_TRANSACTION_COST_PCT': 0.0007, 'data_store_by_symbol': {},\n",
        "    'trained_models_by_symbol': {}, 'live_states_by_symbol': {},\n",
        "    'strategy_performance_insights_by_symbol': {}, 'tick_aggregators_by_symbol': {},\n",
        "    'tick_queue_global': collections.deque(maxlen=50000), 'tick_queue_lock_global': threading.Lock(),\n",
        "    'TARGET_INTERVAL': \"1minute\", 'ATR_PERIOD': 14, 'SMA_PERIODS': [10,20,50], 'EMA_PERIODS': [10,20,50],\n",
        "    'RSI_PERIOD':14, 'MACD_FAST':12, 'MACD_SLOW':26, 'MACD_SIGNAL':9, 'BB_WINDOW':20, 'BB_NUM_STD':2.0,\n",
        "    'AVG_DAILY_RANGE_PERIOD':10, 'LIVE_DATA_DIR': './data_live',\n",
        "    'TRADE_LOG_FILENAME_TEMPLATE': \"tradelog_{symbol}_{date_str}.csv\",\n",
        "    'UPSTOX_API_KEY': None, 'UPSTOX_API_SECRET': None, 'UPSTOX_REDIRECT_URI': None,\n",
        "    'UPSTOX_ACCESS_TOKEN_FILE_PATH': './other_files/upstox_access_token.json',\n",
        "    'UPSTOX_ACCESS_TOKEN_HARDCODED': None, 'TELEGRAM_BOT_TOKEN': None, 'TELEGRAM_CHAT_ID': None,\n",
        "    'UPSTOX_SDK_AVAILABLE': False, 'UPSTOX_PROTOBUF_MODULE_AVAILABLE': False,\n",
        "    'FeedResponse': None, 'UpstoxApiException': type('UpstoxApiExceptionPlaceholder_C8', (Exception,), {}),\n",
        "    'upstox_client': None,\n",
        "    'TelegramApplication': None, 'TelegramBot': None, 'TelegramError': Exception, 'TELEGRAM_BOT_AVAILABLE': False,\n",
        "    'API_RATE_LIMITER_CAPACITY': 10, 'API_RATE_LIMITER_REFILL_RATE': 3, # New config for rate limiter\n",
        "}\n",
        "for param_c8, default_val_c8 in config_defaults_c8.items():\n",
        "    if param_c8 not in globals(): globals()[param_c8] = default_val_c8\n",
        "\n",
        "# --- - Proactive API Rate Limiter (Token Bucket Algorithm) ---\n",
        "class RateLimiter:\n",
        "    \"\"\"A Token Bucket rate limiter to proactively prevent API overuse.\"\"\"\n",
        "    def __init__(self, capacity: int, refill_rate: float):\n",
        "        self.capacity = float(capacity)\n",
        "        self.refill_rate = float(refill_rate)\n",
        "        self.tokens = float(capacity)\n",
        "        self.last_refill_timestamp = time.monotonic()\n",
        "        self.lock = asyncio.Lock()\n",
        "\n",
        "    async def _refill(self):\n",
        "        \"\"\"Adds tokens that have been generated since the last call.\"\"\"\n",
        "        now = time.monotonic()\n",
        "        time_passed = now - self.last_refill_timestamp\n",
        "        new_tokens = time_passed * self.refill_rate\n",
        "        if new_tokens > 0:\n",
        "            self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
        "            self.last_refill_timestamp = now\n",
        "\n",
        "    async def get_token(self):\n",
        "        \"\"\"Waits for and consumes one token from the bucket.\"\"\"\n",
        "        async with self.lock:\n",
        "            await self._refill()\n",
        "            if self.tokens >= 1:\n",
        "                self.tokens -= 1\n",
        "                return\n",
        "            else:\n",
        "                # Calculate wait time needed for 1 full token\n",
        "                wait_time = (1 - self.tokens) / self.refill_rate\n",
        "                await asyncio.sleep(wait_time)\n",
        "                # After waiting, refill and consume\n",
        "                await self._refill()\n",
        "                if self.tokens >= 1: # Should be true unless refill rate is 0\n",
        "                    self.tokens -= 1\n",
        "                return\n",
        "\n",
        "# --- Initialize the global rate limiter ---\n",
        "api_rate_limiter = RateLimiter(capacity=API_RATE_LIMITER_CAPACITY, refill_rate=API_RATE_LIMITER_REFILL_RATE)\n",
        "logger.info(f\"Initialized API Rate Limiter: Capacity={API_RATE_LIMITER_CAPACITY}, Refill Rate={API_RATE_LIMITER_REFILL_RATE}/sec.\")\n",
        "\n",
        "\n",
        "# Pattern functions from Cell 3 (fallbacks)\n",
        "pattern_func_names_c3_c8 = ['find_potential_order_blocks', 'find_engulfing_patterns', 'find_potential_liquidity_sweeps',\n",
        "                            'find_institutional_trading_patterns', 'detect_market_character', 'detect_market_sentiment']\n",
        "for func_name_c3_c8 in pattern_func_names_c3_c8:\n",
        "    if func_name_c3_c8 not in globals(): globals()[func_name_c3_c8] = lambda df, **kwargs: df; logger.warning(f\"{func_name_c3_c8} (Cell 3) placeholder used.\")\n",
        "if 'adapt_strategy_parameters_for_symbol' not in globals(): # From Cell 6\n",
        "    globals()['adapt_strategy_parameters_for_symbol'] = lambda s_name: asyncio.sleep(0); logger.warning(\"adapt_strategy_parameters_for_symbol (Cell 6) placeholder used.\")\n",
        "if 'update_historical_data_in_db' not in globals(): # From Cell 4\n",
        "    globals()['update_historical_data_in_db'] = lambda s, i_k, t_i, d_f: False; logger.warning(\"update_historical_data_in_db (Cell 4) placeholder used.\")\n",
        "\n",
        "# --- - State Persistence Functions ---\n",
        "# These functions save and load the bot's state to prevent data loss on restart.\n",
        "\n",
        "class DateTimeEncoder(json.JSONEncoder):\n",
        "    \"\"\"A custom JSON encoder to handle datetime objects.\"\"\"\n",
        "    def default(self, o):\n",
        "        if isinstance(o, (datetime, datetime_date)):\n",
        "            return o.isoformat()\n",
        "        return super().default(o)\n",
        "\n",
        "def save_state_to_json():\n",
        "    \"\"\"Saves critical global state variables to a JSON file.\"\"\"\n",
        "    global logger, live_states_by_symbol, portfolio_daily_pnl_achieved, portfolio_trades_today_count\n",
        "    global is_trading_halted_for_day_global, can_place_new_order_today_global, last_daily_reset_date_global, STATE_FILE_PATH\n",
        "    try:\n",
        "        state_to_save = {\n",
        "            'live_states_by_symbol': live_states_by_symbol,\n",
        "            'portfolio_daily_pnl_achieved': portfolio_daily_pnl_achieved,\n",
        "            'portfolio_trades_today_count': portfolio_trades_today_count,\n",
        "            'is_trading_halted_for_day_global': is_trading_halted_for_day_global,\n",
        "            'can_place_new_order_today_global': can_place_new_order_today_global,\n",
        "            'last_daily_reset_date_global': last_daily_reset_date_global\n",
        "        }\n",
        "        os.makedirs(os.path.dirname(STATE_FILE_PATH), exist_ok=True)\n",
        "        with open(STATE_FILE_PATH, 'w') as f:\n",
        "            json.dump(state_to_save, f, indent=4, cls=DateTimeEncoder)\n",
        "        logger.debug(f\"Successfully saved state to {STATE_FILE_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save state to {STATE_FILE_PATH}: {e}\", exc_info=True)\n",
        "\n",
        "def load_state_from_json():\n",
        "    \"\"\"Loads bot state from JSON file on startup.\"\"\"\n",
        "    global logger, live_states_by_symbol, portfolio_daily_pnl_achieved, portfolio_trades_today_count\n",
        "    global is_trading_halted_for_day_global, can_place_new_order_today_global, last_daily_reset_date_global, STATE_FILE_PATH, NSE_TZ\n",
        "    if not os.path.exists(STATE_FILE_PATH):\n",
        "        logger.info(\"State file not found. Starting with a fresh state.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(STATE_FILE_PATH, 'r') as f:\n",
        "            loaded_state = json.load(f)\n",
        "\n",
        "        live_states_by_symbol.update(loaded_state.get('live_states_by_symbol', {}))\n",
        "        portfolio_daily_pnl_achieved = loaded_state.get('portfolio_daily_pnl_achieved', 0.0)\n",
        "        portfolio_trades_today_count = loaded_state.get('portfolio_trades_today_count', 0)\n",
        "        is_trading_halted_for_day_global = loaded_state.get('is_trading_halted_for_day_global', False)\n",
        "        can_place_new_order_today_global = loaded_state.get('can_place_new_order_today_global', True)\n",
        "\n",
        "        reset_date_str = loaded_state.get('last_daily_reset_date_global')\n",
        "        if reset_date_str:\n",
        "            last_daily_reset_date_global = datetime.fromisoformat(reset_date_str).date()\n",
        "\n",
        "        # Convert timestamp strings back to datetime objects\n",
        "        for sym_state in live_states_by_symbol.values():\n",
        "            if 'entry_time' in sym_state and isinstance(sym_state['entry_time'], str):\n",
        "                sym_state['entry_time'] = datetime.fromisoformat(sym_state['entry_time']).astimezone(NSE_TZ)\n",
        "\n",
        "        logger.info(f\"Successfully loaded and restored state from {STATE_FILE_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load or parse state from {STATE_FILE_PATH}. Starting fresh. Error: {e}\", exc_info=True)\n",
        "# --- End of State Persistence Functions ---\n",
        "\n",
        "\n",
        "def calculate_all_features_for_df(symbol_name: str, df_full_history_sym: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculates a full feature set for the entire provided historical DataFrame.\n",
        "    This is the new, efficient, vectorized approach.\n",
        "    Returns the DataFrame with all feature columns added, or None on error.\n",
        "    \"\"\"\n",
        "    global logger, data_store_by_symbol, ta, _get_min_periods_c3, NSE_TZ\n",
        "    global SMA_PERIODS, EMA_PERIODS, RSI_PERIOD, MACD_FAST, MACD_SLOW, MACD_SIGNAL, ATR_PERIOD\n",
        "    global BB_WINDOW, BB_NUM_STD, AVG_DAILY_RANGE_PERIOD, LOOKBACK_WINDOW\n",
        "    global find_potential_order_blocks, find_engulfing_patterns, find_potential_liquidity_sweeps\n",
        "    global find_institutional_trading_patterns, detect_market_character, detect_market_sentiment\n",
        "\n",
        "    if df_full_history_sym.empty:\n",
        "        return None\n",
        "\n",
        "    # Ensure minimum length for the largest indicator window\n",
        "    min_len_needed = max(LOOKBACK_WINDOW, max(SMA_PERIODS, default=0), max(EMA_PERIODS, default=0), RSI_PERIOD, MACD_SLOW, ATR_PERIOD, BB_WINDOW, AVG_DAILY_RANGE_PERIOD) + 20\n",
        "    if len(df_full_history_sym) < min_len_needed:\n",
        "        logger.debug(f\"calculate_all_features_for_df {symbol_name}: Data too short ({len(df_full_history_sym)} vs {min_len_needed}).\")\n",
        "        return None\n",
        "\n",
        "    df = df_full_history_sym.copy()\n",
        "\n",
        "    required_raw_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "    if not all(col in df.columns for col in required_raw_cols):\n",
        "        logger.error(f\"calculate_all_features_for_df {symbol_name}: Missing OHLCV columns.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # --- Technical Indicators (Vectorized) ---\n",
        "        close_s, high_s, low_s = df['close'], df['high'], df['low']\n",
        "        for p in SMA_PERIODS: df[f'sma_{p}'] = ta.trend.SMAIndicator(close_s, window=p, fillna=True).sma_indicator()\n",
        "        for p in EMA_PERIODS: df[f'ema_{p}'] = ta.trend.EMAIndicator(close_s, window=p, fillna=True).ema_indicator()\n",
        "        if RSI_PERIOD > 0: df['rsi'] = ta.momentum.RSIIndicator(close_s, window=RSI_PERIOD, fillna=True).rsi()\n",
        "        if all(x > 0 for x in [MACD_FAST, MACD_SLOW, MACD_SIGNAL]):\n",
        "            macd_i = ta.trend.MACD(close_s, MACD_SLOW, MACD_FAST, MACD_SIGNAL, fillna=True)\n",
        "            df['macd'], df['macd_signal'], df['macd_diff'] = macd_i.macd(), macd_i.macd_signal(), macd_i.macd_diff()\n",
        "        if ATR_PERIOD > 0: df['atr'] = ta.volatility.AverageTrueRange(high_s, low_s, close_s, ATR_PERIOD, fillna=True).average_true_range()\n",
        "        else: df['atr'] = close_s * 0.015\n",
        "        df['atr'] = df['atr'].fillna(df['close'] * 0.015 + 1e-7).replace(0, 1e-7) # Fill NaNs and avoid zero\n",
        "        if BB_WINDOW > 0:\n",
        "            bb_i = ta.volatility.BollingerBands(close_s, BB_WINDOW, BB_NUM_STD, fillna=True)\n",
        "            df.update({'bb_mavg':bb_i.bollinger_mavg(), 'bb_hband':bb_i.bollinger_hband(), 'bb_lband':bb_i.bollinger_lband(), 'bb_pband':bb_i.bollinger_pband(), 'bb_wband':bb_i.bollinger_wband()})\n",
        "        df['price_change_pct'] = close_s.pct_change() # Removed fill_method\n",
        "        df['high_low_range'] = high_s - low_s\n",
        "        adr_col = f'adr_{AVG_DAILY_RANGE_PERIOD}'\n",
        "        if AVG_DAILY_RANGE_PERIOD > 0 and not df.empty:\n",
        "            idx_mkt = df.index.tz_convert(NSE_TZ) if df.index.tz is not None else df.index.tz_localize(NSE_TZ, ambiguous='infer', nonexistent='shift_forward')\n",
        "            d_h = df['high'].groupby(idx_mkt.date).transform('max'); d_l = df['low'].groupby(idx_mkt.date).transform('min')\n",
        "            df[adr_col] = (d_h - d_l).rolling(window=AVG_DAILY_RANGE_PERIOD, min_periods=_get_min_periods_c3(AVG_DAILY_RANGE_PERIOD)).mean()\n",
        "        else: df[adr_col] = np.nan\n",
        "\n",
        "        # --- Pattern Features (Vectorized) ---\n",
        "        df_for_patterns = df.rename(columns={'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'})\n",
        "        pattern_map = {'find_potential_order_blocks': find_potential_order_blocks, 'find_engulfing_patterns': find_engulfing_patterns,\n",
        "                       'find_potential_liquidity_sweeps': find_potential_liquidity_sweeps, 'find_institutional_trading_patterns': find_institutional_trading_patterns,\n",
        "                       'detect_market_character': detect_market_character, 'detect_market_sentiment': detect_market_sentiment}\n",
        "        for func_name, pattern_func in pattern_map.items():\n",
        "            try: df_for_patterns = pattern_func(df_for_patterns, copy_df=False)\n",
        "            except Exception as e_pat: logger.error(f\"Feature Calc {symbol_name}: Error in '{func_name}': {e_pat}\", exc_info=False)\n",
        "\n",
        "        pattern_output_map = {'Potential_Bullish_Ob':'pattern_bullish_ob', 'Potential_Bearish_Ob':'pattern_bearish_ob', 'Bullish_Engulfing':'pattern_bullish_engulfing',\n",
        "                              'Bearish_Engulfing':'pattern_bearish_engulfing', 'Potential_Bearish_Sweep':'pattern_bearish_sweep', 'Potential_Bullish_Sweep':'pattern_bullish_sweep',\n",
        "                              'Inst_Buy_Signal':'pattern_inst_buy', 'Inst_Sell_Signal':'pattern_inst_sell', 'Market_Character':'market_character', 'Market_Sentiment':'market_sentiment'}\n",
        "        cat_pattern_feats = ['market_character', 'market_sentiment']\n",
        "        for title_c, lower_c in pattern_output_map.items():\n",
        "            if title_c in df_for_patterns.columns:\n",
        "                df[lower_c] = df_for_patterns[title_c]\n",
        "            else:\n",
        "                df[lower_c] = 0 if lower_c not in cat_pattern_feats else \"Undefined\"\n",
        "\n",
        "        # One-Hot Encode categorical features\n",
        "        df = pd.get_dummies(df, columns=[c for c in cat_pattern_feats if c in df.columns], prefix=cat_pattern_feats, dummy_na=False, dtype=int)\n",
        "\n",
        "        # --- Final Column Alignment ---\n",
        "        expected_feature_cols = data_store_by_symbol.get(symbol_name.upper(), {}).get('feature_columns')\n",
        "        if not expected_feature_cols:\n",
        "            logger.error(f\"calculate_all_features_for_df {symbol_name}: 'feature_columns' not found in data_store.\")\n",
        "            return None\n",
        "\n",
        "        # Add any missing one-hot-encoded columns that might not have appeared in this data slice\n",
        "        for col in expected_feature_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0\n",
        "\n",
        "        # Return the DataFrame with all columns needed for slicing later\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"calculate_all_features_for_df {symbol_name}: Unhandled exception: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "def _get_max_indicator_lookback() -> int:\n",
        "    \"\"\"\n",
        "    Dynamically determines the longest lookback period required by any\n",
        "    technical indicator defined in the global configuration.\n",
        "\n",
        "    This ensures that when calculating live features on a slice of data, the slice\n",
        "    is always large enough to prevent calculation errors.\n",
        "\n",
        "    Returns:\n",
        "        int: The maximum lookback period required, plus a safety buffer.\n",
        "    \"\"\"\n",
        "    global SMA_PERIODS, EMA_PERIODS, RSI_PERIOD, MACD_SLOW, ATR_PERIOD, BB_WINDOW, AVG_DAILY_RANGE_PERIOD, LOOKBACK_WINDOW\n",
        "\n",
        "    # Collect all configured lookback periods into a single list\n",
        "    all_periods = [\n",
        "        max(SMA_PERIODS, default=0),\n",
        "        max(EMA_PERIODS, default=0),\n",
        "        RSI_PERIOD,\n",
        "        MACD_SLOW,\n",
        "        ATR_PERIOD,\n",
        "        BB_WINDOW,\n",
        "        AVG_DAILY_RANGE_PERIOD,\n",
        "        LOOKBACK_WINDOW  # The model's sequence length is also a lookback\n",
        "    ]\n",
        "\n",
        "    # Return the largest value found, plus a safety buffer of 20 candles\n",
        "    # to ensure stability for rolling calculations.\n",
        "    return max(all_periods) + 20\n",
        "\n",
        "# --- Efficient Live Feature Calculation Function ---\n",
        "def calculate_features_for_new_candle(symbol_name: str, ohlcv_history_df: pd.DataFrame, new_candle_series: pd.Series) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Efficiently calculates features for a new candle by operating on a dynamically sized slice of data.\n",
        "    This prevents errors if indicator configurations are changed to use long lookback periods.\n",
        "    \"\"\"\n",
        "    global logger, data_store_by_symbol\n",
        "    try:\n",
        "        base_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "        history_clean_ohlcv = ohlcv_history_df[base_cols].copy()\n",
        "        new_candle_df = new_candle_series[base_cols].to_frame().T\n",
        "        new_candle_df.index.name = 'timestamp'\n",
        "        history_with_new = pd.concat([history_clean_ohlcv, new_candle_df])\n",
        "\n",
        "        # --- Use the new helper function for dynamic slice length ---\n",
        "        slice_length = _get_max_indicator_lookback()\n",
        "\n",
        "        # Check if we have enough historical data to satisfy the longest lookback requirement\n",
        "        if len(history_with_new) < slice_length:\n",
        "             logger.warning(f\"LiveFeatures {symbol_name}: Not enough data ({len(history_with_new)}) for the required dynamic slice length ({slice_length}).\")\n",
        "             return None\n",
        "\n",
        "        # Take a slice of the data that is guaranteed to be large enough\n",
        "        data_slice = history_with_new.tail(slice_length).copy()\n",
        "\n",
        "        # Calculate features on this smaller, sufficient slice\n",
        "        features_df_slice = calculate_all_features_for_df(symbol_name, data_slice)\n",
        "        if features_df_slice is None:\n",
        "            return None\n",
        "\n",
        "        # Extract just the last row which contains the features for our new candle\n",
        "        new_candle_with_features = features_df_slice.iloc[-1:]\n",
        "\n",
        "        # Append the new candle with its features back to the original history\n",
        "        final_history = pd.concat([ohlcv_history_df, new_candle_with_features])\n",
        "        final_history.ffill(inplace=True) # Forward fill to handle any potential NaNs at the calculation edge\n",
        "\n",
        "        return final_history\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"LiveFeatures {symbol_name}: Unhandled exception during incremental feature update: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "# --- Upstox API Client Initialization and Token Management (No changes needed, already robust) ---\n",
        "\n",
        "def save_access_token_to_file_c8(access_token: str, calculated_expiry_timestamp: int):\n",
        "    \"\"\"Saves the access token and its calculated expiry timestamp to the JSON file.\"\"\"\n",
        "    global logger, UPSTOX_ACCESS_TOKEN_FILE_PATH, NSE_TZ\n",
        "\n",
        "    token_data_to_save = {\n",
        "        'access_token': access_token,\n",
        "        'access_token_expires_at': calculated_expiry_timestamp # Store as Unix timestamp\n",
        "    }\n",
        "\n",
        "    expiry_dt_str = datetime.fromtimestamp(calculated_expiry_timestamp, NSE_TZ).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "    logger.info(f\"Token will be saved with calculated expiry: {expiry_dt_str} (Timestamp: {calculated_expiry_timestamp})\")\n",
        "\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(UPSTOX_ACCESS_TOKEN_FILE_PATH), exist_ok=True)\n",
        "        with open(UPSTOX_ACCESS_TOKEN_FILE_PATH, 'w') as f:\n",
        "            json.dump(token_data_to_save, f, indent=4)\n",
        "        logger.info(f\"Upstox access token and expiry saved to {UPSTOX_ACCESS_TOKEN_FILE_PATH}.\")\n",
        "    except Exception as e_save_token:\n",
        "        logger.error(f\"Error saving access token to {UPSTOX_ACCESS_TOKEN_FILE_PATH}: {e_save_token}\", exc_info=True)\n",
        "\n",
        "def clear_access_token_from_file_c8():\n",
        "    \"\"\"Clears the access token file if it exists.\"\"\"\n",
        "    global logger, UPSTOX_ACCESS_TOKEN_FILE_PATH\n",
        "    if os.path.exists(UPSTOX_ACCESS_TOKEN_FILE_PATH):\n",
        "        try:\n",
        "            os.remove(UPSTOX_ACCESS_TOKEN_FILE_PATH)\n",
        "            logger.info(f\"Cleared access token file: {UPSTOX_ACCESS_TOKEN_FILE_PATH}\")\n",
        "        except Exception as e_clear_token:\n",
        "            logger.error(f\"Error clearing access token file {UPSTOX_ACCESS_TOKEN_FILE_PATH}: {e_clear_token}\", exc_info=True)\n",
        "\n",
        "async def initialize_upstox_client(max_retries_auth: int = 2, retry_delay_auth: int = 5) -> bool:\n",
        "    \"\"\"\n",
        "    Initializes the Upstox API client.\n",
        "    - Tries to use an existing token from file or hardcoded.\n",
        "    - Validates the token by checking saved expiry and making a profile API call.\n",
        "    - If token is invalid/missing/expired, initiates manual authorization flow.\n",
        "    - Prioritizes 'expires_in' from API response for expiry calculation.\n",
        "    \"\"\"\n",
        "    global logger, upstox_api_client_global, UPSTOX_SDK_AVAILABLE, upstox_client, UpstoxApiException\n",
        "    global UPSTOX_API_KEY, UPSTOX_API_SECRET, UPSTOX_REDIRECT_URI, api_rate_limiter\n",
        "    global UPSTOX_ACCESS_TOKEN_FILE_PATH, UPSTOX_ACCESS_TOKEN_HARDCODED, NSE_TZ, send_telegram_message\n",
        "\n",
        "    if not UPSTOX_SDK_AVAILABLE:\n",
        "        logger.critical(\"Upstox SDK is not available. Cannot initialize API client.\")\n",
        "        return False\n",
        "    if not upstox_client: # Check if the SDK module itself is loaded\n",
        "        logger.critical(\"Upstox SDK module 'upstox_client' is not loaded/available.\")\n",
        "        return False\n",
        "\n",
        "    current_access_token: Optional[str] = None\n",
        "    access_token_expires_at_ts: Optional[int] = None\n",
        "\n",
        "    # 1. Try loading token from hardcoded config (primarily for quick debug)\n",
        "    if UPSTOX_ACCESS_TOKEN_HARDCODED:\n",
        "        logger.info(\"Attempting to use hardcoded Upstox Access Token.\")\n",
        "        current_access_token = UPSTOX_ACCESS_TOKEN_HARDCODED\n",
        "        # No explicit expiry for hardcoded, assume it's fresh or will fail validation.\n",
        "\n",
        "    # 2. If not hardcoded, try loading token from file\n",
        "    if not current_access_token and os.path.exists(UPSTOX_ACCESS_TOKEN_FILE_PATH):\n",
        "        try:\n",
        "            with open(UPSTOX_ACCESS_TOKEN_FILE_PATH, 'r') as f_token:\n",
        "                token_data_file = json.load(f_token)\n",
        "            current_access_token = token_data_file.get('access_token')\n",
        "            access_token_expires_at_ts = token_data_file.get('access_token_expires_at')\n",
        "            if current_access_token and access_token_expires_at_ts:\n",
        "                expiry_log_str = datetime.fromtimestamp(access_token_expires_at_ts, NSE_TZ).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "                logger.info(f\"Loaded access token from file. Saved expiry: {expiry_log_str}.\")\n",
        "            elif current_access_token:\n",
        "                logger.info(\"Loaded access token from file, but expiry time was missing. Will validate.\")\n",
        "            else:\n",
        "                logger.info(\"Token file found but no access token within. Will proceed to auth.\")\n",
        "        except Exception as e_load_file:\n",
        "            logger.error(f\"Error loading access token from {UPSTOX_ACCESS_TOKEN_FILE_PATH}: {e_load_file}\", exc_info=True)\n",
        "            current_access_token = None # Ensure it's None if loading failed\n",
        "\n",
        "    # 3. Validate existing token (if any)\n",
        "    token_needs_manual_reauth = True\n",
        "    if current_access_token:\n",
        "        is_explicitly_expired = False\n",
        "        if access_token_expires_at_ts and isinstance(access_token_expires_at_ts, (int, float)):\n",
        "            # Check if token is within 5 minutes of expiry or already past\n",
        "            if time.time() >= (access_token_expires_at_ts - 300): # 5 min buffer before official expiry\n",
        "                is_explicitly_expired = True\n",
        "                expiry_dt = datetime.fromtimestamp(access_token_expires_at_ts, NSE_TZ)\n",
        "                logger.info(f\"Access token considered EXPIRED based on saved expiry time: {expiry_dt.isoformat()}. Current time: {datetime.now(NSE_TZ).isoformat()}\")\n",
        "        else:\n",
        "            logger.info(\"No saved expiry time for token or invalid format. Token will be validated via API call.\")\n",
        "\n",
        "        if not is_explicitly_expired:\n",
        "            logger.info(\"Attempting to validate existing access token by fetching profile...\")\n",
        "            try:\n",
        "                sdk_config_test = upstox_client.Configuration()\n",
        "                sdk_config_test.access_token = current_access_token\n",
        "                temp_api_client_for_test = upstox_client.ApiClient(sdk_config_test)\n",
        "\n",
        "                profile_api_instance = upstox_client.UserApi(temp_api_client_for_test)\n",
        "                await api_rate_limiter.get_token() # Proactive rate limit\n",
        "                await asyncio.to_thread(profile_api_instance.get_profile, api_version=\"2.0\")\n",
        "\n",
        "                upstox_api_client_global = temp_api_client_for_test # Token is valid, assign to global client\n",
        "                token_needs_manual_reauth = False\n",
        "                logger.info(\"Existing access token validated successfully via API call.\")\n",
        "            except UpstoxApiException as e_validate:\n",
        "                if e_validate.status == 401: # Unauthorized\n",
        "                    logger.warning(\"Existing access token is INVALID (401 Unauthorized during validation). Needs re-authentication.\")\n",
        "                else:\n",
        "                    logger.warning(f\"API error during access token validation (Status {e_validate.status}, Reason: {e_validate.reason}). Needs re-authentication.\")\n",
        "            except Exception as e_validate_general:\n",
        "                logger.warning(f\"Unexpected error during access token validation: {e_validate_general}. Assuming re-authentication is needed.\")\n",
        "        # If explicitly_expired, token_needs_manual_reauth remains True\n",
        "    else:\n",
        "        logger.info(\"No existing access token found (file/hardcoded). Manual authorization required.\")\n",
        "\n",
        "    # 4. Manual Authorization Flow if needed\n",
        "    if token_needs_manual_reauth:\n",
        "        current_access_token = None # Clear any stale token\n",
        "        clear_access_token_from_file_c8() # Remove old/invalid token file\n",
        "\n",
        "        if not all([UPSTOX_API_KEY, UPSTOX_API_SECRET, UPSTOX_REDIRECT_URI]):\n",
        "             logger.critical(\"Cannot proceed with manual authorization: UPSTOX_API_KEY, UPSTOX_API_SECRET, or UPSTOX_REDIRECT_URI is missing.\"); return False\n",
        "\n",
        "        logger.info(\"Initiating manual authorization flow for new Upstox access token...\")\n",
        "        for attempt_auth in range(max_retries_auth):\n",
        "            try:\n",
        "\n",
        "                sdk_host_config = upstox_client.Configuration() # Fresh config for auth URL\n",
        "                sdk_host = getattr(sdk_host_config, 'host', 'api-v2.upstox.com')\n",
        "                if not sdk_host.startswith(\"http\"): sdk_host = f\"https://{sdk_host}\"\n",
        "\n",
        "                auth_url_state = f\"BotAuth_{int(time.time())}\" # Generate unique state for this attempt\n",
        "                auth_url = (f\"{sdk_host}/v2/login/authorization/dialog\"\n",
        "                            f\"?client_id={UPSTOX_API_KEY}\"\n",
        "                            f\"&redirect_uri={UPSTOX_REDIRECT_URI}\"\n",
        "                            f\"&response_type=code\"\n",
        "                            f\"&state={auth_url_state}\")\n",
        "\n",
        "                print(f\"\\n--- ACTION REQUIRED FOR UPSTOX AUTHENTICATION (Attempt {attempt_auth + 1}/{max_retries_auth}) ---\")\n",
        "                print(f\"1. Open the following URL in your browser:\\n   {auth_url}\")\n",
        "                print(f\"2. Log in to Upstox and authorize the application.\")\n",
        "                print(f\"3. After authorization, you will be redirected. Copy the FULL redirected URL from your browser's address bar.\")\n",
        "\n",
        "                redirected_url_input = await asyncio.to_thread(input, \"Paste the FULL redirected URL here: \")\n",
        "                redirected_url_input = redirected_url_input.strip()\n",
        "\n",
        "                if not redirected_url_input or 'code=' not in redirected_url_input:\n",
        "                    logger.error(\"Invalid redirected URL provided, or 'code=' parameter is missing. Please try again.\")\n",
        "                    if attempt_auth < max_retries_auth - 1: await asyncio.sleep(retry_delay_auth); continue\n",
        "                    else: break\n",
        "\n",
        "                # Extract authorization code and state from the redirected URL\n",
        "                auth_code = None\n",
        "                returned_state = None\n",
        "                query_params = redirected_url_input.split('?')[-1].split('&')\n",
        "                for param in query_params:\n",
        "                    if param.startswith('code='):\n",
        "                        auth_code = param.split('code=')[1]\n",
        "                    elif param.startswith('state='):\n",
        "                        returned_state = param.split('state=')[1]\n",
        "\n",
        "                if not auth_code:\n",
        "                    logger.error(\"Could not extract 'code' from the redirected URL. Please ensure you paste the full URL.\")\n",
        "                    if attempt_auth < max_retries_auth - 1: await asyncio.sleep(retry_delay_auth); continue\n",
        "                    else: break\n",
        "\n",
        "                if returned_state != auth_url_state:\n",
        "                    logger.error(f\"STATE parameter mismatch! Expected: {auth_url_state}, Received: {returned_state}. Authorization aborted for security. Please try the process again carefully.\")\n",
        "                    # Do not proceed with token exchange if state mismatches\n",
        "                    return False # Critical security failure\n",
        "\n",
        "                logger.info(f\"Authorization code received: {auth_code[:15]}... , State validated.\")\n",
        "\n",
        "                # Exchange authorization code for access token\n",
        "                token_exchange_api_client = upstox_client.ApiClient(sdk_host_config) # Use same config for host\n",
        "                token_api_instance = upstox_client.LoginApi(token_exchange_api_client)\n",
        "\n",
        "                await api_rate_limiter.get_token() # Proactive rate limit\n",
        "                token_response_data = await asyncio.to_thread(\n",
        "                    token_api_instance.token,\n",
        "                    api_version=\"2.0\",\n",
        "                    code=auth_code,\n",
        "                    client_id=UPSTOX_API_KEY,\n",
        "                    client_secret=UPSTOX_API_SECRET,\n",
        "                    redirect_uri=UPSTOX_REDIRECT_URI,\n",
        "                    grant_type=\"authorization_code\"\n",
        "                )\n",
        "\n",
        "                # Process token response (Upstox SDK v2 returns an object, not a dict here)\n",
        "                new_access_token_val = getattr(token_response_data, 'access_token', None)\n",
        "                expires_in_seconds = getattr(token_response_data, 'expires_in', None) # Check for 'expires_in'\n",
        "\n",
        "                if new_access_token_val and isinstance(new_access_token_val, str):\n",
        "                    logger.info(f\"Manual authorization successful. New access token obtained: '{new_access_token_val[:15]}...'.\")\n",
        "\n",
        "                    calculated_expiry_ts: int\n",
        "                    if expires_in_seconds and isinstance(expires_in_seconds, int) and expires_in_seconds > 0:\n",
        "                        # Calculate expiry based on 'expires_in' from API response\n",
        "                        calculated_expiry_ts = int(time.time()) + expires_in_seconds\n",
        "                        logger.info(f\"Token 'expires_in': {expires_in_seconds} seconds. Absolute expiry calculated.\")\n",
        "                    else:\n",
        "                        # Fallback to \"next day 3:30 AM IST\" if 'expires_in' is not available or invalid\n",
        "                        now_ist_auth = datetime.now(NSE_TZ)\n",
        "                        next_day_ist_auth = now_ist_auth.date() + timedelta(days=1)\n",
        "                        expiry_datetime_ist_auth = datetime.combine(next_day_ist_auth, datetime_time(3, 30, 0), tzinfo=NSE_TZ)\n",
        "                        calculated_expiry_ts = int(expiry_datetime_ist_auth.timestamp())\n",
        "                        logger.info(f\"Token 'expires_in' not found/invalid in response. Using fallback expiry calculation (next day 3:30 AM IST).\")\n",
        "\n",
        "                    save_access_token_to_file_c8(new_access_token_val, calculated_expiry_ts)\n",
        "\n",
        "                    # Configure the global API client with the new token\n",
        "                    sdk_config_final = upstox_client.Configuration()\n",
        "                    sdk_config_final.access_token = new_access_token_val\n",
        "                    upstox_api_client_global = upstox_client.ApiClient(sdk_config_final)\n",
        "\n",
        "                    expiry_dt_display = datetime.fromtimestamp(calculated_expiry_ts, NSE_TZ)\n",
        "                    await send_telegram_message(f\"‚úÖ Upstox Client Initialized (Manual Auth). Token valid until approx. {expiry_dt_display.strftime('%Y-%m-%d %H:%M:%S %Z')}.\")\n",
        "                    return True # Successfully initialized\n",
        "                else:\n",
        "                    err_msg_extract = \"Failed to extract 'access_token' from Upstox manual auth response.\"\n",
        "                    if hasattr(token_response_data, 'errors'): err_msg_extract += f\" Errors: {getattr(token_response_data, 'errors')}\"\n",
        "                    elif hasattr(token_response_data, 'message'): err_msg_extract += f\" Message: {getattr(token_response_data, 'message')}\"\n",
        "                    logger.error(f\"{err_msg_extract}. Response snippet: {str(token_response_data)[:500]}\")\n",
        "\n",
        "            except UpstoxApiException as e_auth_sdk:\n",
        "                logger.error(f\"UpstoxApiException during manual auth (Attempt {attempt_auth + 1}): Status {e_auth_sdk.status} - Reason {e_auth_sdk.reason}. Body: {str(e_auth_sdk.body)[:200]}\", exc_info=False)\n",
        "            except Exception as e_manual_auth_general:\n",
        "                logger.error(f\"General error during manual authorization (Attempt {attempt_auth + 1}): {e_manual_auth_general}\", exc_info=True)\n",
        "\n",
        "            if attempt_auth < max_retries_auth - 1:\n",
        "                logger.info(f\"Retrying manual authorization in {retry_delay_auth} seconds...\")\n",
        "                await asyncio.sleep(retry_delay_auth)\n",
        "\n",
        "        logger.error(\"Failed to initialize Upstox client after all manual authorization attempts.\"); return False\n",
        "\n",
        "    if not upstox_api_client_global:\n",
        "        logger.critical(\"Upstox client initialization flow completed, but global client 'upstox_api_client_global' is not configured. This indicates an issue.\"); return False\n",
        "\n",
        "    logger.info(\"Upstox API client is configured and ready.\")\n",
        "    return True\n",
        "\n",
        "# --- Telegram Bot Functions (No changes needed)---\n",
        "async def initialize_telegram_bot_async():\n",
        "    global logger, telegram_bot_global, telegram_app_global, telegram_initialized_successfully, TELEGRAM_BOT_TOKEN, TELEGRAM_BOT_AVAILABLE, TelegramApplication, TelegramBot, TelegramError\n",
        "    if not TELEGRAM_BOT_AVAILABLE: logger.warning(\"Telegram lib not available.\"); telegram_initialized_successfully = False; return False\n",
        "    if not TELEGRAM_BOT_TOKEN: logger.warning(\"TELEGRAM_BOT_TOKEN not set.\"); telegram_initialized_successfully = False; return False\n",
        "    if telegram_initialized_successfully and telegram_bot_global and telegram_app_global: logger.info(\"Telegram bot already init.\"); return True\n",
        "    try:\n",
        "        telegram_app_global = TelegramApplication.builder().token(TELEGRAM_BOT_TOKEN).build()\n",
        "        telegram_bot_global = telegram_app_global.bot\n",
        "        bot_info = await telegram_bot_global.get_me()\n",
        "        logger.info(f\"Telegram bot init success: {bot_info.username} (ID: {bot_info.id})\"); telegram_initialized_successfully = True; return True\n",
        "    except TelegramError as e: logger.error(f\"TelegramError during bot init: {e}\", exc_info=True)\n",
        "    except Exception as e: logger.error(f\"Failed to init Telegram bot: {e}\", exc_info=True)\n",
        "    telegram_initialized_successfully = False; return False\n",
        "\n",
        "async def send_telegram_message(message_text: str, chat_id_override: Optional[str] = None) -> bool:\n",
        "    global logger, telegram_bot_global, telegram_initialized_successfully, TELEGRAM_CHAT_ID\n",
        "    if not telegram_initialized_successfully or not telegram_bot_global: logger.debug(f\"Telegram bot not ready. Cannot send: {message_text[:70]}...\"); return False\n",
        "    target_chat_id = chat_id_override if chat_id_override else TELEGRAM_CHAT_ID\n",
        "    if not target_chat_id: logger.warning(f\"Telegram CHAT_ID not set. Cannot send: {message_text[:70]}...\"); return False\n",
        "    try:\n",
        "        if len(message_text) > 4090: message_text = message_text[:4090] + \"...\"\n",
        "        await telegram_bot_global.send_message(chat_id=target_chat_id, text=message_text, parse_mode='HTML')\n",
        "        return True\n",
        "    except TelegramError as e_send_err: logger.error(f\"TelegramError sending: {e_send_err}. Msg: {message_text[:70]}...\", exc_info=False)\n",
        "    except Exception as e_send: logger.error(f\"Failed to send Telegram: {e_send}. Msg: {message_text[:70]}...\", exc_info=False)\n",
        "    return False\n",
        "\n",
        "websocket_connected_event = threading.Event()\n",
        "\n",
        "def _schedule_telegram_message(message: str, main_loop: asyncio.AbstractEventLoop):\n",
        "    \"\"\"\n",
        "    A thread-safe helper to schedule a Telegram message on the main event loop.\n",
        "    \"\"\"\n",
        "    global logger, send_telegram_message\n",
        "    if main_loop and main_loop.is_running():\n",
        "        future = asyncio.run_coroutine_threadsafe(send_telegram_message(message), main_loop)\n",
        "\n",
        "        def log_exception_callback(f):\n",
        "            if f.exception():\n",
        "                logger.error(f\"Error in scheduled Telegram message: {f.exception()}\", exc_info=True)\n",
        "\n",
        "        future.add_done_callback(log_exception_callback)\n",
        "    else:\n",
        "        logger.warning(f\"Main event loop not available/running. Cannot schedule Telegram message: {message[:50]}...\")\n",
        "\n",
        "\n",
        "def on_websocket_open(main_loop: asyncio.AbstractEventLoop):\n",
        "    \"\"\"\n",
        "    Callback for when the WebSocket connection opens.\n",
        "    This version now sets a threading.Event to signal a successful connection.\n",
        "    \"\"\"\n",
        "    global logger, websocket_connected_event\n",
        "    logger.info(\"Upstox WebSocket connection opened successfully.\")\n",
        "    # Signal the main thread that the connection is now open.\n",
        "    websocket_connected_event.set()\n",
        "    _schedule_telegram_message(\"‚úÖ Upstox WebSocket Connected.\", main_loop)\n",
        "\n",
        "def on_websocket_close(code: int, reason: str, main_loop: asyncio.AbstractEventLoop):\n",
        "    \"\"\"\n",
        "    Callback for when the WebSocket connection closes.\n",
        "    This version now clears the connection event.\n",
        "    \"\"\"\n",
        "    global logger, websocket_connected_event\n",
        "    # Clear the event to indicate the connection is no longer active.\n",
        "    websocket_connected_event.clear()\n",
        "    reason_str = reason.decode('utf-8') if isinstance(reason, bytes) else str(reason)\n",
        "    logger.warning(f\"Upstox WebSocket closed. Code: {code}, Reason: {reason_str}\")\n",
        "    _schedule_telegram_message(f\"üîå Upstox WebSocket Closed. Code: {code}, R: {reason_str[:50]}\", main_loop)\n",
        "\n",
        "def on_websocket_error(error: Exception, main_loop: asyncio.AbstractEventLoop):\n",
        "    \"\"\"\n",
        "    Callback for any WebSocket error.\n",
        "    \"\"\"\n",
        "    global logger\n",
        "    err_str = str(error)\n",
        "    logger.error(f\"Upstox WebSocket error: {err_str}\", exc_info=isinstance(error, Exception))\n",
        "    if \"403\" in err_str:\n",
        "        _schedule_telegram_message(\n",
        "            \"üö® WS Handshake 403 Forbidden: API credentials may have expired. Please re-authenticate.\", main_loop\n",
        "        )\n",
        "    else:\n",
        "        _schedule_telegram_message(f\"‚ö†Ô∏è Upstox WebSocket Error: {err_str[:100]}\", main_loop)\n",
        "\n",
        "def on_websocket_message(message: dict):\n",
        "    \"\"\"\n",
        "    Handles incoming, decoded WebSocket messages.\n",
        "    \"\"\"\n",
        "    global logger, tick_queue_global, tick_queue_lock_global, NSE_TZ\n",
        "    try:\n",
        "        instrument_key = message.get('instrument_key')\n",
        "        ltp = message.get('ltp')\n",
        "        ltq = message.get('ltq')\n",
        "        ltt_str = message.get('ltt')\n",
        "\n",
        "        if not all([instrument_key, ltp, ltq, ltt_str]):\n",
        "            return\n",
        "\n",
        "        ts_utc = datetime.fromisoformat(ltt_str.replace('Z', '+00:00'))\n",
        "        ts_local = ts_utc.astimezone(NSE_TZ)\n",
        "\n",
        "        tick = {\n",
        "            \"instrument_key\": instrument_key,\n",
        "            \"price\": float(ltp),\n",
        "            \"volume\": int(ltq),\n",
        "            \"timestamp\": ts_local,\n",
        "            \"bid_price\": float(message.get('bid_price', 0.0)),\n",
        "            \"ask_price\": float(message.get('ask_price', 0.0)),\n",
        "            \"oi\": int(message.get('oi', 0.0))\n",
        "        }\n",
        "        with tick_queue_lock_global:\n",
        "            tick_queue_global.append(tick)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in on_websocket_message: {e}\", exc_info=True)\n",
        "\n",
        "async def _reconcile_data_after_reconnect(main_loop: asyncio.AbstractEventLoop):\n",
        "    \"\"\"\n",
        "    Fetches historical data via REST API to fill any gaps that may have occurred\n",
        "    during a WebSocket disconnection.\n",
        "    \"\"\"\n",
        "    global logger, data_store_by_symbol, tick_aggregators_by_symbol, UPSTOX_INSTRUMENT_KEYS\n",
        "    global TARGET_INTERVAL, UPSTOX_HISTORY_INTERVAL_MAP, NSE_TZ\n",
        "    global get_upstox_historical_candles_robust, _schedule_telegram_message\n",
        "\n",
        "    logger.warning(\"WebSocket reconnected. Reconciling historical data for subscribed symbols.\")\n",
        "    # Pass the main_loop to the scheduler\n",
        "    _schedule_telegram_message(\"‚ö†Ô∏è WS Reconnected. Reconciling missed data...\", main_loop)\n",
        "\n",
        "    api_interval = UPSTOX_HISTORY_INTERVAL_MAP.get(TARGET_INTERVAL, \"1minute\")\n",
        "\n",
        "    for instrument_key, aggregator_state in list(tick_aggregators_by_symbol.items()):\n",
        "        symbol_name = aggregator_state['symbol_name']\n",
        "        data_store = data_store_by_symbol.get(symbol_name, {})\n",
        "        ohlcv_df = data_store.get('ohlcv_df')\n",
        "\n",
        "        if ohlcv_df is None or ohlcv_df.empty:\n",
        "            logger.warning(f\"Data reconciliation for {symbol_name} skipped: no existing OHLCV data.\")\n",
        "            continue\n",
        "\n",
        "        last_known_timestamp = ohlcv_df.index.max()\n",
        "        now_nse = datetime.now(NSE_TZ)\n",
        "\n",
        "        logger.info(\n",
        "            f\"Reconciling {symbol_name}: Fetching '{api_interval}' data from \"\n",
        "            f\"{last_known_timestamp.strftime('%Y-%m-%d %H:%M:%S')} to now.\"\n",
        "        )\n",
        "\n",
        "        fetched_df = await get_upstox_historical_candles_robust(\n",
        "            instrument_key=instrument_key,\n",
        "            interval_str_api=api_interval,\n",
        "            to_date_obj=now_nse,\n",
        "            from_date_obj=last_known_timestamp.date()\n",
        "        )\n",
        "\n",
        "        if fetched_df is not None and not fetched_df.empty:\n",
        "            new_candles_df = fetched_df[fetched_df.index > last_known_timestamp]\n",
        "\n",
        "            if not new_candles_df.empty:\n",
        "                combined_df = pd.concat([ohlcv_df, new_candles_df])\n",
        "                combined_df = combined_df[~combined_df.index.duplicated(keep='last')]\n",
        "                combined_df.sort_index(inplace=True)\n",
        "                data_store_by_symbol[symbol_name]['ohlcv_df'] = combined_df\n",
        "                logger.info(f\"‚úÖ Successfully reconciled and merged {len(new_candles_df)} new candles for {symbol_name}.\")\n",
        "            else:\n",
        "                logger.info(f\"Reconciliation for {symbol_name} complete. No new candles were found.\")\n",
        "        else:\n",
        "            logger.error(f\"‚ùå Failed to fetch reconciliation data for {symbol_name}. A data gap may exist.\")\n",
        "\n",
        "\n",
        "def connect_market_websocket_upstox() -> bool:\n",
        "    \"\"\"\n",
        "    Initializes and starts the Upstox WebSocket in a background thread.\n",
        "    This version uses a threading.Event for reliable connection status checking,\n",
        "    fixing the race condition.\n",
        "    \"\"\"\n",
        "    global logger, upstox_api_client_global, websocket_thread_global\n",
        "    global upstox_market_streamer_global, selected_symbols_for_session, UPSTOX_INSTRUMENT_KEYS, upstox_client\n",
        "    global websocket_connected_event\n",
        "\n",
        "    if not UPSTOX_SDK_AVAILABLE or not upstox_api_client_global:\n",
        "        logger.error(\"WS Connect: SDK or API client not available.\")\n",
        "        return False\n",
        "    if websocket_thread_global and websocket_thread_global.is_alive():\n",
        "        logger.info(\"WS connect skipped: active WS thread already running.\")\n",
        "        return True\n",
        "\n",
        "    keys_to_sub = [\n",
        "        UPSTOX_INSTRUMENT_KEYS[s.upper()]\n",
        "        for s in selected_symbols_for_session\n",
        "        if UPSTOX_INSTRUMENT_KEYS.get(s.upper()) and \"INVALID_KEY\" not in UPSTOX_INSTRUMENT_KEYS[s.upper()]\n",
        "    ]\n",
        "    if not keys_to_sub:\n",
        "        logger.error(\"WS Connect: no valid instrument keys to subscribe.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        main_event_loop = asyncio.get_running_loop()\n",
        "        upstox_market_streamer_global = upstox_client.MarketDataStreamer(\n",
        "            upstox_api_client_global\n",
        "        )\n",
        "\n",
        "        upstox_market_streamer_global.on(\"open\", lambda: on_websocket_open(main_event_loop))\n",
        "        upstox_market_streamer_global.on(\"message\", on_websocket_message)\n",
        "        upstox_market_streamer_global.on(\"error\", lambda err: on_websocket_error(err, main_event_loop))\n",
        "        upstox_market_streamer_global.on(\"close\", lambda code, reason: on_websocket_close(code, reason, main_event_loop))\n",
        "\n",
        "        logger.info(\"MarketDataStreamer initialized and handlers registered.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to instantiate MarketDataStreamer: {e}\", exc_info=True)\n",
        "        return False\n",
        "\n",
        "    def ws_target():\n",
        "        try:\n",
        "            upstox_market_streamer_global.connect()\n",
        "        except Exception as e:\n",
        "            logger.critical(f\"CRITICAL EXCEPTION in WebSocket thread: {e}\", exc_info=True)\n",
        "\n",
        "    # Clear the event flag before starting the thread\n",
        "    websocket_connected_event.clear()\n",
        "\n",
        "    websocket_thread_global = threading.Thread(\n",
        "        target=ws_target, daemon=True, name=\"UpstoxWSListener\"\n",
        "    )\n",
        "    websocket_thread_global.start()\n",
        "\n",
        "    logger.info(\"Upstox WebSocket listener thread started. Waiting for connection event (timeout: 10s)...\")\n",
        "\n",
        "    # Wait for the on_open callback to set the event, with a timeout.\n",
        "    was_connected = websocket_connected_event.wait(timeout=10.0)\n",
        "\n",
        "    if was_connected:\n",
        "        logger.info(\"WebSocket connection event received and confirmed.\")\n",
        "    else:\n",
        "        logger.error(\"Timed out waiting for WebSocket connection to open.\")\n",
        "\n",
        "    return was_connected\n",
        "\n",
        "\n",
        "def stop_upstox_websocket():\n",
        "    \"\"\"\n",
        "    Stops the WebSocket connection and cleans up resources.\n",
        "    \"\"\"\n",
        "    global logger, websocket_thread_global, upstox_market_streamer_global, stop_websocket_flag_global, websocket_connected_event\n",
        "\n",
        "    logger.info(\"Attempting to stop Upstox WebSocket...\")\n",
        "    stop_websocket_flag_global.set()\n",
        "\n",
        "    if upstox_market_streamer_global:\n",
        "        try:\n",
        "            upstox_market_streamer_global.disconnect()\n",
        "            logger.info(\"WebSocket disconnect signal sent.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling disconnect(): {e}\", exc_info=True)\n",
        "\n",
        "    # Ensure the event is cleared on shutdown\n",
        "    websocket_connected_event.clear()\n",
        "\n",
        "    upstox_market_streamer_global = None\n",
        "    if websocket_thread_global and websocket_thread_global.is_alive():\n",
        "        websocket_thread_global.join(timeout=3)\n",
        "    websocket_thread_global = None\n",
        "    logger.info(\"Upstox WebSocket stop process completed and resources cleared.\")\n",
        "\n",
        "# --- Live Trading API Call Functions (Upstox) ---\n",
        "async def place_upstox_order_live(\n",
        "    instrument_key: str, quantity: int, transaction_type: str, order_type: str,\n",
        "    price: float = 0.0, trigger_price: float = 0.0, tag: Optional[str] = None,\n",
        "    max_retries: Optional[int] = None, retry_delay: int = 5,\n",
        "    base_price_for_limit: float = 0.0\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Places an order on Upstox. Now includes internal logic to calculate limit prices for entry orders.\n",
        "    \"\"\"\n",
        "    # Ensure the required config variable is in the global scope\n",
        "    global logger, upstox_api_client_global, UpstoxApiException, upstox_client, UPSTOX_PRODUCT_TYPE, UPSTOX_ORDER_VALIDITY, MAX_ORDER_RETRY_ATTEMPTS, send_telegram_message, ENTRY_LIMIT_PRICE_BUFFER_PCT, api_rate_limiter\n",
        "\n",
        "    if not upstox_api_client_global: logger.error(f\"Order Fail {instrument_key}: API client not init.\"); return None\n",
        "    eff_retries = max_retries if max_retries is not None else MAX_ORDER_RETRY_ATTEMPTS\n",
        "    try: order_api = upstox_client.OrderApi(upstox_api_client_global)\n",
        "    except Exception as e: logger.error(f\"Order Fail {instrument_key}: Could not init OrderApi: {e}\"); return None\n",
        "\n",
        "    tt_upper, ot_upper = transaction_type.upper(), order_type.upper()\n",
        "    if tt_upper not in [\"BUY\", \"SELL\"] or ot_upper not in [\"MARKET\", \"LIMIT\", \"SL\", \"SL-M\"] or quantity <= 0:\n",
        "        logger.error(f\"Order Fail {instrument_key}: Invalid params. TT:{tt_upper}, OT:{ot_upper}, Qty:{quantity}\"); return None\n",
        "\n",
        "    order_tag = (tag if tag else f\"Bot_{instrument_key.split('|')[-1][:5]}_{uuid.uuid4().hex[:6]}\"[:30])\n",
        "\n",
        "    eff_price = price\n",
        "    # Check if this is a LIMIT order for a new entry by inspecting the tag\n",
        "    is_limit_entry = (ot_upper == \"LIMIT\" and tag and \"ENTRY\" in tag.upper())\n",
        "\n",
        "    if is_limit_entry:\n",
        "        if base_price_for_limit <= 0:\n",
        "            logger.error(f\"Order Fail {instrument_key}: 'base_price_for_limit' must be > 0 for LIMIT entry orders.\")\n",
        "            return None\n",
        "        # Calculate the actual limit price with the configured buffer internally\n",
        "        buffer = base_price_for_limit * ENTRY_LIMIT_PRICE_BUFFER_PCT\n",
        "        if tt_upper == \"BUY\":\n",
        "            eff_price = base_price_for_limit + buffer # Allow a slightly higher price to improve fill chance\n",
        "        else: # SELL\n",
        "            eff_price = base_price_for_limit - buffer # Allow a slightly lower price to improve fill chance\n",
        "        logger.info(f\"Calculated LIMIT entry price for {instrument_key}: {eff_price:.2f} (Base: {base_price_for_limit:.2f})\")\n",
        "\n",
        "    # These checks now apply to non-entry orders (e.g., SL-L exits) or if price is passed manually\n",
        "    elif ot_upper in [\"LIMIT\", \"SL\"] and price <= 0:\n",
        "        logger.error(f\"Order Fail {instrument_key}: Price > 0 required for non-entry {ot_upper} order.\"); return None\n",
        "    if ot_upper in [\"SL\", \"SL-M\"] and trigger_price <= 0:\n",
        "        logger.error(f\"Order Fail {instrument_key}: Trigger price > 0 for {ot_upper} order.\"); return None\n",
        "\n",
        "    place_req = upstox_client.PlaceOrderRequest(\n",
        "        quantity=int(quantity), product=UPSTOX_PRODUCT_TYPE, validity=UPSTOX_ORDER_VALIDITY,\n",
        "        price=round(float(eff_price), 2) if ot_upper in [\"LIMIT\", \"SL\"] else 0.0,\n",
        "        instrument_token=instrument_key, order_type=ot_upper, transaction_type=tt_upper,\n",
        "        disclosed_quantity=0, trigger_price=round(float(trigger_price), 2) if ot_upper in [\"SL\", \"SL-M\"] else 0.0, tag=order_tag)\n",
        "\n",
        "    log_attempt = f\"Attempt Order: {place_req.transaction_type} {place_req.quantity} {instrument_key.split('|')[-1]} OT:{place_req.order_type} P:{place_req.price} TrgP:{place_req.trigger_price} Tag:{place_req.tag}\"\n",
        "    logger.info(log_attempt); await send_telegram_message(f\"üÖøÔ∏è {log_attempt}\")\n",
        "\n",
        "    for attempt in range(eff_retries):\n",
        "        try:\n",
        "            await api_rate_limiter.get_token() # Proactive rate limiting\n",
        "            api_res = await asyncio.to_thread(order_api.place_order, body=place_req, api_version=\"2.0\")\n",
        "            if (\n",
        "                hasattr(api_res, 'status') and str(api_res.status).lower() == 'success' and\n",
        "                hasattr(api_res, 'data') and api_res.data and\n",
        "                hasattr(api_res.data, 'order_id') and api_res.data.order_id\n",
        "            ):\n",
        "                order_id = api_res.data.order_id\n",
        "                success_msg = f\"Order Placed for {instrument_key.split('|')[-1]}. ID: {order_id}\"\n",
        "                logger.info(success_msg)\n",
        "                await send_telegram_message(f\"‚úÖ {success_msg}\")\n",
        "                return order_id\n",
        "            else:\n",
        "                logger.error(\n",
        "                    f\"Order Fail {instrument_key.split('|')[-1]} (Att {attempt+1}). \"\n",
        "                    f\"Status:{getattr(api_res,'status','N/A')}, \"\n",
        "                    f\"Detail:{getattr(api_res, 'message', getattr(api_res, 'errors', 'Unknown'))}\"\n",
        "                )\n",
        "        except UpstoxApiException as e_sdk:\n",
        "            logger.error(\n",
        "                f\"UpstoxApiException order {instrument_key.split('|')[-1]} (Att {attempt+1}): \"\n",
        "                f\"{e_sdk.status}-{e_sdk.reason}. Body:{str(e_sdk.body)[:200]}\",\n",
        "                exc_info=False\n",
        "            )\n",
        "\n",
        "            if e_sdk.status == 401:\n",
        "                logger.critical(\"CRITICAL: Order failed (401). Token issue. Re-init attempt.\")\n",
        "                await initialize_upstox_client()\n",
        "                await asyncio.sleep(retry_delay)\n",
        "                try:\n",
        "                    order_api = upstox_client.OrderApi(upstox_api_client_global)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            elif e_sdk.status == 429:\n",
        "                logger.warning(f\"Rate limit order {instrument_key.split('|')[-1]}. Retrying in {retry_delay * (attempt + 2)}s...\")\n",
        "                await asyncio.sleep(retry_delay * (attempt + 2))\n",
        "\n",
        "            elif e_sdk.status in [400, 403]:\n",
        "               await send_telegram_message(\n",
        "                    f\"‚ùå Order Failed (Non-Retryable {e_sdk.status}) for {tt_upper} {instrument_key.split('|')[-1]}. R: {e_sdk.reason}\"\n",
        "                )\n",
        "               return None\n",
        "\n",
        "            else:\n",
        "                await asyncio.sleep(retry_delay)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"General error order {instrument_key.split('|')[-1]} (Att {attempt+1}): {e}\", exc_info=True)\n",
        "            await asyncio.sleep(retry_delay)\n",
        "\n",
        "        if attempt == eff_retries - 1:\n",
        "            logger.error(f\"Max retries order {instrument_key.split('|')[-1]}. Failed.\")\n",
        "            await send_telegram_message(f\"‚ùå Order Failed (Max Retries) for {tt_upper} {instrument_key.split('|')[-1]}.\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "async def cancel_upstox_order_live(order_id: str, max_retries: int = 3, retry_delay: int = 3) -> bool:\n",
        "    global logger, upstox_api_client_global, UpstoxApiException, upstox_client, send_telegram_message, api_rate_limiter\n",
        "\n",
        "    if not upstox_api_client_global:\n",
        "        logger.error(f\"Order Cancel Fail ({order_id}): API client not initialized.\"); return False\n",
        "    if not order_id:\n",
        "        logger.error(\"Order Cancel Fail: No order_id provided.\"); return False\n",
        "\n",
        "    try:\n",
        "        order_api = upstox_client.OrderApi(upstox_api_client_global)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Order Cancel Fail ({order_id}): Could not initialize OrderApi: {e}\"); return False\n",
        "\n",
        "    logger.info(f\"Attempting to cancel order ID: {order_id}\")\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            await api_rate_limiter.get_token() # Proactive rate limiting\n",
        "            api_res = await asyncio.to_thread(\n",
        "                order_api.cancel_order,\n",
        "                api_version=\"2.0\",\n",
        "                order_id=order_id\n",
        "            )\n",
        "            if (hasattr(api_res, 'status') and str(api_res.status).lower() == 'success' and\n",
        "                hasattr(api_res, 'data') and api_res.data and\n",
        "                getattr(api_res.data, 'order_id', None) == order_id):\n",
        "\n",
        "                success_msg = f\"Order Cancelled Successfully. ID: {order_id}\"\n",
        "                logger.info(success_msg)\n",
        "                await send_telegram_message(f\"‚úÖ {success_msg}\")\n",
        "                return True\n",
        "            else:\n",
        "                err_detail = getattr(api_res, 'message', getattr(api_res, 'errors', 'Unknown error'))\n",
        "                logger.error(f\"Order Cancel Fail ({order_id}) (Att {attempt+1}). Status: {getattr(api_res,'status','N/A')}, Detail: {err_detail}\")\n",
        "        except UpstoxApiException as e_sdk:\n",
        "            logger.error(f\"UpstoxApiException on cancel ({order_id}) (Att {attempt+1}): {e_sdk.status}-{e_sdk.reason}\", exc_info=False)\n",
        "            if e_sdk.status in [401, 429]: # Unauthorized or Rate Limit\n",
        "                await asyncio.sleep(retry_delay * (attempt + 1))\n",
        "            elif e_sdk.status == 404: # Not found, maybe already executed/cancelled\n",
        "                logger.warning(f\"Order {order_id} not found on cancel (404). It might be already filled or cancelled.\")\n",
        "                return True # Treat as success\n",
        "            else:\n",
        "                await asyncio.sleep(retry_delay)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"General error cancelling order {order_id} (Att {attempt+1}): {e}\", exc_info=True)\n",
        "            await asyncio.sleep(retry_delay)\n",
        "\n",
        "    logger.error(f\"Max retries reached for cancelling order {order_id}. Failed.\")\n",
        "    await send_telegram_message(f\"‚ùå FAILED to cancel order {order_id} after max retries.\")\n",
        "    return False\n",
        "\n",
        "async def get_upstox_funds_and_margin_live(segment: str = \"SEC\", max_retries: int = 2, retry_delay: int = 3) -> Optional[Any]:\n",
        "    global logger, upstox_api_client_global, UpstoxApiException, upstox_client, api_rate_limiter\n",
        "    if not upstox_api_client_global: logger.error(\"Funds/Margin: API client not init.\"); return None\n",
        "    try: user_api = upstox_client.UserApi(upstox_api_client_global)\n",
        "    except Exception as e: logger.error(f\"Funds/Margin: Failed to init UserApi: {e}\"); return None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            await api_rate_limiter.get_token() # Proactive rate limiting\n",
        "            api_res = await asyncio.to_thread(user_api.get_user_fund_margin, api_version=\"2.0\", segment=segment.upper())\n",
        "            if hasattr(api_res, 'status') and str(api_res.status).lower() == 'success' and hasattr(api_res, 'data'):\n",
        "                logger.info(f\"Funds/margin for '{segment}' fetched.\"); return api_res.data\n",
        "            else: logger.warning(f\"Could not fetch funds/margin for '{segment}' (Att {attempt+1}). Status:{getattr(api_res,'status','N/A')}, Msg:{getattr(api_res,'message','N/A')}\")\n",
        "        except UpstoxApiException as e_sdk:\n",
        "            logger.error(f\"UpstoxApiException funds/margin '{segment}' (Att {attempt+1}): {e_sdk.status}-{e_sdk.reason}\", exc_info=False)\n",
        "            if e_sdk.status == 401: logger.critical(\"Funds/Margin fetch (401). Token issue.\"); await initialize_upstox_client(); await asyncio.sleep(retry_delay)\n",
        "            elif e_sdk.status == 429: await asyncio.sleep(retry_delay * (attempt + 2))\n",
        "            elif e_sdk.status == 403: return None\n",
        "            else: await asyncio.sleep(retry_delay)\n",
        "        except Exception as ex: logger.error(f\"General error funds/margin '{segment}' (Att {attempt+1}): {ex}\", exc_info=True); await asyncio.sleep(retry_delay)\n",
        "        if attempt == max_retries - 1: logger.error(f\"Max retries funds/margin for '{segment}'.\"); return None\n",
        "    return None\n",
        "\n",
        "async def get_upstox_order_details_live(order_id: str, max_retries: int = 3, retry_delay: int = 3) -> Optional[List[Any]]:\n",
        "    global logger, upstox_api_client_global, UpstoxApiException, upstox_client, api_rate_limiter\n",
        "    if not upstox_api_client_global: logger.error(f\"Order Details ({order_id}): API client not init.\"); return None\n",
        "    if not order_id: logger.error(\"Order Details: No order_id.\"); return None\n",
        "    try: order_api = upstox_client.OrderApi(upstox_api_client_global)\n",
        "    except Exception as e: logger.error(f\"Order Details ({order_id}): Failed to init OrderApi: {e}\"); return None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            await api_rate_limiter.get_token() # Proactive rate limiting\n",
        "            api_res = await asyncio.to_thread(order_api.get_order_history, api_version=\"2.0\", order_id=order_id)\n",
        "            if hasattr(api_res, 'status') and str(api_res.status).lower() == 'success' and hasattr(api_res, 'data'):\n",
        "                logger.info(f\"Order details for '{order_id}' fetched ({len(api_res.data if api_res.data else [])} legs).\"); return api_res.data\n",
        "            else: logger.warning(f\"Could not fetch order details '{order_id}' (Att {attempt+1}). Status:{getattr(api_res,'status','N/A')}, Msg:{getattr(api_res,'message','N/A')}\")\n",
        "        except UpstoxApiException as e_sdk:\n",
        "            logger.error(f\"UpstoxApiException order details '{order_id}' (Att {attempt+1}): {e_sdk.status}-{e_sdk.reason}\", exc_info=False)\n",
        "            if e_sdk.status == 401: logger.critical(f\"Order details fetch '{order_id}' (401). Token issue.\"); await initialize_upstox_client(); await asyncio.sleep(retry_delay)\n",
        "            elif e_sdk.status == 429: await asyncio.sleep(retry_delay * (attempt + 2))\n",
        "            elif e_sdk.status in [403, 404]: return None\n",
        "            else: await asyncio.sleep(retry_delay)\n",
        "        except Exception as ex: logger.error(f\"General error order details '{order_id}' (Att {attempt+1}): {ex}\", exc_info=True); await asyncio.sleep(retry_delay)\n",
        "        if attempt == max_retries - 1: logger.error(f\"Max retries order details for '{order_id}'.\"); return None\n",
        "    return None\n",
        "\n",
        "async def get_upstox_positions_live(max_retries: int = 2, retry_delay: int = 3) -> Optional[List[Any]]:\n",
        "    global logger, upstox_api_client_global, UpstoxApiException, upstox_client, api_rate_limiter\n",
        "    if not upstox_api_client_global: logger.error(\"Positions: API client not init.\"); return None\n",
        "    try: portfolio_api = upstox_client.PortfolioApi(upstox_api_client_global)\n",
        "    except Exception as e: logger.error(f\"Positions: Failed to init PortfolioApi: {e}\"); return None\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            await api_rate_limiter.get_token() # Proactive rate limiting\n",
        "            api_res = await asyncio.to_thread(portfolio_api.get_positions, api_version=\"2.0\")\n",
        "            if hasattr(api_res, 'status') and str(api_res.status).lower() == 'success' and hasattr(api_res, 'data'):\n",
        "                logger.info(f\"Positions fetched ({len(api_res.data if api_res.data else [])} positions).\"); return api_res.data\n",
        "            else: logger.warning(f\"Could not fetch positions (Att {attempt+1}). Status:{getattr(api_res,'status','N/A')}, Msg:{getattr(api_res,'message','N/A')}\")\n",
        "        except UpstoxApiException as e_sdk:\n",
        "            logger.error(f\"UpstoxApiException positions (Att {attempt+1}): {e_sdk.status}-{e_sdk.reason}\", exc_info=False)\n",
        "            if e_sdk.status == 401: logger.critical(\"Positions fetch (401). Token issue.\"); await initialize_upstox_client(); await asyncio.sleep(retry_delay);\n",
        "            elif e_sdk.status == 429: await asyncio.sleep(retry_delay * (attempt + 2))\n",
        "            else: await asyncio.sleep(retry_delay)\n",
        "        except Exception as ex: logger.error(f\"General error positions (Att {attempt+1}): {ex}\", exc_info=True); await asyncio.sleep(retry_delay)\n",
        "        if attempt == max_retries - 1: logger.error(\"Max retries positions.\"); return None\n",
        "    return None\n",
        "\n",
        "# --- Risk, State, and Time Management Functions ---\n",
        "async def _calculate_current_max_daily_loss_live() -> float:\n",
        "    global logger, calculated_max_daily_loss_global, portfolio_available_margin, MAX_DAILY_LOSS_FIXED_CONFIG, MAX_DAILY_LOSS_MARGIN_THRESHOLD_CONFIG, MAX_DAILY_LOSS_MARGIN_PERCENTAGE_CONFIG\n",
        "    current_max_loss = MAX_DAILY_LOSS_FIXED_CONFIG\n",
        "    if portfolio_available_margin > 0:\n",
        "        if portfolio_available_margin <= MAX_DAILY_LOSS_MARGIN_THRESHOLD_CONFIG: current_max_loss = MAX_DAILY_LOSS_FIXED_CONFIG\n",
        "        else: current_max_loss = portfolio_available_margin * MAX_DAILY_LOSS_MARGIN_PERCENTAGE_CONFIG\n",
        "    else: logger.warning(f\"Max Daily Loss Calc: Margin ‚Çπ{portfolio_available_margin:.2f}. Using fixed limit ‚Çπ{MAX_DAILY_LOSS_FIXED_CONFIG:.2f}.\")\n",
        "    calculated_max_daily_loss_global = abs(current_max_loss)\n",
        "    logger.info(f\"Max Daily Portfolio Loss Limit set to: ‚Çπ{calculated_max_daily_loss_global:.2f} (Margin ‚Çπ{portfolio_available_margin:.2f})\")\n",
        "    return calculated_max_daily_loss_global\n",
        "\n",
        "async def _reset_daily_limits_and_state_live():\n",
        "    \"\"\"\n",
        "    Performs the daily state reset at the start of the trading day.\n",
        "    This enhanced version now includes the logic for initial capital allocation\n",
        "    based on the selected CAPITAL_ALLOCATION_MODE.\n",
        "    \"\"\"\n",
        "    global logger, portfolio_daily_pnl_achieved, portfolio_trades_today_count, is_trading_halted_for_day_global, last_daily_reset_date_global, can_place_new_order_today_global, NSE_TZ, MARKET_HOLIDAYS, live_states_by_symbol, CONSECUTIVE_LOSS_DAYS_HALT_THRESHOLD, portfolio_available_margin, MAX_TRADES_PER_SYMBOL_PER_DAY, MAX_TRADES_PER_DAY_GLOBAL, calculated_max_portfolio_trades_today, selected_symbols_for_session, UPSTOX_INSTRUMENT_KEYS, get_market_holidays_upstox, send_telegram_message, adapt_strategy_parameters_for_symbol, CAPITAL_ALLOCATION_MODE, capital_per_symbol_allowance, SYMBOLS_REQUIRING_RETRAINING\n",
        "\n",
        "    now_nse_date = datetime.now(NSE_TZ).date()\n",
        "    logger.info(f\"--- Performing Daily Reset for Trading Day: {now_nse_date.strftime('%Y-%m-%d')} ---\")\n",
        "\n",
        "    # Fetch latest funds and margin information\n",
        "    funds_data = await get_upstox_funds_and_margin_live(\"SEC\")\n",
        "    portfolio_available_margin = 0.0\n",
        "    if funds_data and hasattr(funds_data, 'equity') and funds_data.equity and hasattr(funds_data.equity, 'available_margin') and funds_data.equity.available_margin is not None:\n",
        "        try:\n",
        "            portfolio_available_margin = float(funds_data.equity.available_margin)\n",
        "        except (ValueError, TypeError):\n",
        "            logger.error(\"Could not parse available margin. Setting to 0.\")\n",
        "    else:\n",
        "        logger.warning(\"Failed to fetch/parse available margin. Margin set to 0.\")\n",
        "\n",
        "    # Reset core portfolio state variables\n",
        "    await _calculate_current_max_daily_loss_live()\n",
        "    portfolio_daily_pnl_achieved = 0.0\n",
        "    portfolio_trades_today_count = 0\n",
        "    is_trading_halted_for_day_global = False\n",
        "    can_place_new_order_today_global = True\n",
        "    last_daily_reset_date_global = now_nse_date\n",
        "    globals()['_eod_sq_off_done_today_flag'] = False\n",
        "    globals()['_eod_consecutive_loss_processed_flag'] = False\n",
        "\n",
        "    # Determine number of active symbols to calculate max trades\n",
        "    active_symbols_list = [s for s in selected_symbols_for_session if s.upper() in UPSTOX_INSTRUMENT_KEYS and not live_states_by_symbol.get(s.upper(), {}).get('is_halted_for_performance', False) and \"INVALID_KEY\" not in UPSTOX_INSTRUMENT_KEYS.get(s.upper(),'')]\n",
        "    calculated_max_portfolio_trades_today = len(active_symbols_list) * MAX_TRADES_PER_SYMBOL_PER_DAY\n",
        "    if MAX_TRADES_PER_DAY_GLOBAL > 0:\n",
        "        calculated_max_portfolio_trades_today = min(calculated_max_portfolio_trades_today, MAX_TRADES_PER_DAY_GLOBAL)\n",
        "    logger.info(f\"Dynamic max portfolio trades today: {calculated_max_portfolio_trades_today} ({len(active_symbols_list)} active symbols * {MAX_TRADES_PER_SYMBOL_PER_DAY} trades/sym).\")\n",
        "\n",
        "    # Dynamic Capital Allocation Logic\n",
        "    if not active_symbols_list:\n",
        "        capital_per_symbol_allowance.clear()\n",
        "        logger.warning(\"No active symbols to allocate capital to.\")\n",
        "    elif CAPITAL_ALLOCATION_MODE == 'EQUAL' or CAPITAL_ALLOCATION_MODE == 'DYNAMIC_PNL':\n",
        "        capital_per_symbol = portfolio_available_margin / len(active_symbols_list) if len(active_symbols_list) > 0 else 0\n",
        "        capital_per_symbol_allowance.clear()\n",
        "        for sym in active_symbols_list:\n",
        "            capital_per_symbol_allowance[sym.upper()] = capital_per_symbol\n",
        "        logger.info(f\"Capital allocation mode '{CAPITAL_ALLOCATION_MODE}': Initial capital set to ‚Çπ{capital_per_symbol:,.2f} for each of the {len(active_symbols_list)} active symbols.\")\n",
        "\n",
        "    # Reset individual symbol states for the new day\n",
        "    for sym_name, sym_state in list(live_states_by_symbol.items()):\n",
        "        sym_state['last_trading_day_net_pnl_symbol'] = sym_state.get('daily_pnl_symbol', 0.0)\n",
        "        sym_state['trades_today_symbol_prev_day'] = sym_state.get('trades_today_symbol', 0)\n",
        "        if sym_state['last_trading_day_net_pnl_symbol'] < 0:\n",
        "            sym_state['consecutive_loss_days'] = sym_state.get('consecutive_loss_days', 0) + 1\n",
        "        elif sym_state['trades_today_symbol_prev_day'] > 0:\n",
        "            sym_state['consecutive_loss_days'] = 0\n",
        "\n",
        "        if sym_state.get('consecutive_loss_days', 0) >= CONSECUTIVE_LOSS_DAYS_HALT_THRESHOLD and not sym_state.get('is_halted_for_performance', False):\n",
        "            sym_state['is_halted_for_performance'] = True\n",
        "            if sym_name.upper() not in SYMBOLS_REQUIRING_RETRAINING:\n",
        "                SYMBOLS_REQUIRING_RETRAINING.append(sym_name.upper())\n",
        "            halt_msg = f\"‚ö†Ô∏è SYMBOL HALTED (Perf): {sym_name} ({sym_state.get('consecutive_loss_days', 0)} loss days). Retraining needed.\"\n",
        "            logger.warning(halt_msg)\n",
        "            await send_telegram_message(halt_msg)\n",
        "\n",
        "        sym_state['daily_pnl_symbol'] = 0.0\n",
        "        sym_state['trades_today_symbol'] = 0\n",
        "        await adapt_strategy_parameters_for_symbol(sym_name)\n",
        "\n",
        "    if upstox_api_client_global:\n",
        "        # --- - Run the blocking call in a separate thread ---\n",
        "        await asyncio.to_thread(get_market_holidays_upstox, now_nse_date.year)\n",
        "    else:\n",
        "        logger.warning(\"Upstox client not ready for holiday fetch during daily reset.\")\n",
        "\n",
        "    await send_telegram_message(f\"‚òÄÔ∏è **Daily Reset Complete {now_nse_date.strftime('%Y-%m-%d')}** ‚òÄÔ∏è\\n  Mode: {CAPITAL_ALLOCATION_MODE}\\n  Margin: ‚Çπ{portfolio_available_margin:,.2f}\\n  Max Loss: ‚Çπ{calculated_max_daily_loss_global:,.2f}\\n  Max Trades: {calculated_max_portfolio_trades_today}\")\n",
        "\n",
        "    save_state_to_json()\n",
        "\n",
        "\n",
        "def is_market_open_now_live(current_nse_datetime: datetime) -> bool:\n",
        "    global logger, MARKET_OPEN_TIME_STR, MARKET_CLOSE_TIME_STR, MARKET_HOLIDAYS\n",
        "    try: open_t = datetime.strptime(MARKET_OPEN_TIME_STR, \"%H:%M:%S\").time(); close_t = datetime.strptime(MARKET_CLOSE_TIME_STR, \"%H:%M:%S\").time()\n",
        "    except ValueError: logger.error(\"Invalid market open/close time format. Assuming market closed.\"); return False\n",
        "    curr_d, curr_t = current_nse_datetime.date(), current_nse_datetime.time()\n",
        "    if curr_d.weekday() >= 5: return False # Sat/Sun\n",
        "    if MARKET_HOLIDAYS and curr_d in MARKET_HOLIDAYS: return False\n",
        "    return open_t <= curr_t < close_t\n",
        "\n",
        "async def _check_and_handle_time_based_rules_live(current_nse_datetime: datetime) -> bool:\n",
        "    global logger, live_states_by_symbol, selected_symbols_for_session, can_place_new_order_today_global, NO_NEW_ENTRY_AFTER_TIME_STR\n",
        "    global SQUARE_OFF_ALL_START_TIME_STR, SQUARE_OFF_ALL_END_TIME_STR, AUTO_ORDER_EXECUTION_ENABLED, UPSTOX_INSTRUMENT_KEYS\n",
        "    global portfolio_daily_pnl_achieved, is_trading_halted_for_day_global, EXIT_ORDER_TYPE, BACKTEST_TRANSACTION_COST_PCT, NSE_TZ, send_telegram_message, ENTRY_ORDER_TYPE_DEFAULT\n",
        "\n",
        "    now_time = current_nse_datetime.time(); eod_sq_off_done = globals().get('_eod_sq_off_done_today_flag', False)\n",
        "    try: # No new entries\n",
        "        no_new_entry_t = datetime.strptime(NO_NEW_ENTRY_AFTER_TIME_STR, \"%H:%M:%S\").time()\n",
        "        if now_time >= no_new_entry_t and can_place_new_order_today_global:\n",
        "            logger.info(f\"Past NO_NEW_ENTRY_AFTER_TIME ({NO_NEW_ENTRY_AFTER_TIME_STR}). No new global entries.\"); await send_telegram_message(f\"üö´ No new global entries after {NO_NEW_ENTRY_AFTER_TIME_STR}.\")\n",
        "            can_place_new_order_today_global = False\n",
        "            ## --- - Save state after time-based rule change ---\n",
        "            save_state_to_json()\n",
        "    except ValueError: logger.error(f\"Invalid NO_NEW_ENTRY_AFTER_TIME_STR format: '{NO_NEW_ENTRY_AFTER_TIME_STR}'. Rule ignored.\")\n",
        "    try: # EOD Square-Off\n",
        "        sq_off_start_t = datetime.strptime(SQUARE_OFF_ALL_START_TIME_STR, \"%H:%M:%S\").time(); sq_off_end_t = datetime.strptime(SQUARE_OFF_ALL_END_TIME_STR, \"%H:%M:%S\").time()\n",
        "        if sq_off_start_t <= now_time < sq_off_end_t and not eod_sq_off_done:\n",
        "            if not is_trading_halted_for_day_global: logger.info(f\"EOD Square-Off window active. Closing positions.\"); await send_telegram_message(f\"‚è≥ EOD Square-Off Active. Closing positions.\")\n",
        "            any_closed_eod = False\n",
        "            for sym, state in list(live_states_by_symbol.items()):\n",
        "                if state.get('current_position') != 'None' and state.get('current_order_quantity', 0) > 0:\n",
        "                    instr_key = UPSTOX_INSTRUMENT_KEYS.get(sym.upper()); qty = state['current_order_quantity']; pos_type = state['current_position']; entry_p = state['entry_price']\n",
        "                    if not instr_key or \"INVALID_KEY\" in instr_key: logger.error(f\"EOD SqOff {sym}: Invalid instr key.\"); continue\n",
        "                    exit_action = \"BUY\" if pos_type == \"Short\" else \"SELL\"; eod_tag = f\"EOD_SQ_{sym[:5]}_{uuid.uuid4().hex[:4]}\"\n",
        "                    logger.info(f\"EOD SqOff: {exit_action} {qty} {sym} at {EXIT_ORDER_TYPE}.\")\n",
        "                    eod_order_id, fill_p, fill_q = f\"SIM_EOD_{sym}\", entry_p, qty # Defaults for manual/sim\n",
        "                    if AUTO_ORDER_EXECUTION_ENABLED:\n",
        "                        eod_order_id = await place_upstox_order_live(instr_key, qty, exit_action, EXIT_ORDER_TYPE, tag=eod_tag) # Market order for EOD\n",
        "                        if eod_order_id: # Attempt to confirm fill\n",
        "                            await send_telegram_message(f\"üî∑ EOD SqOff Order Sent: {exit_action} {qty} {sym}. ID: {eod_order_id}\")\n",
        "                            for _ in range(3): # Poll for 3 times\n",
        "                                await asyncio.sleep(2 + _); order_details = await get_upstox_order_details_live(eod_order_id)\n",
        "                                if order_details:\n",
        "                                    for leg in order_details:\n",
        "                                        if str(leg.status).lower() == 'complete':\n",
        "                                            fill_q = leg.filled_quantity if hasattr(leg,'filled_quantity') and leg.filled_quantity else qty\n",
        "                                            avg_p = leg.average_price if hasattr(leg,'average_price') and leg.average_price else entry_p\n",
        "                                            if fill_q == qty and avg_p > 0: fill_p = avg_p; logger.info(f\"EOD Fill Confirmed {sym}: {fill_q} @ ‚Çπ{fill_p:.2f}\"); break\n",
        "                                    else: continue\n",
        "                                    break\n",
        "                            if not (fill_q == qty and fill_p > 0 and abs(fill_p - entry_p) > 1e-5): logger.warning(f\"EOD Fill for {sym} (ID {eod_order_id}) not fully confirmed. Using estimated fill: ‚Çπ{fill_p:.2f}\")\n",
        "                    else: await send_telegram_message(f\"üî∑ EOD SQ-OFF ALERT (Manual): Close {pos_type} {qty} for {sym}.\")\n",
        "\n",
        "                    pnl_g = (fill_p - entry_p if pos_type == 'Long' else entry_p - fill_p) * fill_q\n",
        "                    costs = (abs(entry_p*fill_q) + abs(fill_p*fill_q)) * BACKTEST_TRANSACTION_COST_PCT; pnl_n = pnl_g - costs\n",
        "                    state['daily_pnl_symbol'] = state.get('daily_pnl_symbol', 0.0) + pnl_n; portfolio_daily_pnl_achieved += pnl_n\n",
        "                    log_trade_to_db({'timestamp':datetime.now(NSE_TZ).isoformat(),'symbol':sym,'type':pos_type,'action':'EXIT_EOD_SQUARE_OFF','price':fill_p,'qty':fill_q,'order_id':eod_order_id,'pnl_trade':pnl_n,'reason':'EOD_WINDOW','daily_pnl_symbol':state['daily_pnl_symbol'],'daily_pnl_portfolio':portfolio_daily_pnl_achieved})\n",
        "                    state.update({'current_position':'None','active_entry_order_id':None,'current_order_quantity':0,'last_trading_day_net_pnl_symbol':state['daily_pnl_symbol']}); any_closed_eod = True\n",
        "            if any_closed_eod or not any(s.get('current_position') != 'None' for s in live_states_by_symbol.values()):\n",
        "                if not is_trading_halted_for_day_global: logger.info(\"EOD SqOff complete. Halting new entries.\"); await send_telegram_message(\"üõë EOD SqOff Complete. New entries halted.\")\n",
        "                is_trading_halted_for_day_global = True; can_place_new_order_today_global = False; globals()['_eod_sq_off_done_today_flag'] = True\n",
        "                ## --- - Save state after EOD halt ---\n",
        "                save_state_to_json()\n",
        "    except ValueError: logger.error(f\"Invalid EOD time format. Rule ignored.\")\n",
        "    return is_trading_halted_for_day_global\n",
        "\n",
        "async def _handle_live_sl_tp_exit_for_symbol(symbol_name: str, current_ltp_sym: float, active_positions_set: set) -> bool:\n",
        "    \"\"\"\n",
        "    Handles Stop-Loss (SL) and Take-Profit (TP) logic. REVISED to be fault-tolerant.\n",
        "    It now saves the *intent* to exit before the API call to prevent state desync on crash.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, UPSTOX_INSTRUMENT_KEYS, AUTO_ORDER_EXECUTION_ENABLED, EXIT_ORDER_TYPE, send_telegram_message, NSE_TZ\n",
        "\n",
        "    sym_upper, sym_state = symbol_name.upper(), live_states_by_symbol.get(symbol_name.upper())\n",
        "    # This function now only acts if a position is fully active (not pending exit or pending an API call)\n",
        "    if not sym_state or sym_state.get('current_position') == 'None' or sym_state.get('pending_exit_order_id') or sym_state.get('pending_exit_api_call'):\n",
        "        return False\n",
        "\n",
        "    pos_type, entry_p, sl_target, tp_target, qty_held = sym_state['current_position'], sym_state['entry_price'], sym_state['current_sl_price'], sym_state['current_tp_price'], sym_state['current_order_quantity']\n",
        "    instr_key = UPSTOX_INSTRUMENT_KEYS.get(sym_upper)\n",
        "    if not instr_key or \"INVALID_KEY\" in instr_key:\n",
        "        logger.error(f\"SL/TP Exit {sym_upper}: Invalid instrument key.\")\n",
        "        return False\n",
        "\n",
        "    exit_reason = None\n",
        "    if pos_type == 'Long':\n",
        "        if sl_target > 0 and current_ltp_sym <= sl_target: exit_reason = \"SL_HIT\"\n",
        "        elif tp_target > 0 and current_ltp_sym >= tp_target: exit_reason = \"TP_HIT\"\n",
        "    elif pos_type == 'Short':\n",
        "        if sl_target > 0 and current_ltp_sym >= sl_target: exit_reason = \"SL_HIT\"\n",
        "        elif tp_target > 0 and current_ltp_sym <= tp_target: exit_reason = \"TP_HIT\"\n",
        "\n",
        "    if exit_reason:\n",
        "        logger.info(f\"--- {exit_reason} TRIGGERED for {pos_type} on {sym_upper} ---\")\n",
        "        exit_action = \"BUY\" if pos_type == \"Short\" else \"SELL\"\n",
        "        sltp_tag = f\"{exit_reason[:6]}_{sym_upper[:5]}_{uuid.uuid4().hex[:4]}\"\n",
        "\n",
        "        # --- Save state BEFORE the API call ---\n",
        "        # 1. Flag the state as \"pending an API call\".\n",
        "        sym_state['pending_exit_api_call'] = sltp_tag\n",
        "        sym_state['pending_exit_reason'] = exit_reason\n",
        "        save_state_to_json()\n",
        "        logger.debug(f\"State saved for {sym_upper} BEFORE placing exit order.\")\n",
        "\n",
        "        exit_order_id = None\n",
        "        try:\n",
        "            if AUTO_ORDER_EXECUTION_ENABLED:\n",
        "                # 2. Make the potentially risky API call.\n",
        "                exit_order_id = await place_upstox_order_live(instr_key, qty_held, exit_action, EXIT_ORDER_TYPE, tag=sltp_tag)\n",
        "            else:\n",
        "                # In manual mode, we simulate a successful API call.\n",
        "                exit_order_id = f\"SIM_EXIT_{exit_reason}\"\n",
        "\n",
        "            # --- Save state AFTER the API call succeeds ---\n",
        "            if exit_order_id:\n",
        "                # 3. If the call succeeds, update the state with the real order ID and remove the pending flag.\n",
        "                sym_state['pending_exit_order_id'] = exit_order_id\n",
        "                sym_state['exit_reason'] = sym_state.pop('pending_exit_reason', exit_reason) # Use stored reason\n",
        "                sym_state.pop('pending_exit_api_call', None) # Remove the pending flag\n",
        "                sym_state['pending_exit_since'] = datetime.now(NSE_TZ)\n",
        "\n",
        "                active_positions_set.discard(sym_upper)\n",
        "                log_trade_to_db({'timestamp':datetime.now(NSE_TZ).isoformat(), 'symbol':sym_upper, 'type':pos_type, 'action':f'EXIT_TRIGGERED_{exit_reason}', 'price':current_ltp_sym, 'qty':qty_held, 'order_id':exit_order_id, 'reason':f'LTP_based_trigger'})\n",
        "                save_state_to_json()\n",
        "                logger.debug(f\"State saved for {sym_upper} AFTER placing exit order {exit_order_id}.\")\n",
        "                return True\n",
        "            else:\n",
        "                # 4. If the API call fails gracefully (returns None), clear the pending flag.\n",
        "                logger.error(f\"SL/TP exit order placement FAILED for {sym_upper}. Position remains open.\")\n",
        "                await send_telegram_message(f\"üö® CRITICAL: FAILED to place {exit_reason} order for {sym_upper}. MANUAL INTERVENTION REQUIRED!\")\n",
        "                sym_state.pop('pending_exit_api_call', None)\n",
        "                sym_state.pop('pending_exit_reason', None)\n",
        "                save_state_to_json()\n",
        "\n",
        "        except Exception as e:\n",
        "            # 5. If the API call crashes, the 'pending_exit_api_call' flag remains in the saved state.\n",
        "            # The reconciliation logic at startup will handle this.\n",
        "            logger.error(f\"Exception during exit order placement for {sym_upper}: {e}\", exc_info=True)\n",
        "\n",
        "    return False\n",
        "\n",
        "# --- Live Trade Logging ---\n",
        "def log_trade_to_db(trade_details: dict):\n",
        "\n",
        "    global logger, NSE_TZ, append_record_to_db\n",
        "\n",
        "    try:\n",
        "        sym_name = trade_details.get('symbol', 'UNKNOWN').upper()\n",
        "        if sym_name == 'UNKNOWN':\n",
        "            logger.error(\"Cannot log trade to DB: Symbol name is missing from trade_details.\")\n",
        "            return\n",
        "\n",
        "        # Ensure the timestamp is in the correct string format for the database\n",
        "        ts_log = trade_details.get('timestamp')\n",
        "        if isinstance(ts_log, datetime):\n",
        "            trade_details['timestamp'] = ts_log.isoformat()\n",
        "        else:\n",
        "            trade_details['timestamp'] = str(ts_log)\n",
        "\n",
        "        header = [\n",
        "            'timestamp','symbol','type','action','price','qty','order_id',\n",
        "            'pnl_trade','reason','sl_price','tp_price','atr_at_entry',\n",
        "            'confidence','daily_pnl_symbol','daily_pnl_portfolio'\n",
        "        ]\n",
        "\n",
        "        # Create a clean log entry dictionary with all possible keys, defaulting to None if missing\n",
        "        log_entry = {key: trade_details.get(key) for key in header}\n",
        "\n",
        "        append_record_to_db(log_entry, 'trade_logs', sym_name)\n",
        "\n",
        "        # The logger message can be simplified as we no longer need the file path\n",
        "        logger.debug(f\"Successfully logged trade for {sym_name} to database.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error logging trade to database for {trade_details.get('symbol','N/A')}: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "# --- Live Order Quantity Calculation ---\n",
        "def calculate_dynamic_order_quantity_live(stock_price: float, capital_allocated: float, leverage: Optional[float]=None, margin_util_pct: Optional[float]=None) -> int:\n",
        "    global logger, UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER, MARGIN_UTILIZATION_PERCENT\n",
        "    eff_lev = leverage if leverage is not None else UPSTOX_INTRADAY_LEVERAGE_MULTIPLIER\n",
        "    eff_margin_util = margin_util_pct if margin_util_pct is not None else MARGIN_UTILIZATION_PERCENT\n",
        "    if stock_price <= 0 or capital_allocated <= 0: logger.debug(f\"OrderQtyLive: Invalid price (‚Çπ{stock_price:.2f}) or capital (‚Çπ{capital_allocated:.2f}). Qty=0.\"); return 0\n",
        "    actual_margin = capital_allocated * eff_margin_util;\n",
        "    if actual_margin <= 0: logger.debug(f\"OrderQtyLive: Actual margin (‚Çπ{actual_margin:.2f}) <=0. Qty=0.\"); return 0\n",
        "    exposure = actual_margin * eff_lev;\n",
        "    if exposure <= 0: logger.debug(f\"OrderQtyLive: Exposure (‚Çπ{exposure:.2f}) <=0. Qty=0.\"); return 0\n",
        "    qty = int(np.floor(exposure / stock_price))\n",
        "    logger.info(f\"OrderQtyLive: Px=‚Çπ{stock_price:.2f}, CapAlloc=‚Çπ{capital_allocated:.2f} (Utilizing {eff_margin_util*100:.0f}% -> Margin=‚Çπ{actual_margin:.2f}), Lev={eff_lev:.1f}x, Exposure=‚Çπ{exposure:.2f} => Qty={qty}\")\n",
        "    return max(0, qty)\n",
        "\n",
        "# --- Live Model Prediction with Caching ---\n",
        "async def _get_live_prediction_for_symbol(symbol_name: str, feature_sequence_unscaled_df: pd.DataFrame) -> Tuple[Optional[str], float]:\n",
        "    \"\"\"\n",
        "    Makes a live prediction. This REVISED version caches the scaler and encoder\n",
        "    objects in memory to avoid repeated disk I/O.\n",
        "    \"\"\"\n",
        "    global logger, trained_models_by_symbol, MC_DROPOUT_SAMPLES, CLASS_LABELS, LOOKBACK_WINDOW, data_store_by_symbol\n",
        "    sym_upper = symbol_name.upper()\n",
        "    if sym_upper not in trained_models_by_symbol or not trained_models_by_symbol[sym_upper]: logger.warning(f\"LivePred {sym_upper}: No trained models found.\"); return None, 0.0\n",
        "    model_artefacts = trained_models_by_symbol[sym_upper]\n",
        "    feature_cols = data_store_by_symbol.get(sym_upper, {}).get('feature_columns')\n",
        "    if not feature_cols: logger.error(f\"LivePred {sym_upper}: 'feature_columns' not found. Cannot predict.\"); return None, 0.0\n",
        "    if feature_sequence_unscaled_df.shape[0] != LOOKBACK_WINDOW: logger.warning(f\"LivePred {sym_upper}: Input sequence length ({feature_sequence_unscaled_df.shape[0]}) != LOOKBACK_WINDOW ({LOOKBACK_WINDOW}).\"); return None, 0.0\n",
        "    missing_cols = [col for col in feature_cols if col not in feature_sequence_unscaled_df.columns]\n",
        "    if missing_cols: logger.error(f\"LivePred {sym_upper}: Missing features in sequence: {missing_cols}.\"); return None, 0.0\n",
        "\n",
        "    all_model_probs = []\n",
        "    for model_info in model_artefacts:\n",
        "        try:\n",
        "            # --- CACHING LOGIC ---\n",
        "            if 'scaler_object' not in model_info:\n",
        "                model_info['scaler_object'] = joblib.load(model_info['scaler_path'])\n",
        "            if 'encoder_object' not in model_info:\n",
        "                model_info['encoder_object'] = joblib.load(model_info['encoder_path'])\n",
        "            if 'model_object' not in model_info:\n",
        "                model_info['model_object'] = keras_load_model(model_info['model_path'])\n",
        "\n",
        "            scaler = model_info['scaler_object']\n",
        "            model = model_info['model_object']\n",
        "\n",
        "            ordered_features = feature_sequence_unscaled_df[scaler.feature_names_in_ if hasattr(scaler, 'feature_names_in_') else feature_cols]\n",
        "            scaled_np = scaler.transform(ordered_features)\n",
        "            input_reshaped = np.expand_dims(scaled_np, axis=0)\n",
        "            has_dropout = any(isinstance(layer, tf.keras.layers.Dropout) for layer in model.layers)\n",
        "            num_passes = MC_DROPOUT_SAMPLES if MC_DROPOUT_SAMPLES > 0 and has_dropout else 1\n",
        "            pass_probs = [model(input_reshaped, training=(num_passes > 1)).numpy()[0] for _ in range(num_passes)]\n",
        "            if pass_probs: all_model_probs.append(np.mean(pass_probs, axis=0))\n",
        "        except FileNotFoundError as e: logger.error(f\"LivePred {sym_upper}: Artefact not found for model '{model_info.get('fold_num_or_id','N/A')}'. Path: {e.filename}. Skipping.\")\n",
        "        except Exception as e: logger.error(f\"LivePred {sym_upper}: Error with model instance '{model_info.get('fold_num_or_id','N/A')}': {e}\", exc_info=True)\n",
        "\n",
        "    if not all_model_probs: logger.warning(f\"LivePred {sym_upper}: No valid predictions from any model instance.\"); return None, 0.0\n",
        "    final_probs = np.mean(all_model_probs, axis=0)\n",
        "    pred_idx = np.argmax(final_probs); pred_conf = float(final_probs[pred_idx])\n",
        "    pred_label = CLASS_LABELS.get(pred_idx, \"UNKNOWN\")\n",
        "    return pred_label, pred_conf\n",
        "\n",
        "# --- - Decoupled Actor/Worker Tasks for a Fault-Tolerant System ---\n",
        "\n",
        "# Shared queues and state for communication between tasks\n",
        "candle_queue_global = asyncio.Queue()\n",
        "latest_ltp_by_symbol = {}\n",
        "active_positions_for_monitoring = set()\n",
        "\n",
        "async def data_ingestion_task(candle_queue: asyncio.Queue):\n",
        "    \"\"\"\n",
        "    WORKER 1: Connects to WebSocket, receives ticks, aggregates them into\n",
        "    completed 1-minute candles, and places them onto a shared queue.\n",
        "    This task is responsible for data integrity and availability.\n",
        "    \"\"\"\n",
        "    global logger, USE_REALTIME_WEBSOCKET_FEED, tick_queue_global, tick_queue_lock_global, tick_aggregators_by_symbol, latest_ltp_by_symbol, LIVE_AGGREGATION_INTERVAL_SECONDS, TARGET_INTERVAL, LOOKBACK_WINDOW, data_store_by_symbol, connect_market_websocket_upstox, websocket_connected_event\n",
        "\n",
        "    logger.info(\"WORKER-Data: Starting data ingestion task.\")\n",
        "    if USE_REALTIME_WEBSOCKET_FEED and not connect_market_websocket_upstox():\n",
        "        logger.error(\"WORKER-Data: Initial WebSocket connection failed. Will retry.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Ensure websocket is connected, retry if not\n",
        "            if USE_REALTIME_WEBSOCKET_FEED and not websocket_connected_event.is_set():\n",
        "                logger.warning(\"WORKER-Data: WebSocket is disconnected. Attempting to reconnect...\")\n",
        "                connect_market_websocket_upstox()\n",
        "                await asyncio.sleep(10) # Wait after reconnect attempt\n",
        "                continue\n",
        "\n",
        "            # Process ticks from the global queue populated by the websocket thread\n",
        "            temp_ticks = []\n",
        "            with tick_queue_lock_global:\n",
        "                while tick_queue_global:\n",
        "                    temp_ticks.append(tick_queue_global.popleft())\n",
        "\n",
        "            if not temp_ticks:\n",
        "                await asyncio.sleep(0.05) # No ticks, brief sleep\n",
        "                continue\n",
        "\n",
        "            new_1m_candles_formed_syms = set()\n",
        "            for tick in temp_ticks:\n",
        "                aggregator = tick_aggregators_by_symbol.get(tick['instrument_key'])\n",
        "                if not aggregator: continue\n",
        "\n",
        "                sym_name_tick = aggregator['symbol_name']\n",
        "                latest_ltp_by_symbol[sym_name_tick] = tick['price'] # Update latest price for monitor task\n",
        "\n",
        "                # Aggregate ticks into 10-second micro-candles\n",
        "                current_10s_slot_utc = tick['timestamp'].replace(second=(tick['timestamp'].second // LIVE_AGGREGATION_INTERVAL_SECONDS) * LIVE_AGGREGATION_INTERVAL_SECONDS, microsecond=0)\n",
        "                if aggregator.get('last_10s_candle_start_ts') is None:\n",
        "                    aggregator['last_10s_candle_start_ts'] = current_10s_slot_utc\n",
        "                    aggregator['current_10s_ohlcv'] = {'open':tick['price'], 'high':tick['price'], 'low':tick['price'], 'close':tick['price'], 'volume':tick['volume']}\n",
        "                elif current_10s_slot_utc > aggregator['last_10s_candle_start_ts']:\n",
        "                    if 'open' in aggregator.get('current_10s_ohlcv', {}):\n",
        "                        aggregator['ticks_10s'].append(pd.Series(aggregator['current_10s_ohlcv'], name=aggregator['last_10s_candle_start_ts']))\n",
        "                    aggregator['last_10s_candle_start_ts'] = current_10s_slot_utc\n",
        "                    aggregator['current_10s_ohlcv'] = {'open':tick['price'], 'high':tick['price'], 'low':tick['price'], 'close':tick['price'], 'volume':tick['volume']}\n",
        "                elif 'open' in aggregator.get('current_10s_ohlcv', {}):\n",
        "                    aggregator['current_10s_ohlcv']['high'] = max(aggregator['current_10s_ohlcv']['high'], tick['price'])\n",
        "                    aggregator['current_10s_ohlcv']['low'] = min(aggregator['current_10s_ohlcv']['low'], tick['price'])\n",
        "                    aggregator['current_10s_ohlcv']['close'] = tick['price']; aggregator['current_10s_ohlcv']['volume'] += tick['volume']\n",
        "\n",
        "                # Check if a new 1-minute candle is complete\n",
        "                current_1m_slot_utc = tick['timestamp'].replace(second=0, microsecond=0)\n",
        "                if aggregator.get('last_1m_candle_start_ts') is None: aggregator['last_1m_candle_start_ts'] = current_1m_slot_utc\n",
        "                if current_1m_slot_utc > aggregator.get('last_1m_candle_start_ts') and aggregator.get('ticks_10s'):\n",
        "                    # Aggregate 10s candles into a 1-minute candle\n",
        "                    last_minute_start = aggregator['last_1m_candle_start_ts']\n",
        "                    relevant_10s_candles_df = pd.DataFrame([s for s in aggregator['ticks_10s'] if s.name >= last_minute_start and s.name < current_1m_slot_utc])\n",
        "\n",
        "                    if not relevant_10s_candles_df.empty:\n",
        "                        ohlc_1m = {'open': relevant_10s_candles_df['open'].iloc[0], 'high': relevant_10s_candles_df['high'].max(), 'low': relevant_10s_candles_df['low'].min(), 'close': relevant_10s_candles_df['close'].iloc[-1], 'volume': relevant_10s_candles_df['volume'].sum()}\n",
        "                        completed_1m_series = pd.Series(ohlc_1m, name=last_minute_start)\n",
        "\n",
        "                        # Put the completed candle onto the shared queue for the signal task\n",
        "                        await candle_queue.put({'symbol': sym_name_tick, 'candle': completed_1m_series})\n",
        "                        logger.debug(f\"WORKER-Data: Generated 1m candle for {sym_name_tick} at {last_minute_start}\")\n",
        "\n",
        "                        # Keep the 10s tick buffer from growing indefinitely\n",
        "                        aggregator['ticks_10s'] = [s for s in aggregator['ticks_10s'] if s.name >= current_1m_slot_utc]\n",
        "\n",
        "                    aggregator['last_1m_candle_start_ts'] = current_1m_slot_utc\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WORKER-Data: Unhandled exception in data ingestion task: {e}\", exc_info=True)\n",
        "            await asyncio.sleep(5) # Cooldown before retrying\n",
        "\n",
        "async def _dynamically_reallocate_capital_live():\n",
        "    \"\"\"\n",
        "    Helper function to dynamically redistribute the total portfolio margin among all\n",
        "    symbols that are still eligible to trade for the day.\n",
        "    \"\"\"\n",
        "    global logger, portfolio_available_margin, capital_per_symbol_allowance, selected_symbols_for_session\n",
        "    global live_states_by_symbol, MAX_TRADES_PER_SYMBOL_PER_DAY, send_telegram_message\n",
        "\n",
        "    # First, identify which symbols are still eligible for new trades.\n",
        "    # An eligible symbol is one that is not halted for poor performance and has not yet hit its daily trade limit.\n",
        "    eligible_symbols = [\n",
        "        s.upper() for s in selected_symbols_for_session\n",
        "        if not live_states_by_symbol.get(s.upper(), {}).get('is_halted_for_performance', False) and\n",
        "           live_states_by_symbol.get(s.upper(), {}).get('trades_today_symbol', 0) < MAX_TRADES_PER_SYMBOL_PER_DAY\n",
        "    ]\n",
        "\n",
        "    if not eligible_symbols:\n",
        "        logger.info(\"Capital Reallocation: No symbols are eligible for further trading today.\")\n",
        "        return\n",
        "\n",
        "    # Divide the total current available margin equally among the remaining eligible symbols.\n",
        "    capital_per_eligible_symbol = portfolio_available_margin / len(eligible_symbols)\n",
        "\n",
        "    # Clear the old allocations before setting the new ones.\n",
        "    capital_per_symbol_allowance.clear()\n",
        "    for sym_upper in eligible_symbols:\n",
        "        capital_per_symbol_allowance[sym_upper] = capital_per_eligible_symbol\n",
        "\n",
        "    recalc_msg = (f\"Capital RE-ALLOCATED among {len(eligible_symbols)} eligible symbols.\\n\"\n",
        "                  f\"  New allocation: ‚Çπ{capital_per_eligible_symbol:,.2f} per symbol.\")\n",
        "    logger.info(recalc_msg)\n",
        "    await send_telegram_message(f\"‚öôÔ∏è {recalc_msg}\")\n",
        "\n",
        "async def signal_generation_task(candle_queue: asyncio.Queue):\n",
        "    \"\"\"\n",
        "    WORKER 2a: Listens for new candles, calculates features, gets model predictions,\n",
        "    and places new entry orders if trading conditions are met.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, data_store_by_symbol, LOOKBACK_WINDOW, CONFIDENCE_THRESHOLD_TRADE\n",
        "    global can_place_new_order_today_global, is_trading_halted_for_day_global\n",
        "    global MAX_TRADES_PER_SYMBOL_PER_DAY, calculated_max_portfolio_trades_today, portfolio_trades_today_count\n",
        "    global capital_per_symbol_allowance, UPSTOX_INSTRUMENT_KEYS, ENTRY_ORDER_TYPE_DEFAULT\n",
        "\n",
        "    logger.info(\"WORKER-SignalGen: Starting signal generation task.\")\n",
        "    while True:\n",
        "        try:\n",
        "            # This task now waits indefinitely for a candle, making it very efficient.\n",
        "            candle_data = await candle_queue.get()\n",
        "\n",
        "            sym_proc = candle_data['symbol']\n",
        "            new_candle = candle_data['candle']\n",
        "\n",
        "            if sym_proc not in live_states_by_symbol:\n",
        "                continue\n",
        "\n",
        "            sym_state = live_states_by_symbol[sym_proc]\n",
        "            data_store = data_store_by_symbol.get(sym_proc)\n",
        "            if not data_store or 'ohlcv_df' not in data_store:\n",
        "                continue\n",
        "\n",
        "            updated_history_df = calculate_features_for_new_candle(sym_proc, data_store['ohlcv_df'], new_candle)\n",
        "            if updated_history_df is None:\n",
        "                continue\n",
        "            data_store['ohlcv_df'] = updated_history_df\n",
        "\n",
        "            # Check if we can place a new trade for this symbol\n",
        "            can_trade_sym = (\n",
        "                not is_trading_halted_for_day_global and can_place_new_order_today_global and\n",
        "                sym_state.get('current_position') == 'None' and not sym_state.get('pending_entry_order_id') and\n",
        "                not sym_state.get('is_halted_for_performance', False) and\n",
        "                sym_state.get('trades_today_symbol', 0) < MAX_TRADES_PER_SYMBOL_PER_DAY and\n",
        "                portfolio_trades_today_count < calculated_max_portfolio_trades_today\n",
        "            )\n",
        "\n",
        "            if can_trade_sym:\n",
        "                sequence_to_predict = updated_history_df.tail(LOOKBACK_WINDOW)\n",
        "                if len(sequence_to_predict) == LOOKBACK_WINDOW:\n",
        "                    signal, confidence = await _get_live_prediction_for_symbol(sym_proc, sequence_to_predict)\n",
        "                    sym_state['last_prediction_label'], sym_state['last_prediction_confidence'] = signal, confidence\n",
        "\n",
        "                    if signal in [\"BUY\", \"SELL\"] and confidence >= CONFIDENCE_THRESHOLD_TRADE:\n",
        "                        capital_for_sym = capital_per_symbol_allowance.get(sym_proc, 0)\n",
        "                        last_known_ltp = new_candle['close']\n",
        "                        atr_at_entry = sequence_to_predict.iloc[-1].get('atr', last_known_ltp * 0.015)\n",
        "\n",
        "                        if capital_for_sym > 0 and last_known_ltp > 0:\n",
        "                            order_qty = calculate_dynamic_order_quantity_live(last_known_ltp, capital_for_sym)\n",
        "                            if order_qty > 0:\n",
        "\n",
        "                                # --- Moved trade counting to AFTER successful order placement ---\n",
        "                                sym_state.update({'entry_signal_type': \"Long\" if signal == \"BUY\" else \"Short\", 'atr_at_entry': atr_at_entry})\n",
        "                                entry_tag = f\"ENTRY_{signal}_{sym_proc[:5]}_{uuid.uuid4().hex[:4]}\"\n",
        "\n",
        "                                entry_order_id = await place_upstox_order_live(\n",
        "                                    instrument_key=UPSTOX_INSTRUMENT_KEYS[sym_proc],\n",
        "                                    quantity=order_qty,\n",
        "                                    transaction_type=signal,\n",
        "                                    order_type=ENTRY_ORDER_TYPE_DEFAULT,\n",
        "                                    tag=entry_tag,\n",
        "                                    base_price_for_limit=last_known_ltp\n",
        "                                )\n",
        "\n",
        "                                # Only if we get a valid order_id back, do we count the trade and update the state.\n",
        "                                if entry_order_id:\n",
        "                                    portfolio_trades_today_count += 1\n",
        "                                    sym_state['trades_today_symbol'] += 1\n",
        "                                    sym_state['pending_entry_order_id'] = entry_order_id\n",
        "                                    logger.info(f\"WORKER-SignalGen: Entry order {entry_order_id} placed for {sym_proc}. Awaiting confirmation.\")\n",
        "                                    save_state_to_json()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WORKER-SignalGen: Unhandled exception: {e}\", exc_info=True)\n",
        "            await asyncio.sleep(5)\n",
        "async def order_polling_task():\n",
        "    \"\"\"\n",
        "    WORKER 2b: Continuously polls for the status of all pending entry and exit orders\n",
        "    and updates the bot's state upon completion, cancellation, or rejection.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, POLL_INTERVAL_SECONDS, UPSTOX_INSTRUMENT_KEYS, BACKTEST_TRANSACTION_COST_PCT\n",
        "    global portfolio_available_margin, active_positions_for_monitoring, portfolio_trades_today_count, NSE_TZ\n",
        "\n",
        "    logger.info(\"WORKER-Polling: Starting order status polling task.\")\n",
        "    while True:\n",
        "        try:\n",
        "            symbols_with_pending_orders = {\n",
        "                s: state.get('pending_entry_order_id') or state.get('pending_exit_order_id')\n",
        "                for s, state in live_states_by_symbol.items()\n",
        "                if state.get('pending_entry_order_id') or state.get('pending_exit_order_id')\n",
        "            }\n",
        "\n",
        "            for sym_poll, order_id_poll in list(symbols_with_pending_orders.items()):\n",
        "                if not order_id_poll: continue\n",
        "\n",
        "                state_poll = live_states_by_symbol.get(sym_poll)\n",
        "                if not state_poll: continue\n",
        "\n",
        "                now = time.monotonic()\n",
        "                last_checked = state_poll.get('last_order_poll_time', 0)\n",
        "                if (now - last_checked) < POLL_INTERVAL_SECONDS:\n",
        "                    continue\n",
        "\n",
        "                state_poll['last_order_poll_time'] = now\n",
        "\n",
        "                # --- Handle potentially stuck exit orders ---\n",
        "                if state_poll.get('pending_exit_order_id') and state_poll.get('pending_exit_since'):\n",
        "                    time_since_exit_pending = (datetime.now(NSE_TZ) - state_poll['pending_exit_since']).total_seconds()\n",
        "                    EXIT_TIMEOUT_SECONDS = 300 # 5 minutes\n",
        "                    if time_since_exit_pending > EXIT_TIMEOUT_SECONDS:\n",
        "                        logger.critical(f\"WORKER-Polling: Exit order {order_id_poll} for {sym_poll} has been pending for over {EXIT_TIMEOUT_SECONDS}s. Forcing exit.\")\n",
        "                        await send_telegram_message(f\"üö® CRITICAL: Stuck exit order for {sym_poll}. Forcing market exit now!\")\n",
        "                        await cancel_upstox_order_live(order_id_poll)\n",
        "                        instr_key_force = UPSTOX_INSTRUMENT_KEYS.get(sym_poll.upper())\n",
        "                        force_exit_action = \"BUY\" if state_poll['current_position'] == \"Short\" else \"SELL\"\n",
        "                        await place_upstox_order_live(instr_key_force, state_poll['current_order_quantity'], force_exit_action, \"MARKET\", tag=\"FORCE_EXIT\")\n",
        "                        state_poll.pop('pending_exit_since', None)\n",
        "                        continue\n",
        "\n",
        "                order_details = await get_upstox_order_details_live(order_id_poll)\n",
        "                if not order_details: continue\n",
        "\n",
        "                latest_leg = order_details[-1]\n",
        "                if str(getattr(latest_leg, 'status', '')).lower() == 'complete':\n",
        "                    # --- Handle filled ENTRY order ---\n",
        "                    if order_id_poll == state_poll.get('pending_entry_order_id'):\n",
        "                        actual_fill_price = float(getattr(latest_leg, 'average_price', 0.0))\n",
        "                        actual_fill_qty = int(getattr(latest_leg, 'filled_quantity', 0))\n",
        "                        if actual_fill_price > 0 and actual_fill_qty > 0:\n",
        "                            pos_type_live = state_poll['entry_signal_type']\n",
        "                            sl_m, tp_m = state_poll['current_sl_atr_multiplier'], state_poll['current_tp_atr_multiplier']\n",
        "                            state_poll.update({\n",
        "                                'current_position': pos_type_live, 'entry_price': actual_fill_price, 'current_order_quantity': actual_fill_qty,\n",
        "                                'current_sl_price': round(actual_fill_price - (state_poll['atr_at_entry'] * sl_m), 2) if pos_type_live == 'Long' else round(actual_fill_price + (state_poll['atr_at_entry'] * sl_m), 2),\n",
        "                                'current_tp_price': round(actual_fill_price + (state_poll['atr_at_entry'] * tp_m), 2) if pos_type_live == 'Long' else round(actual_fill_price - (state_poll['atr_at_entry'] * tp_m), 2),\n",
        "                                'entry_time': datetime.now(NSE_TZ), 'active_entry_order_id': order_id_poll, 'pending_entry_order_id': None,\n",
        "                            })\n",
        "                            active_positions_for_monitoring.add(sym_poll)\n",
        "                            save_state_to_json()\n",
        "                            logger.info(f\"WORKER-Polling: Entry confirmed for {sym_poll}. Position is now active.\")\n",
        "                            await send_telegram_message(f\"‚úÖ ENTRY CONFIRMED: {pos_type_live} {actual_fill_qty} {sym_poll} @ ‚Çπ{actual_fill_price:.2f}\")\n",
        "\n",
        "                    # --- Handle filled EXIT order ---\n",
        "                    elif order_id_poll == state_poll.get('pending_exit_order_id'):\n",
        "                        actual_fill_price = float(getattr(latest_leg, 'average_price', state_poll.get('entry_price', 0)))\n",
        "                        actual_fill_qty = int(getattr(latest_leg, 'filled_quantity', state_poll.get('current_order_quantity', 0)))\n",
        "                        pnl_net = ((actual_fill_price - state_poll['entry_price']) if state_poll['current_position'] == 'Long' else (state_poll['entry_price'] - actual_fill_price)) * actual_fill_qty\n",
        "                        pnl_net -= ((state_poll['entry_price'] * actual_fill_qty + actual_fill_price * actual_fill_qty) * BACKTEST_TRANSACTION_COST_PCT)\n",
        "\n",
        "                        state_poll['daily_pnl_symbol'] += pnl_net\n",
        "                        portfolio_available_margin += pnl_net # Update global margin with P&L\n",
        "\n",
        "                        exit_msg = f\"‚úÖ EXIT CONFIRMED: {sym_poll} closed. Net P&L: ‚Çπ{pnl_net:,.2f}\"\n",
        "                        logger.info(f\"WORKER-Polling: {exit_msg}\")\n",
        "                        await send_telegram_message(exit_msg)\n",
        "\n",
        "                        # Reset the state for this symbol\n",
        "                        state_poll.update({'current_position':'None', 'active_entry_order_id':None, 'current_order_quantity':0, 'pending_exit_order_id':None, 'exit_reason':None, 'pending_exit_since':None})\n",
        "\n",
        "                        await _dynamically_reallocate_capital_live() # Reallocate capital after closing a position\n",
        "                        save_state_to_json()\n",
        "\n",
        "                # --- Handle REJECTED or CANCELLED orders ---\n",
        "                elif str(getattr(latest_leg, 'status', '')).lower() in ['rejected', 'cancelled']:\n",
        "                    rejection_reason = getattr(latest_leg, 'message', 'No reason provided')\n",
        "                    logger.warning(f\"WORKER-Polling: Order {order_id_poll} for {sym_poll} was {latest_leg.status}. Reason: {rejection_reason}. Resetting state.\")\n",
        "\n",
        "                    if order_id_poll == state_poll.get('pending_entry_order_id'):\n",
        "                        state_poll['pending_entry_order_id'] = None\n",
        "                        # Give back the trade count since the order failed.\n",
        "                        state_poll['trades_today_symbol'] = max(0, state_poll.get('trades_today_symbol', 1) - 1)\n",
        "                        portfolio_trades_today_count = max(0, portfolio_trades_today_count - 1)\n",
        "                        logger.info(f\"Reverted trade count for {sym_poll} due to failed order. New count: {state_poll['trades_today_symbol']}\")\n",
        "                        await send_telegram_message(f\"‚ö†Ô∏è Entry order for {sym_poll} FAILED ({latest_leg.status}). Reason: {rejection_reason}\")\n",
        "\n",
        "                    elif order_id_poll == state_poll.get('pending_exit_order_id'):\n",
        "                        state_poll['pending_exit_order_id'] = None\n",
        "                        # Re-activate monitoring since the position is still open.\n",
        "                        active_positions_for_monitoring.add(sym_poll)\n",
        "                        await send_telegram_message(f\"üö® CRITICAL WARNING: Exit order for {sym_poll} FAILED ({latest_leg.status}). Position is still active and being monitored. MANUAL INTERVENTION MAY BE NEEDED.\")\n",
        "\n",
        "                    save_state_to_json()\n",
        "\n",
        "            await asyncio.sleep(POLL_INTERVAL_SECONDS) # Main polling delay\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WORKER-Polling: Unhandled exception: {e}\", exc_info=True)\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "async def position_monitor_task():\n",
        "    \"\"\"\n",
        "    WORKER 3: Periodically checks the live price against the SL/TP levels\n",
        "    for all active positions and triggers exits.\n",
        "    This is a high-frequency, focused risk management task.\n",
        "    \"\"\"\n",
        "    global logger, active_positions_for_monitoring, latest_ltp_by_symbol, LIVE_MONITORING_INTERVAL_SECONDS\n",
        "\n",
        "    logger.info(\"WORKER-Monitor: Starting position monitoring task.\")\n",
        "    while True:\n",
        "        try:\n",
        "            # Iterate on a copy to avoid issues with modification during iteration\n",
        "            for sym_mon in list(active_positions_for_monitoring):\n",
        "                ltp_mon = latest_ltp_by_symbol.get(sym_mon)\n",
        "                if ltp_mon:\n",
        "                    exit_initiated = await _handle_live_sl_tp_exit_for_symbol(sym_mon, ltp_mon, active_positions_for_monitoring)\n",
        "                    if exit_initiated:\n",
        "                        logger.info(f\"WORKER-Monitor: Exit initiated for {sym_mon}. It will be handled by the order task.\")\n",
        "\n",
        "            await asyncio.sleep(LIVE_MONITORING_INTERVAL_SECONDS)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"WORKER-Monitor: Unhandled exception in position monitor task: {e}\", exc_info=True)\n",
        "            await asyncio.sleep(5)\n",
        "\n",
        "async def _reconcile_pending_states_on_startup():\n",
        "    \"\"\"\n",
        "    Checks for any states that were interrupted during an API call on startup.\n",
        "    This function queries the broker to determine the true state of orders and positions.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, get_upstox_positions_live, send_telegram_message\n",
        "\n",
        "    logger.info(\"--- Starting Startup State Reconciliation ---\")\n",
        "    symbols_to_reconcile = [s for s, state in live_states_by_symbol.items() if state.get('pending_exit_api_call')]\n",
        "\n",
        "    if not symbols_to_reconcile:\n",
        "        logger.info(\"No pending states to reconcile. Startup is clean.\")\n",
        "        return\n",
        "\n",
        "    await send_telegram_message(f\"‚ö†Ô∏è Bot restarting. Reconciling potentially stuck trades for: {', '.join(symbols_to_reconcile)}\")\n",
        "\n",
        "    # Get all current positions from the broker once\n",
        "    current_positions_from_broker = await get_upstox_positions_live()\n",
        "    broker_positions = {pos.tradingsymbol.upper(): pos for pos in current_positions_from_broker} if current_positions_from_broker else {}\n",
        "\n",
        "    for sym_upper in symbols_to_reconcile:\n",
        "        state = live_states_by_symbol[sym_upper]\n",
        "        logger.warning(f\"Reconciling state for {sym_upper}. Found 'pending_exit_api_call' flag: {state['pending_exit_api_call']}\")\n",
        "\n",
        "        # Check if the position still exists at the broker\n",
        "        if sym_upper in broker_positions:\n",
        "            # The position is still open. The exit order likely failed or didn't get placed.\n",
        "            logger.warning(f\"Reconciliation: {sym_upper} position is still OPEN at broker. Resetting state to active.\")\n",
        "            state.pop('pending_exit_api_call', None)\n",
        "            state.pop('pending_exit_reason', None)\n",
        "            # The position monitor will automatically try to exit it again if conditions are met.\n",
        "        else:\n",
        "            # The position is closed at the broker. The exit order must have gone through.\n",
        "            logger.warning(f\"Reconciliation: {sym_upper} position is CLOSED at broker. The bot will reset its internal state.\")\n",
        "            state.update({\n",
        "                'current_position':'None',\n",
        "                'active_entry_order_id':None,\n",
        "                'current_order_quantity':0,\n",
        "                'pending_exit_order_id':None,\n",
        "                'pending_exit_api_call': None, # Clear the flag\n",
        "                'exit_reason': state.get('pending_exit_reason', 'RECONCILED_ON_RESTART')\n",
        "            })\n",
        "            state['daily_pnl_symbol'] += 0 # Or fetch from broker if possible\n",
        "\n",
        "        save_state_to_json()\n",
        "\n",
        "    logger.info(\"--- Startup State Reconciliation Complete ---\")\n",
        "\n",
        "async def run_adv_live_trading_supervisor():\n",
        "    \"\"\"\n",
        "    SUPERVISOR: Main entry point for the live trading system.\n",
        "    Initializes all components and supervises the independent worker tasks,\n",
        "    providing resilience and fault tolerance with exponential backoff.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, trained_models_by_symbol, data_store_by_symbol, selected_symbols_for_session\n",
        "    global last_daily_reset_date_global, NSE_TZ, candle_queue_global, active_positions_for_monitoring, live_trading_mode, CAPITAL_ALLOCATION_MODE\n",
        "\n",
        "    # --- Initialization phase ---\n",
        "    start_msg = f\"--- SUPERVISOR: Initializing Advanced Live Trading System ---\\nSymbols: {', '.join(selected_symbols_for_session) if selected_symbols_for_session else 'None'}\\nMode: {live_trading_mode}, CapAlloc: {CAPITAL_ALLOCATION_MODE}\"\n",
        "    logger.info(start_msg)\n",
        "    if not await initialize_telegram_bot_async():\n",
        "        logger.warning(\"Telegram bot init failed, proceeding without it.\")\n",
        "    await send_telegram_message(f\"üöÄ {start_msg}\")\n",
        "    load_state_from_json()\n",
        "    await _reconcile_pending_states_on_startup()\n",
        "    active_positions_for_monitoring.update({s for s, state in live_states_by_symbol.items() if state.get('current_position') != 'None' and not state.get('pending_exit_order_id')})\n",
        "    logger.info(f\"SUPERVISOR: Restored and reconciled {len(active_positions_for_monitoring)} active positions for monitoring.\")\n",
        "    if not globals().get('upstox_api_client_global') and not await initialize_upstox_client():\n",
        "        critical_msg = \"CRITICAL: Upstox API client init failed. Live system cannot start.\"\n",
        "        logger.critical(critical_msg)\n",
        "        await send_telegram_message(f\"üö® {critical_msg}\")\n",
        "        return\n",
        "    if not selected_symbols_for_session:\n",
        "        critical_msg_no_sym = \"CRITICAL: No symbols selected. Live system cannot start.\"\n",
        "        logger.critical(critical_msg_no_sym)\n",
        "        await send_telegram_message(f\"üö® {critical_msg_no_sym}\")\n",
        "        return\n",
        "    for sym_init in selected_symbols_for_session:\n",
        "        sym_upper = sym_init.upper()\n",
        "        if sym_upper not in trained_models_by_symbol or not trained_models_by_symbol.get(sym_upper):\n",
        "            logger.error(f\"Critical: No trained models for {sym_upper}. Skipped.\")\n",
        "            continue\n",
        "        if sym_upper not in data_store_by_symbol or 'feature_columns' not in data_store_by_symbol.get(sym_upper, {}):\n",
        "            logger.error(f\"Critical: Feature config missing for {sym_upper}. Skipped.\")\n",
        "            continue\n",
        "        if sym_upper not in live_states_by_symbol:\n",
        "            live_states_by_symbol[sym_upper] = {\n",
        "                'symbol_name': sym_upper, 'current_position': 'None', 'pending_entry_order_id': None, 'entry_signal_type': None,\n",
        "                'entry_price': 0.0, 'current_sl_price': 0.0, 'current_tp_price': 0.0,\n",
        "                'current_sl_atr_multiplier': SL_ATR_MULTIPLIER_DEFAULT, 'current_tp_atr_multiplier': TP_ATR_MULTIPLIER_DEFAULT,\n",
        "                'entry_time': None, 'active_entry_order_id': None, 'current_order_quantity': 0, 'atr_at_entry': 0.0,\n",
        "                'last_prediction_label': None, 'last_prediction_confidence': 0.0, 'daily_pnl_symbol': 0.0, 'trades_today_symbol': 0,\n",
        "                'consecutive_loss_days': 0, 'is_halted_for_performance': False,\n",
        "            }\n",
        "        await adapt_strategy_parameters_for_symbol(sym_upper)\n",
        "        instr_key_init = UPSTOX_INSTRUMENT_KEYS.get(sym_upper)\n",
        "        if instr_key_init and \"INVALID_KEY\" not in instr_key_init and instr_key_init not in tick_aggregators_by_symbol:\n",
        "            tick_aggregators_by_symbol[instr_key_init] = {'symbol_name': sym_upper, 'ticks_10s': [], 'last_10s_candle_start_ts': None, 'current_10s_ohlcv': {}, 'last_1m_candle_start_ts': None}\n",
        "\n",
        "\n",
        "    # --- Main Resilient Loop ---\n",
        "    consecutive_failures = 0\n",
        "    max_consecutive_failures = 5 # Shut down after 5 consecutive failures\n",
        "    workers = []\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            if not selected_symbols_for_session:\n",
        "                logger.warning(\"SUPERVISOR: No symbols are selected for the current session. Waiting for user selection from CLI...\")\n",
        "                await asyncio.sleep(15)\n",
        "                continue\n",
        "\n",
        "            now_nse = datetime.now(NSE_TZ)\n",
        "            if globals().get('last_daily_reset_date_global') != now_nse.date():\n",
        "                await _reset_daily_limits_and_state_live()\n",
        "            if await _check_and_handle_time_based_rules_live(now_nse):\n",
        "                logger.info(\"SUPERVISOR: Time-based rules indicate trading halt. Waiting...\")\n",
        "                await asyncio.sleep(30)\n",
        "                continue\n",
        "\n",
        "            logger.info(\"SUPERVISOR: Starting all worker tasks...\")\n",
        "            ingestion_task = asyncio.create_task(data_ingestion_task(candle_queue_global))\n",
        "\n",
        "            signal_task = asyncio.create_task(signal_generation_task(candle_queue_global))\n",
        "            polling_task = asyncio.create_task(order_polling_task())\n",
        "\n",
        "            monitor_task = asyncio.create_task(position_monitor_task())\n",
        "\n",
        "            # Update the list of workers for the supervisor to manage\n",
        "            workers = [ingestion_task, signal_task, polling_task, monitor_task]\n",
        "\n",
        "            await asyncio.gather(*workers)\n",
        "\n",
        "            if consecutive_failures > 0:\n",
        "                logger.info(\"SUPERVISOR: System recovered successfully. Resetting failure count.\")\n",
        "                await send_telegram_message(\"‚úÖ SYSTEM RECOVERED: Worker tasks are running normally.\")\n",
        "            consecutive_failures = 0\n",
        "\n",
        "        except asyncio.CancelledError:\n",
        "            logger.info(\"SUPERVISOR: Main supervisor task was cancelled. Shutting down.\")\n",
        "            break\n",
        "\n",
        "        except Exception as e:\n",
        "            consecutive_failures += 1\n",
        "            logger.critical(f\"SUPERVISOR: A critical worker task has failed! (Failure #{consecutive_failures})\", exc_info=True)\n",
        "\n",
        "            # Cancel any other running tasks to ensure a clean slate\n",
        "            for worker in workers:\n",
        "                if not worker.done():\n",
        "                    worker.cancel()\n",
        "\n",
        "            # Circuit Breaker: If it fails too many times, shut down completely.\n",
        "            if consecutive_failures >= max_consecutive_failures:\n",
        "                shutdown_msg = f\"üö®üö® SHUTDOWN: Bot failed {consecutive_failures} consecutive times. System is halting to prevent further issues.\"\n",
        "                logger.critical(shutdown_msg)\n",
        "                await send_telegram_message(shutdown_msg)\n",
        "                break\n",
        "\n",
        "            # Exponential Backoff\n",
        "            cooldown_period = 10 * (2 ** (consecutive_failures - 1))\n",
        "\n",
        "            restart_msg = (f\"üö® BOT CRITICAL ERROR: {str(e)[:100]}. \"\n",
        "                           f\"Restarting in {cooldown_period} seconds. (Attempt {consecutive_failures}/{max_consecutive_failures})\")\n",
        "            await send_telegram_message(restart_msg)\n",
        "\n",
        "            logger.info(f\"SUPERVISOR: Restarting worker tasks in {cooldown_period} seconds...\")\n",
        "            await asyncio.sleep(cooldown_period)\n",
        "\n",
        "\n",
        "    # --- Cleanup on exit ---\n",
        "    logger.info(\"--- SUPERVISOR: Live Trading System Terminating. Cleanup... ---\")\n",
        "    if USE_REALTIME_WEBSOCKET_FEED and websocket_thread_global and websocket_thread_global.is_alive():\n",
        "        stop_upstox_websocket()\n",
        "    open_pos_exit = [f\"{s}: {st.get('current_position')} {st.get('current_order_quantity',0)} @ ‚Çπ{st.get('entry_price',0):.2f}\" for s, st in live_states_by_symbol.items() if st.get('current_position') != 'None']\n",
        "    if open_pos_exit:\n",
        "        await send_telegram_message(f\"üîå System Terminated. WARNING: Open Positions Require Manual Check:\\n\" + \"\\n\".join(open_pos_exit))\n",
        "    else:\n",
        "        await send_telegram_message(f\"üîå System Terminated. No open positions managed by bot.\")\n",
        "    await send_telegram_message(f\"  Final Portfolio PNL for session/day: ‚Çπ{portfolio_daily_pnl_achieved:.2f}\")\n",
        "\n",
        "\n",
        "logger.info(\"Cell 8: Live Trading Loop functions have been fixed and refactored into a resilient supervisor/worker architecture.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5GzN-WSuORm",
        "outputId": "7b09d93e-a6dc-4c7e-d7fa-8a5b2885d267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing Cell 9: Main Execution Block (CLI Menu)\n",
            "2025-06-17 17:49:06 - TradingBotLogger - WARNING - [<ipython-input-11-81474271>.<cell line: 0>:62] - Function 'run_adv_live_trading_loop' not found globally. Using placeholder for Cell 9 CLI.\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.<cell line: 0>:501] - An event loop is already running. Applying nest_asyncio.\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.display_main_header_cli:67] - \n",
            "====================================================\n",
            "      ü§ñ ADVANCED MULTI-SYMBOL TRADING BOT ü§ñ      \n",
            "====================================================\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:240] - Initializing Trading Bot CLI Menu...\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.initialize_telegram_bot_async:628] - Telegram bot init success: Trading_update_udhay_bot (ID: 8095737189)\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.initialize_upstox_client:445] - Loaded access token from file. Saved expiry: 2025-06-18 03:07:00 IST.\n",
            "2025-06-17 17:49:06 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.initialize_upstox_client:468] - Attempting to validate existing access token by fetching profile...\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.initialize_upstox_client:480] - Existing access token validated successfully via API call.\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-10-3683736236>.initialize_upstox_client:615] - Upstox API client is configured and ready.\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:244] - --- Initializing Databases for All Configured & Validated Symbols ---\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:248] - Database for IRFC checked/initialized successfully.\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:248] - Database for IRB checked/initialized successfully.\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:249] - --- Database Initialization Complete ---\n",
            "2025-06-17 17:49:07 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.display_main_header_cli:67] - \n",
            "====================================================\n",
            "      ü§ñ ADVANCED MULTI-SYMBOL TRADING BOT ü§ñ      \n",
            "====================================================\n",
            "\n",
            "Available Symbols (with valid instrument keys):\n",
            "  1. IRFC\n",
            "  2. IRB\n",
            "--------------------------------------------------------\n",
            "Enter symbol numbers (e.g., 1,3), 'all', or '0' to Exit.\n",
            "--------------------------------------------------------\n",
            "Select symbol(s) for this session: 0\n",
            "2025-06-17 17:49:15 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:490] - Trading bot CLI shutting down...\n",
            "2025-06-17 17:49:15 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.main_cli_menu:493] - Program exited.\n",
            "2025-06-17 17:49:15 - TradingBotLogger - INFO - [<ipython-input-11-81474271>.<cell line: 0>:517] - Main execution scope finished. Bot shutdown complete.\n"
          ]
        }
      ],
      "source": [
        "# --- Cell 9: Main Execution Block (CLI Menu) ---\n",
        "\n",
        "print(\"\\nInitializing Cell 9: Main Execution Block (CLI Menu)\")\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import asyncio\n",
        "import os\n",
        "import sys\n",
        "from typing import Union, List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import upstox_client\n",
        "import upstox_client.api.portfolio_api\n",
        "import upstox_client.api.user_api\n",
        "\n",
        "try:\n",
        "    import nest_asyncio\n",
        "except ImportError:\n",
        "    nest_asyncio = None\n",
        "\n",
        "# --- Ensure necessary variables and functions from previous cells are available ---\n",
        "if 'logger' not in globals():\n",
        "    import logging as pylogging_c9\n",
        "    logger = pylogging_c9.getLogger(\"TradingBotLogger_C9_Fallback\")\n",
        "    if not logger.handlers:\n",
        "        _ch_c9 = pylogging_c9.StreamHandler(sys.stdout)\n",
        "        _ch_c9.setFormatter(pylogging_c9.Formatter('%(asctime)s - %(levelname)s - C9_FALLBACK - %(message)s'))\n",
        "        logger.addHandler(_ch_c9); logger.setLevel(pylogging_c9.INFO)\n",
        "    logger.warning(\"Cell 1 'logger' not found. Using a basic fallback logger for Cell 9.\")\n",
        "\n",
        "config_defaults_c9 = {\n",
        "    'selected_symbols_for_session': [], 'SYMBOLS_LIST': [\"FALLBACK_SYM1\"], 'VALIDATED_SYMBOLS_LIST': [],\n",
        "    'AUTO_ORDER_EXECUTION_ENABLED': False, 'TARGET_INTERVAL': \"1minute\", 'NSE_TZ': pytz.utc,\n",
        "    'portfolio_daily_pnl_achieved': 0.0, 'portfolio_trades_today_count': 0,\n",
        "    'MAX_TRADES_PER_SYMBOL_PER_DAY': 2, 'calculated_max_portfolio_trades_today': 0,\n",
        "    'is_trading_halted_for_day_global': False, 'calculated_max_daily_loss_global': 400.0,\n",
        "    'live_trading_mode': \"NOT_SET\", 'portfolio_available_margin': 0.0,\n",
        "    'live_states_by_symbol': {}, 'SL_ATR_MULTIPLIER_DEFAULT': 0.75, 'TP_ATR_MULTIPLIER_DEFAULT': 1.5,\n",
        "    'HISTORICAL_DATA_LOOKBACK_DAYS': 880, 'KERAS_TUNER_ENABLED': False,\n",
        "    'data_store_by_symbol': {}, 'trained_models_by_symbol': {},\n",
        "    'upstox_api_client_global': None, 'telegram_initialized_successfully': False,\n",
        "    'USE_REALTIME_WEBSOCKET_FEED': True, 'websocket_thread_global': None,\n",
        "    'UpstoxApiException': type('UpstoxApiExceptionPlaceholder_C9', (Exception,), {}),\n",
        "    'CAPITAL_ALLOCATION_MODE': 'EQUAL', 'upstox_client': None,\n",
        "    'SYMBOLS_REQUIRING_RETRAINING': [], 'is_long_running_task_active' : False,\n",
        "    'cancel_upstox_order_live': lambda order_id: asyncio.sleep(0, result=False),\n",
        "}\n",
        "for param_c9, default_val_c9 in config_defaults_c9.items():\n",
        "    if param_c9 not in globals(): globals()[param_c9] = default_val_c9\n",
        "func_placeholders_c9 = {\n",
        "    'load_and_preprocess_data_for_symbol': lambda s, t: (None, None),'process_symbol_trade_logs_for_learning': lambda s: [],\n",
        "    'run_standalone_tuning_pipeline': lambda s_list: asyncio.sleep(0),'run_adv_training_pipeline': lambda s_list: asyncio.sleep(0),\n",
        "    'run_adv_backtesting_pipeline': lambda s_list, cap=0: None, 'initialize_upstox_client': lambda: asyncio.sleep(0, result=False),\n",
        "    'initialize_telegram_bot_async': lambda: asyncio.sleep(0, result=False), 'run_adv_live_trading_loop': lambda: asyncio.sleep(0),\n",
        "    'get_upstox_positions_live': lambda: asyncio.sleep(0, result=[]), 'get_upstox_funds_and_margin_live': lambda segment=\"SEC\": asyncio.sleep(0, result=None),\n",
        "    'stop_upstox_websocket': lambda: None, 'adapt_strategy_parameters_for_symbol': lambda s_name: asyncio.sleep(0),\n",
        "    'send_telegram_message': lambda msg, cid=None: asyncio.sleep(0, result=True), 'save_state_to_json': lambda: None,\n",
        "    'initialize_database_for_symbol': lambda s: None,\n",
        "}\n",
        "for func_name_c9, placeholder_fn_c9 in func_placeholders_c9.items():\n",
        "    if func_name_c9 not in globals():\n",
        "        globals()[func_name_c9] = placeholder_fn_c9; logger.warning(f\"Function '{func_name_c9}' not found globally. Using placeholder for Cell 9 CLI.\")\n",
        "\n",
        "\n",
        "# --- CLI HELPER FUNCTIONS ---\n",
        "def clear_console(): os.system('cls' if os.name == 'nt' else 'clear')\n",
        "def display_main_header_cli(): logger.info(\"\\n====================================================\\n      ü§ñ ADVANCED MULTI-SYMBOL TRADING BOT ü§ñ      \\n====================================================\")\n",
        "def display_symbol_selection_menu_cli():\n",
        "    global VALIDATED_SYMBOLS_LIST\n",
        "    display_main_header_cli()\n",
        "    print(\"\\nAvailable Symbols (with valid instrument keys):\")\n",
        "    if not VALIDATED_SYMBOLS_LIST:\n",
        "        print(\"  No valid symbols configured. Please check config.yaml and Cell 2.\")\n",
        "        return False\n",
        "    for i, symbol_name_menu in enumerate(VALIDATED_SYMBOLS_LIST): print(f\"  {i + 1}. {symbol_name_menu}\")\n",
        "    print(\"--------------------------------------------------------\\nEnter symbol numbers (e.g., 1,3), 'all', or '0' to Exit.\\n--------------------------------------------------------\")\n",
        "    return True\n",
        "def process_symbol_selection_input_cli() -> Optional[List[str]]:\n",
        "    global VALIDATED_SYMBOLS_LIST, logger\n",
        "    user_input_str = input(\"Select symbol(s) for this session: \").strip().lower()\n",
        "    if user_input_str == '0': return None\n",
        "    selected_symbols_list = []\n",
        "    if user_input_str == 'all':\n",
        "        selected_symbols_list = list(VALIDATED_SYMBOLS_LIST) if VALIDATED_SYMBOLS_LIST else []\n",
        "    else:\n",
        "        try:\n",
        "            indices = [int(x.strip()) - 1 for x in user_input_str.split(',') if x.strip().isdigit()]\n",
        "            for idx in indices:\n",
        "                if 0 <= idx < len(VALIDATED_SYMBOLS_LIST) and VALIDATED_SYMBOLS_LIST[idx] not in selected_symbols_list:\n",
        "                    selected_symbols_list.append(VALIDATED_SYMBOLS_LIST[idx])\n",
        "        except (ValueError, IndexError): pass\n",
        "    if not selected_symbols_list: print(\"No valid symbols were selected.\")\n",
        "    else: logger.info(f\"Symbols selected for session: {', '.join(selected_symbols_list)}\")\n",
        "    return selected_symbols_list\n",
        "def reset_global_states_for_new_session_cli():\n",
        "    globals().update({k: v for k, v in config_defaults_c9.items() if k in ['data_store_by_symbol', 'trained_models_by_symbol', 'best_hyperparameters_by_symbol', 'live_states_by_symbol', 'tick_aggregators_by_symbol', 'capital_per_symbol_allowance', 'portfolio_daily_pnl_achieved', 'portfolio_trades_today_count', 'is_trading_halted_for_day_global', 'can_place_new_order_today_global', 'global_trade_active_flag', 'calculated_max_portfolio_trades_today']})\n",
        "    logger.info(\"Global states reset for new symbol selection.\")\n",
        "async def display_operation_menu_and_status_cli():\n",
        "    \"\"\"Displays the main CLI menu and current bot status.\"\"\"\n",
        "    global selected_symbols_for_session, AUTO_ORDER_EXECUTION_ENABLED, TARGET_INTERVAL, NSE_TZ\n",
        "    global portfolio_daily_pnl_achieved, portfolio_trades_today_count, MAX_TRADES_PER_SYMBOL_PER_DAY, calculated_max_portfolio_trades_today\n",
        "    global is_trading_halted_for_day_global, calculated_max_daily_loss_global, live_trading_mode, portfolio_available_margin\n",
        "    global live_states_by_symbol, SL_ATR_MULTIPLIER_DEFAULT, TP_ATR_MULTIPLIER_DEFAULT, CAPITAL_ALLOCATION_MODE\n",
        "\n",
        "    display_main_header_cli()\n",
        "    current_time_str = datetime.now(NSE_TZ).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
        "    print(f\"\\n--- ü§ñ Bot Operations Menu & Status ({current_time_str}) ---\")\n",
        "    active_sym_str = ', '.join(selected_symbols_for_session) if selected_symbols_for_session else 'None - Use [9] to Select'\n",
        "    print(f\"Active Symbols: {active_sym_str}\")\n",
        "    print(f\"Target Interval: {TARGET_INTERVAL}, Auto Orders: {'ENABLED' if AUTO_ORDER_EXECUTION_ENABLED else 'DISABLED (Alerts Only)'}\")\n",
        "    print(f\"Capital Mode: {CAPITAL_ALLOCATION_MODE} (Use [C] to toggle)\")\n",
        "    print(\"\\n--- Portfolio Status ---\")\n",
        "    print(f\"  Trading Mode: {live_trading_mode if live_trading_mode != 'NOT_SET' else 'Not Set (Run Live Trading)'}\")\n",
        "    print(f\"  Margin (Upstox): ‚Çπ{portfolio_available_margin:,.2f}\" if portfolio_available_margin > 0 else \"  Margin: Not fetched/Zero\")\n",
        "    print(f\"  Max Daily Loss Limit: ‚Çπ{calculated_max_daily_loss_global:,.2f}\" if calculated_max_daily_loss_global > 0 else \"  Max Daily Loss: Not set\")\n",
        "    print(f\"  Today's Portfolio Net P&L: ‚Çπ{portfolio_daily_pnl_achieved:.2f}\")\n",
        "    print(f\"  Portfolio Trades Today: {portfolio_trades_today_count} / {calculated_max_portfolio_trades_today if calculated_max_portfolio_trades_today > 0 else 'N/A'}\")\n",
        "    print(f\"  Max Trades Per Symbol: {MAX_TRADES_PER_SYMBOL_PER_DAY}\")\n",
        "    print(f\"  Global Trading Halted: {'YES' if is_trading_halted_for_day_global else 'NO'}\")\n",
        "    print(f\"  New Entries Allowed: {'YES' if globals().get('can_place_new_order_today_global', True) else 'NO'}\")\n",
        "    print(\"\\n--- Symbol-Specific Status ---\")\n",
        "    if selected_symbols_for_session:\n",
        "        for sym_name in selected_symbols_for_session:\n",
        "            state = live_states_by_symbol.get(sym_name.upper(), {})\n",
        "            pos = state.get('current_position', 'N/A'); qty = state.get('current_order_quantity', 0)\n",
        "            pos_disp = f\"{pos} ({qty} sh)\" if pos not in ['None', 'N/A'] and qty > 0 else pos\n",
        "            trades_sym = state.get('trades_today_symbol',0); halt_perf = 'YES' if state.get('is_halted_for_performance', False) else 'NO'\n",
        "            loss_d = state.get('consecutive_loss_days',0)\n",
        "            slm = state.get('current_sl_atr_multiplier',SL_ATR_MULTIPLIER_DEFAULT); tpm = state.get('current_tp_atr_multiplier',TP_ATR_MULTIPLIER_DEFAULT)\n",
        "            print(f\"  - {sym_name.upper()}: Pos:{pos_disp}, Trades:{trades_sym}/{MAX_TRADES_PER_SYMBOL_PER_DAY}, PNL:‚Çπ{state.get('daily_pnl_symbol',0):.2f}, Halt(Perf):{halt_perf}({loss_d}d), SLM:{slm:.2f}, TPM:{tpm:.2f}\")\n",
        "    else: print(\"  No symbols selected for detailed status.\")\n",
        "    print(\"-----------------------------------------------------------------------\\n\"\n",
        "          \"  [1] Load & Preprocess Data          [P] Process Symbol Trade Logs\\n\"\n",
        "          \"  [2] Tune Hyperparameters (Async)    [S] Adapt Symbol Strategy Params (Async)\\n\"\n",
        "          \"  [3] Train Model(s) (Async)\\n\"\n",
        "          \"  [4] Backtest Model(s) (Sync)\\n\"\n",
        "          \"-----------------------------------------------------------------------\\n\"\n",
        "          \"  [5] Live Trading (ALERTS ONLY)      [7] Get Current Positions (Upstox)\\n\"\n",
        "          \"  [6] Live Trading (AUTO ORDERS)      [8] Get Funds & Margin (Upstox)\\n\"\n",
        "          \"-----------------------------------------------------------------------\\n\"\n",
        "          \"  [9] Change Symbol Selection         [C] Toggle Capital Allocation Mode\\n\"\n",
        "          \"  [X] PANIC! -> Close All Positions   [0] Exit Program\\n\"\n",
        "          \"-----------------------------------------------------------------------\")\n",
        "    pass\n",
        "\n",
        "async def get_user_input_non_blocking(prompt: str) -> str:\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(None, input, prompt)\n",
        "\n",
        "\n",
        "async def handle_panic_button_cli():\n",
        "    \"\"\"\n",
        "    Handles the emergency 'panic' action. This version is non-blocking and\n",
        "    comprehensively cancels ALL pending orders before liquidating positions.\n",
        "    \"\"\"\n",
        "    global logger, live_states_by_symbol, is_trading_halted_for_day_global, save_state_to_json\n",
        "    global cancel_upstox_order_live, get_upstox_positions_live, place_upstox_order_live, send_telegram_message\n",
        "\n",
        "    logger.warning(\"!!! PANIC BUTTON INITIATED !!!\")\n",
        "\n",
        "    confirm = await get_user_input_non_blocking(\"ARE YOU SURE you want to cancel all pending orders and close all positions? (yes/no): \")\n",
        "    if confirm.lower() != 'yes':\n",
        "        logger.info(\"Panic action cancelled by user.\")\n",
        "        return\n",
        "\n",
        "    await send_telegram_message(\"üö® **PANIC BUTTON ACTIVATED!** üö®\\nAttempting to close all positions and cancel orders now.\")\n",
        "\n",
        "    # 1. Get ALL pending order IDs from the internal state (both entry and exit)\n",
        "    all_pending_order_ids = []\n",
        "    for state in live_states_by_symbol.values():\n",
        "        if state.get('pending_entry_order_id'):\n",
        "            all_pending_order_ids.append(state['pending_entry_order_id'])\n",
        "        if state.get('pending_exit_order_id'):\n",
        "            all_pending_order_ids.append(state['pending_exit_order_id'])\n",
        "\n",
        "    logger.info(f\"--- Step 1: Cancelling {len(all_pending_order_ids)} pending orders from bot state... ---\")\n",
        "    if all_pending_order_ids:\n",
        "        await asyncio.gather(*(cancel_upstox_order_live(order_id) for order_id in all_pending_order_ids))\n",
        "    else:\n",
        "        logger.info(\"No pending orders found in bot's internal state.\")\n",
        "\n",
        "    # 2. Get the definitive list of open positions from the broker\n",
        "    logger.info(\"--- Step 2: Fetching and closing all open positions from broker... ---\")\n",
        "    try:\n",
        "        open_positions = await get_upstox_positions_live()\n",
        "        if not open_positions:\n",
        "            logger.info(\"No open positions reported by the broker.\")\n",
        "        else:\n",
        "            liquidation_tasks = []\n",
        "            for pos in open_positions:\n",
        "                qty = int(getattr(pos, 'quantity', 0))\n",
        "                instr_key = getattr(pos, 'instrument_token', None)\n",
        "                if qty == 0 or not instr_key: continue\n",
        "\n",
        "                transaction_type = \"SELL\" if qty > 0 else \"BUY\"\n",
        "                logger.warning(f\"PANIC EXIT: Placing {transaction_type} order for {abs(qty)} shares of {getattr(pos, 'tradingsymbol', 'N/A')}.\")\n",
        "                liquidation_tasks.append(\n",
        "                    place_upstox_order_live(instrument_key=instr_key, quantity=abs(qty), transaction_type=transaction_type, order_type=\"MARKET\", tag=\"PANIC_EXIT\")\n",
        "                )\n",
        "            if liquidation_tasks: await asyncio.gather(*liquidation_tasks)\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"CRITICAL ERROR during panic exit: {e}. MANUAL INTERVENTION REQUIRED.\", exc_info=True)\n",
        "        await send_telegram_message(\"üö® CRITICAL ERROR during panic exit. CHECK POSITIONS MANUALLY!\")\n",
        "\n",
        "    logger.info(\"--- Step 3: Halting all trading and resetting state. ---\")\n",
        "    is_trading_halted_for_day_global = True\n",
        "    globals()['can_place_new_order_today_global'] = False\n",
        "    for state in live_states_by_symbol.values():\n",
        "        state.update({'current_position': 'None', 'pending_entry_order_id': None, 'active_entry_order_id': None, 'current_order_quantity': 0, 'pending_exit_order_id': None})\n",
        "\n",
        "    await send_telegram_message(\"üõë Bot is now HALTED. All positions liquidated. No further trades will be placed today.\")\n",
        "    save_state_to_json()\n",
        "    logger.info(\"Panic sequence complete. The bot is now in a safe, halted state.\")\n",
        "\n",
        "\n",
        "async def main_cli_menu():\n",
        "    \"\"\"\n",
        "    Main asynchronous function to run the CLI menu, featuring automated retraining,\n",
        "    non-blocking input, state locking, and granular prerequisite checks.\n",
        "    \"\"\"\n",
        "    global logger, selected_symbols_for_session, AUTO_ORDER_EXECUTION_ENABLED, is_long_running_task_active\n",
        "    global data_store_by_symbol, trained_models_by_symbol, SYMBOLS_REQUIRING_RETRAINING, KERAS_TUNER_ENABLED\n",
        "\n",
        "    _handle_panic_fn = handle_panic_button_cli\n",
        "    _initialize_db_fn = globals().get('initialize_database_for_symbol', lambda s: logger.error(\"DB init function not found.\"))\n",
        "    _load_preprocess_fn = globals().get('load_and_preprocess_data_for_symbol')\n",
        "    _process_logs_fn = globals().get('process_symbol_trade_logs_for_learning')\n",
        "    _run_tuning_fn = globals().get('run_standalone_tuning_pipeline')\n",
        "    _run_training_fn = globals().get('run_adv_training_pipeline')\n",
        "    _run_backtesting_fn = globals().get('run_adv_backtesting_pipeline')\n",
        "    _run_live_trading_fn = globals().get('run_adv_live_trading_loop')\n",
        "    _init_upstox_fn = globals().get('initialize_upstox_client')\n",
        "    _init_telegram_fn = globals().get('initialize_telegram_bot_async')\n",
        "    _get_positions_fn = globals().get('get_upstox_positions_live')\n",
        "    _get_funds_fn = globals().get('get_upstox_funds_and_margin_live')\n",
        "    _stop_ws_fn = globals().get('stop_upstox_websocket')\n",
        "    _adapt_params_fn = globals().get('adapt_strategy_parameters_for_symbol')\n",
        "\n",
        "    display_main_header_cli()\n",
        "    logger.info(\"Initializing Trading Bot CLI Menu...\")\n",
        "    if not telegram_initialized_successfully: await _init_telegram_fn()\n",
        "    if not upstox_api_client_global: await _init_upstox_fn()\n",
        "\n",
        "    logger.info(\"--- Initializing Databases for All Configured & Validated Symbols ---\")\n",
        "    if VALIDATED_SYMBOLS_LIST and isinstance(VALIDATED_SYMBOLS_LIST, list):\n",
        "        for symbol in VALIDATED_SYMBOLS_LIST:\n",
        "            await asyncio.to_thread(_initialize_db_fn, symbol)\n",
        "            logger.info(f\"Database for {symbol} checked/initialized successfully.\")\n",
        "    logger.info(\"--- Database Initialization Complete ---\")\n",
        "\n",
        "    program_running = True\n",
        "    while program_running:\n",
        "        clear_console()\n",
        "\n",
        "        if is_long_running_task_active:\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n    ü§ñ A major task is currently running... ü§ñ\\n      Please wait or press Ctrl+C to interrupt.\\n\" + \"=\"*50)\n",
        "            await asyncio.sleep(5)\n",
        "            continue\n",
        "\n",
        "        if SYMBOLS_REQUIRING_RETRAINING:\n",
        "            symbols_to_process_now = list(SYMBOLS_REQUIRING_RETRAINING)\n",
        "            warn_msg = f\"PERFORMANCE ALERT: The following symbols have been automatically halted and require retraining: {', '.join(symbols_to_process_now)}\"\n",
        "            logger.warning(warn_msg)\n",
        "            await send_telegram_message(f\"‚ö†Ô∏è {warn_msg}\")\n",
        "\n",
        "            print(f\"\\n--- ü§ñ Auto-Retraining Initiated for Halted Symbols: {', '.join(symbols_to_process_now)} ---\")\n",
        "\n",
        "            _training_fn = globals().get('run_adv_training_pipeline')\n",
        "            if _training_fn:\n",
        "                await _training_fn(symbols_to_process_now)\n",
        "            else:\n",
        "                logger.error(\"Could not find training function `run_adv_training_pipeline` to run automatically.\")\n",
        "\n",
        "            SYMBOLS_REQUIRING_RETRAINING.clear()\n",
        "\n",
        "            print(f\"\\n--- ‚úÖ Auto-Retraining Complete. Returning to main menu... ---\")\n",
        "            await asyncio.sleep(3) # Pause for user to see the message\n",
        "\n",
        "        if not selected_symbols_for_session:\n",
        "            if not display_symbol_selection_menu_cli(): program_running = False; break\n",
        "            new_symbols = process_symbol_selection_input_cli()\n",
        "            if new_symbols is None: program_running = False; break\n",
        "            elif new_symbols:\n",
        "                reset_global_states_for_new_session_cli()\n",
        "                selected_symbols_for_session = new_symbols\n",
        "                for sym_init_cli in selected_symbols_for_session:\n",
        "                    sym_upper_cli = sym_init_cli.upper()\n",
        "                    if sym_upper_cli not in live_states_by_symbol: live_states_by_symbol[sym_upper_cli] = {}\n",
        "                    live_states_by_symbol[sym_upper_cli].update({'symbol_name':sym_upper_cli,'current_position':'None','daily_pnl_symbol':0.0,'trades_today_symbol':0,'consecutive_loss_days':0,'is_halted_for_performance':False,'current_sl_atr_multiplier':SL_ATR_MULTIPLIER_DEFAULT,'current_tp_atr_multiplier':TP_ATR_MULTIPLIER_DEFAULT})\n",
        "                logger.info(f\"Initialized states for selected symbols: {', '.join(selected_symbols_for_session)}\")\n",
        "            else: clear_console(); continue\n",
        "\n",
        "        await display_operation_menu_and_status_cli()\n",
        "        choice = await get_user_input_non_blocking(f\"Select operation for ({', '.join(selected_symbols_for_session) or 'GLOBAL'}): \")\n",
        "        choice = choice.strip().lower()\n",
        "\n",
        "        try:\n",
        "            long_running_choices = ['2', '3', '4', '5']\n",
        "            if choice in long_running_choices:\n",
        "                is_long_running_task_active = True\n",
        "                clear_console()\n",
        "                print(f\"\\n--- Starting Operation [{choice}]. This may take some time. Press Ctrl+C to interrupt. ---\")\n",
        "\n",
        "            # --- Menu choices with granular checks ---\n",
        "            if choice in ['2', '3', '4', '5', '6']:\n",
        "\n",
        "                op_name = {'2':'Tuning','3':'Training','4':'Backtesting','5':'Live Trading','6':'Live Trading'}[choice]\n",
        "\n",
        "                ready_symbols, skipped_symbols = [], []\n",
        "                prereq_msg = \"\"\n",
        "\n",
        "                for s in selected_symbols_for_session:\n",
        "                    s_upper = s.upper()\n",
        "                    data_ready = s_upper in data_store_by_symbol and not data_store_by_symbol[s_upper].get('processed_ohlcv_df', pd.DataFrame()).empty\n",
        "                    model_ready = s_upper in trained_models_by_symbol and trained_models_by_symbol[s_upper]\n",
        "\n",
        "                    is_ready = False\n",
        "                    if choice in ['2', '3']: # Tune or Train\n",
        "                        is_ready = data_ready\n",
        "                        prereq_msg = \"Run [1] Load & Preprocess Data first.\"\n",
        "                    elif choice in ['4', '5', '6']: # Backtest or Live Trading\n",
        "                        is_ready = data_ready and model_ready\n",
        "                        prereq_msg = \"Run [1] and [3] first.\"\n",
        "\n",
        "                    if is_ready: ready_symbols.append(s)\n",
        "                    else: skipped_symbols.append(s)\n",
        "\n",
        "                if skipped_symbols:\n",
        "                    logger.warning(f\"Skipped {op_name} for: {', '.join(skipped_symbols)}. {prereq_msg}\")\n",
        "                    await send_telegram_message(f\"‚ö†Ô∏è Skipped {op_name} for {', '.join(skipped_symbols)} (prerequisites not met).\")\n",
        "\n",
        "                if not ready_symbols:\n",
        "                    logger.warning(f\"No symbols were ready for {op_name}.\")\n",
        "                    await asyncio.sleep(2)\n",
        "                else:\n",
        "                    is_long_running_task_active = True\n",
        "                    clear_console()\n",
        "                    print(f\"\\n--- Starting {op_name} for {', '.join(ready_symbols)}. Press Ctrl+C to interrupt. ---\")\n",
        "\n",
        "                    if choice == '2': await _run_tuning_fn(ready_symbols)\n",
        "                    elif choice == '3': await _run_training_fn(ready_symbols)\n",
        "                    elif choice == '4': await asyncio.to_thread(_run_backtesting_fn, ready_symbols)\n",
        "                    elif choice == '5':\n",
        "                        AUTO_ORDER_EXECUTION_ENABLED = False\n",
        "                        await _run_live_trading_fn()\n",
        "                    elif choice == '6':\n",
        "                        confirmation = await get_user_input_non_blocking(\"CONFIRM: Start live AUTO ORDERS? (yes/no): \")\n",
        "                        if confirmation.lower() == 'yes':\n",
        "                            AUTO_ORDER_EXECUTION_ENABLED = True\n",
        "                            await _run_live_trading_fn()\n",
        "                        else:\n",
        "                            is_long_running_task_active = False # Release lock if not confirmed\n",
        "                            logger.info(\"Auto order execution not confirmed.\")\n",
        "            elif choice == '1':\n",
        "                logger.info(f\"Op: Load & Preprocess for: {', '.join(selected_symbols_for_session)}\")\n",
        "                for sym_op in selected_symbols_for_session:\n",
        "                    logger.info(f\"  Processing: {sym_op}...\")\n",
        "                    df_proc, f_cols = await _load_preprocess_fn(sym_op, TARGET_INTERVAL)\n",
        "                    if df_proc is not None and f_cols:\n",
        "                        data_store_by_symbol.setdefault(sym_op.upper(), {})['processed_ohlcv_df'] = df_proc\n",
        "                        data_store_by_symbol[sym_op.upper()]['feature_columns'] = f_cols\n",
        "                        logger.info(f\"  Data processed for {sym_op}.\")\n",
        "                    else:\n",
        "                        logger.error(f\"  Failed to load/process data for {sym_op}.\")\n",
        "                await send_telegram_message(f\"üìä Data Load & Preprocess attempted for: {', '.join(selected_symbols_for_session)} (Check logs).\")\n",
        "            elif choice == '7': # Get Current Positions\n",
        "                logger.info(\"Operation: Get Current Positions (from Upstox)\")\n",
        "                if _get_positions_fn and upstox_api_client_global:\n",
        "                    positions_list = await _get_positions_fn()\n",
        "                    if positions_list is not None and isinstance(positions_list, list):\n",
        "                        if positions_list:\n",
        "                            logger.info(f\"Fetched {len(positions_list)} Open Position(s):\")\n",
        "                            pos_details_msgs = []\n",
        "                            for p_idx, pos_item in enumerate(positions_list):\n",
        "                                sym_pos = getattr(pos_item, 'tradingsymbol', getattr(pos_item, 'instrument_token', 'N/A'))\n",
        "                                qty_pos = getattr(pos_item, 'quantity', 'N/A')\n",
        "                                pnl_pos = getattr(pos_item, 'pnl', getattr(pos_item, 'unrealised_profit', 'N/A'))\n",
        "                                avg_pr_pos = getattr(pos_item, 'average_price', getattr(pos_item, 'buy_avg', getattr(pos_item, 'sell_avg', 'N/A')))\n",
        "                                ltp_pos = getattr(pos_item, 'last_price', 'N/A')\n",
        "                                pos_msg = f\"  {p_idx+1}. {sym_pos}: Qty={qty_pos}, AvgP={avg_pr_pos}, LTP={ltp_pos}, PNL={pnl_pos}\"\n",
        "                                logger.info(pos_msg); pos_details_msgs.append(pos_msg)\n",
        "                            await send_telegram_message(\"üìä Current Open Positions:\\n\" + \"\\n\".join(pos_details_msgs))\n",
        "                        else:\n",
        "                            logger.info(\"No open positions found.\")\n",
        "                            await send_telegram_message(\"‚ÑπÔ∏è No open positions found on Upstox.\")\n",
        "                    else:\n",
        "                        logger.error(\"Failed to fetch positions or received unexpected data.\")\n",
        "                        await send_telegram_message(\"‚ö†Ô∏è Failed to fetch positions from Upstox.\")\n",
        "                else:\n",
        "                    logger.warning(\"Upstox client not initialized or get_positions function missing.\")\n",
        "            elif choice == '8': # Get Funds & Margin\n",
        "                logger.info(\"Operation: Get Funds & Margin (from Upstox)\")\n",
        "                if _get_funds_fn and upstox_api_client_global:\n",
        "\n",
        "                    funds_data_obj = await _get_funds_fn(\"SEC\")\n",
        "\n",
        "                    if funds_data_obj:\n",
        "                        equity_segment_data = None\n",
        "\n",
        "                        if hasattr(funds_data_obj, 'equity'):\n",
        "                            equity_segment_data = funds_data_obj.equity\n",
        "                        elif isinstance(funds_data_obj, dict) and 'equity' in funds_data_obj:\n",
        "                            equity_segment_data = funds_data_obj['equity']\n",
        "\n",
        "                        if equity_segment_data is not None:\n",
        "\n",
        "                            def format_value_for_display(value_raw):\n",
        "                                \"\"\"Helper to format raw SDK values for display.\"\"\"\n",
        "                                if value_raw is None: return \"N/A\"\n",
        "                                try: return f\"{float(value_raw):.2f}\"\n",
        "                                except (ValueError, TypeError): return str(value_raw)\n",
        "\n",
        "                            avail_margin_raw = getattr(equity_segment_data, 'available_margin', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('available_margin')\n",
        "                            used_margin_raw = getattr(equity_segment_data, 'used_margin', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('used_margin')\n",
        "                            payin_amt_raw = getattr(equity_segment_data, 'payin_amount', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('payin_amount')\n",
        "\n",
        "                            span_margin_raw = getattr(equity_segment_data, 'span_margin', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('span_margin')\n",
        "                            adhoc_margin_raw = getattr(equity_segment_data, 'adhoc_margin', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('adhoc_margin')\n",
        "                            notional_cash_raw = getattr(equity_segment_data, 'notional_cash', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('notional_cash')\n",
        "                            exposure_margin_raw = getattr(equity_segment_data, 'exposure_margin', None) if not isinstance(equity_segment_data, dict) else equity_segment_data.get('exposure_margin')\n",
        "\n",
        "                            avail_margin_str = format_value_for_display(avail_margin_raw)\n",
        "                            used_margin_str = format_value_for_display(used_margin_raw)\n",
        "                            payin_amt_str = format_value_for_display(payin_amt_raw)\n",
        "\n",
        "                            span_margin_str = format_value_for_display(span_margin_raw)\n",
        "                            adhoc_margin_str = format_value_for_display(adhoc_margin_raw)\n",
        "                            notional_cash_str = format_value_for_display(notional_cash_raw)\n",
        "                            exposure_margin_str = format_value_for_display(exposure_margin_raw)\n",
        "\n",
        "                            funds_msg_parts = [\n",
        "                                \"üí∞ Upstox Funds & Margin (Equity Segment):\",\n",
        "                                f\"  Available Margin: ‚Çπ{avail_margin_str}\",\n",
        "                                f\"  Used Margin: ‚Çπ{used_margin_str}\",\n",
        "                                f\"  PayIn Amount Today: ‚Çπ{payin_amt_str}\",\n",
        "                                f\"  Span Margin: ‚Çπ{span_margin_str}\",\n",
        "                                f\"  AdHoc Margin: ‚Çπ{adhoc_margin_str}\",\n",
        "                                f\"  Notional Cash: ‚Çπ{notional_cash_str}\",\n",
        "                                f\"  Exposure Margin: ‚Çπ{exposure_margin_str}\"\n",
        "                            ]\n",
        "                            log_msg_display = \"\\n\".join(funds_msg_parts)\n",
        "                            logger.info(log_msg_display)\n",
        "                            await send_telegram_message(log_msg_display)\n",
        "                        else:\n",
        "                            logger.warning(f\"Funds data received, but the 'equity' segment data was missing or None. Full funds_data_obj: {str(funds_data_obj)[:500]}\")\n",
        "                            await send_telegram_message(\"‚ö†Ô∏è Funds data for equity segment is missing/None from Upstox. Check logs.\")\n",
        "                    else:\n",
        "                        logger.error(\"Failed to fetch funds/margin from Upstox (API call result was None or an error occurred before data extraction).\")\n",
        "                        await send_telegram_message(\"‚ùå Failed to fetch funds/margin from Upstox.\")\n",
        "                else:\n",
        "                    logger.warning(\"Upstox client not initialized or get_funds function missing. Cannot fetch funds/margin.\")\n",
        "            elif choice == 'c':\n",
        "                if CAPITAL_ALLOCATION_MODE == 'EQUAL': CAPITAL_ALLOCATION_MODE = 'DYNAMIC_PNL'\n",
        "                else: CAPITAL_ALLOCATION_MODE = 'EQUAL'\n",
        "                success_msg = f\"Capital Allocation Mode switched to: {CAPITAL_ALLOCATION_MODE}\"\n",
        "                logger.info(success_msg); await send_telegram_message(f\"‚öôÔ∏è {success_msg}\")\n",
        "                if portfolio_available_margin > 0:\n",
        "                    active_symbols = [s for s in selected_symbols_for_session if not live_states_by_symbol.get(s.upper(), {}).get('is_halted_for_performance', False)]\n",
        "                    if active_symbols:\n",
        "                        capital_per_symbol = portfolio_available_margin / len(active_symbols)\n",
        "                        for sym in active_symbols: capital_per_symbol_allowance[sym.upper()] = capital_per_symbol\n",
        "                        recalc_msg = f\"Capital re-calculated. New allocation: ‚Çπ{capital_per_symbol:,.2f} per symbol.\"\n",
        "                        logger.info(recalc_msg); await send_telegram_message(f\"‚ÑπÔ∏è {recalc_msg}\")\n",
        "            elif choice == 'p':\n",
        "                logger.info(f\"Op: Process Trade Logs for: {', '.join(selected_symbols_for_session)}\")\n",
        "                for sym_log in selected_symbols_for_session: logger.info(f\"  Processing logs for: {sym_log}...\"); await asyncio.to_thread(_process_logs_fn, sym_log.upper())\n",
        "                await send_telegram_message(f\"üìù Trade log processing attempted for: {', '.join(selected_symbols_for_session)}.\")\n",
        "            elif choice == 's':\n",
        "                logger.info(f\"Op: Adapt Strategy Params for: {', '.join(selected_symbols_for_session)}\")\n",
        "                for sym_adpt in selected_symbols_for_session: logger.info(f\"  Adapting strategy for: {sym_adpt}...\"); await _adapt_params_fn(sym_adpt.upper())\n",
        "                await send_telegram_message(f\"üõ†Ô∏è Strategy param adaptation attempted for: {', '.join(selected_symbols_for_session)}.\")\n",
        "            elif choice == 'x':\n",
        "                await _handle_panic_fn()\n",
        "            elif choice == '9': logger.info(\"Changing symbol selection.\"); selected_symbols_for_session.clear()\n",
        "\n",
        "            elif choice == '0':\n",
        "                program_running = False\n",
        "        except UpstoxApiException as e_cli_upstox_ex:\n",
        "             logger.error(f\"Upstox API Exception in CLI op '{choice}': Status {e_cli_upstox_ex.status} - {e_cli_upstox_ex.reason}\", exc_info=False)\n",
        "             await send_telegram_message(f\"‚ö†Ô∏è Upstox API Error (Op: {choice}): {e_cli_upstox_ex.status}. Check logs.\")\n",
        "             await asyncio.sleep(2)\n",
        "        except Exception as e_cli_gen:\n",
        "            logger.error(f\"Error in CLI op '{choice}': {e_cli_gen}\", exc_info=True)\n",
        "            await send_telegram_message(f\"‚ö†Ô∏è CLI Error (Op: {choice}): {str(e_cli_gen)[:100]}.\")\n",
        "            await asyncio.sleep(2)\n",
        "        finally:\n",
        "            if is_long_running_task_active:\n",
        "                logger.info(f\"Operation finished. Releasing task lock.\")\n",
        "                is_long_running_task_active = False\n",
        "    logger.info(\"Trading bot CLI shutting down...\")\n",
        "    if globals().get('USE_REALTIME_WEBSOCKET_FEED') and globals().get('websocket_thread_global') and globals().get('websocket_thread_global').is_alive(): # type: ignore\n",
        "        logger.info(\"Stopping Upstox WebSocket...\"); _stop_ws_fn()\n",
        "    logger.info(\"Program exited.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if nest_asyncio:\n",
        "        try:\n",
        "            # This logic correctly applies nest_asyncio only when needed\n",
        "            asyncio.get_running_loop()\n",
        "            logger.info(\"An event loop is already running. Applying nest_asyncio.\")\n",
        "            nest_asyncio.apply()\n",
        "        except RuntimeError:\n",
        "            logger.info(\"No event loop detected or loop is closed. nest_asyncio not applied.\")\n",
        "        except Exception as e_nest_apply_check:\n",
        "            logger.warning(f\"Could not definitively check for running event loop or apply nest_asyncio: {e_nest_apply_check}\")\n",
        "\n",
        "    try:\n",
        "        asyncio.run(main_cli_menu())\n",
        "    except KeyboardInterrupt: logger.info(\"\\nProgram terminated by user (Ctrl+C).\")\n",
        "    except RuntimeError as e_main_rt:\n",
        "        if \"cannot be called from a running event loop\" in str(e_main_rt).lower():\n",
        "            if not nest_asyncio: logger.critical(\"FATAL: asyncio.run() error. Install/ensure 'nest_asyncio' for Jupyter/Spyder.\")\n",
        "            else: logger.critical(f\"FATAL: asyncio.run() error despite nest_asyncio: {e_main_rt}. Event loop conflict.\")\n",
        "        else: logger.critical(f\"Unhandled RuntimeError at main exec: {e_main_rt}\", exc_info=True)\n",
        "    except Exception as e_main_fatal: logger.critical(f\"Fatal error at main execution: {e_main_fatal}\", exc_info=True)\n",
        "    finally: logger.info(\"Main execution scope finished. Bot shutdown complete.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}